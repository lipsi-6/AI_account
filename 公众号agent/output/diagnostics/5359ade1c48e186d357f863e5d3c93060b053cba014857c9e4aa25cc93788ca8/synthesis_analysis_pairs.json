[
  {
    "section_id": "heading",
    "confidence": 0.3,
    "original_preview": "###### Abstract Reasoning, the process of devising and executing complex goal-oriented action sequences, remains a critical challenge in AI. Inspired by the hierarchical and multi-timescale processing in the human brain, we propose the Hierarchical Reasoning Model (HRM), a novel recurrent architecture that attains significant computational depth while maintaining both training stability and efficiency. HRM executes sequential reasoning tasks in a single forward pass without explicit supervision of the intermediate process, through two interdependent recurrent modules: a high-level module respo",
    "analysis_preview": "对意群内容的基本描述：###### Abstract Reasoning, the process of devising and executing complex goal-oriented action sequences, remains a critical challenge in AI. Inspired by the hierarchical and multi-timescale processing..."
  },
  {
    "section_id": "heading",
    "confidence": 0.3,
    "original_preview": "Current large language models (LLMs) primarily employ Chain-of-Thought (CoT) techniques, which suffer from brittle task decomposition, extensive data requirements, and high latency.",
    "analysis_preview": "对意群内容的基本描述：Current large language models (LLMs) primarily employ Chain-of-Thought (CoT) techniques, which suffer from brittle task decomposition, extensive data requirements, and high latency...."
  },
  {
    "section_id": "introduction",
    "confidence": 0.3,
    "original_preview": "Introduction Deep learning, as its name suggests, emerged from the idea of stacking more layers to achieve increased representation power and improved performance [1, 2]. However, despite the remarkable success of large language models, their core architecture is paradoxically shallow [3]. This imposes a fundamental constraint on their most sought-after capability: reasoning. The fixed depth of standard Transformers places them in computational complexity classes such as \\(AC^{0}\\) or \\(TC^{0}\\)[4], preventing them from solving problems that require polynomial time [5, 6]. LLMs are not Turing-",
    "analysis_preview": "对意群内容的基本描述：Introduction Deep learning, as its name suggests, emerged from the idea of stacking more layers to achieve increased representation power and improved performance [1, 2]. However, despite the remarkab..."
  },
  {
    "section_id": "introduction",
    "confidence": 0.3,
    "original_preview": "Footnote 1: Simply increasing the model width does not improve performance here.",
    "analysis_preview": "对意群内容的基本描述：Footnote 1: Simply increasing the model width does not improve performance here...."
  },
  {
    "section_id": "introduction",
    "confidence": 0.3,
    "original_preview": "A more efficient approach is needed to minimize these data requirements [14]. 3%**, which substantially surpasses leading CoT-based models like o3-mini-high (34. 5%) and Claude 3. 7 8K context (21. 2%), despite their considerably larger parameter sizes and context lengths, as shown in Figure 1.",
    "analysis_preview": "对意群内容的基本描述：A more efficient approach is needed to minimize these data requirements [14]. 3%**, which substantially surpasses leading CoT-based models like o3-mini-high (34. 5%) and Claude 3. 7 8K context (21. 2%..."
  },
  {
    "section_id": "heading",
    "confidence": 0.3,
    "original_preview": "## 2 Hierarchical Reasoning Model We present the HRM, inspired by three fundamental principles of neural computation observed in the brain: * [leftmargin=*] * **Hierarchical processing:** The brain processes information across a hierarchy of cortical areas. Higher-level areas integrate information over longer timescales and form abstract representations, while lower-level areas handle more immediate, detailed sensory and motor processing [20, 22, 21]. Figure 2: **The necessity of depth for complex reasoning. Left:** On _Sudoku-Extreme Full_, which require extensive tree-search and backtracking",
    "analysis_preview": "对意群内容的基本描述：## 2 Hierarchical Reasoning Model We present the HRM, inspired by three fundamental principles of neural computation observed in the brain: * [leftmargin=*] * **Hierarchical processing:** The brain pr..."
  },
  {
    "section_id": "heading",
    "confidence": 0.3,
    "original_preview": ", slow theta waves, 4-8 Hz and fast gamma waves, 30-100 Hz) [30, 31]. Based Figure 3: Comparison of forward residuals and PCA trajectories.",
    "analysis_preview": "对意群内容的基本描述：, slow theta waves, 4-8 Hz and fast gamma waves, 30-100 Hz) [30, 31]. Based Figure 3: Comparison of forward residuals and PCA trajectories...."
  },
  {
    "section_id": "heading",
    "confidence": 0.3,
    "original_preview": "A halting mechanism (detailed later in this section) determines whether the model should terminate, in which case \\(\\hat{y}\\) will be used as the final prediction, or continue with an additional forward pass. Let \\(M\\) denote the total number of segments executed before termination. A Q-head uses the final state of the H-module to predict the Q-values \\(\\hat{Q}^{m}=(\\hat{Q}^{m}_{\\text{halt}},\\hat{Q}^{m}_{\\text{continue}})\\) of the \"halt\" and \"continue\" actions: \\[\\hat{Q}^{m}=\\sigma(\\theta_{Q}^{\\top}z_{H}^{mNT})\\,,\\] where \\(\\sigma\\) denotes the sigmoid function applied element-wise. The halt o",
    "analysis_preview": "对意群内容的基本描述：A halting mechanism (detailed later in this section) determines whether the model should terminate, in which case \\(\\hat{y}\\) will be used as the final prediction, or continue with an additional forwa..."
  },
  {
    "section_id": "heading",
    "confidence": 0.3,
    "original_preview": "2 Stability of Q-learning in ACT The deep Q-learning that underpins our ACT mechanism is known to be prone to instability, often requiring stabilization techniques such as replay buffers and target networks [48], which are absent in our design. Our approach, however, achieves stability through the intrinsic properties of our model and training procedure. [49] shows that Q-learning can achieve convergence if network parameters are bounded, weight decay is incorporated during training, and post-normalization layers are implemented.",
    "analysis_preview": "对意群内容的基本描述：2 Stability of Q-learning in ACT The deep Q-learning that underpins our ACT mechanism is known to be prone to instability, often requiring stabilization techniques such as replay buffers and target ne..."
  },
  {
    "section_id": "heading",
    "confidence": 0.3,
    "original_preview": "Recent theoretical work by Gallici et al.",
    "analysis_preview": "对意群内容的基本描述：Recent theoretical work by Gallici et al...."
  },
  {
    "section_id": "heading",
    "confidence": 0.3,
    "original_preview": "Our model satisfies these conditions through its Post-Norm architecture that employs RMSNorm (a layer normalization variant) and the AdamW optimizer. AdamW has been shown to solve an \\(L_{\\infty}\\)-constrained optimization problem, ensuring that model parameters remain bounded by \\(1/\\lambda\\)[50].",
    "analysis_preview": "对意群内容的基本描述：Our model satisfies these conditions through its Post-Norm architecture that employs RMSNorm (a layer normalization variant) and the AdamW optimizer. AdamW has been shown to solve an \\(L_{\\infty}\\)-co..."
  },
  {
    "section_id": "heading",
    "confidence": 0.3,
    "original_preview": "3 Architectural details We employ a sequence-to-sequence architecture for HRM. For small-sample experiments, we replace softmax with stablemax [51] to improve generalization performance. Both the low-level and high-level recurrent modules \\(f_{L}\\) and \\(f_{H}\\) are implemented using encoder-only Transformer [52] blocks with identical architectures and dimensions. These modules take multiple inputs, and we use straightforward element-wise addition to combine them, though more sophisticated merging techniques such as gating mechanisms could potentially improve performance and is left for future",
    "analysis_preview": "对意群内容的基本描述：3 Architectural details We employ a sequence-to-sequence architecture for HRM. For small-sample experiments, we replace softmax with stablemax [51] to improve generalization performance. Both the low-..."
  },
  {
    "section_id": "heading",
    "confidence": 0.3,
    "original_preview": "Both input and output are represented as token sequences: \\(x=(x_{1},\\ldots,x_{l})\\) and \\(y=(y_{1},\\ldots,y_{l^{\\prime}})\\) respectively. The model includes an embedding layer \\(f_{I}\\) that converts discrete tokens into vector representations, and an output head \\(f_{O}(z; \\theta_{O})=\\text{softmax}(\\theta_{O}z)\\) that transforms hidden states into token probability distributions \\(\\hat{y}\\). The sequence-to-sequence loss is averaged over all tokens, \\(\\textsc{Loss}(\\hat{y},y)=\\frac{1}{l^{\\prime}}\\sum_{i=1}^{l^{\\prime}}\\log p(y_{ i})\\), where \\(p(y_{i})\\) is the probability that distribution",
    "analysis_preview": "对意群内容的基本描述：Both input and output are represented as token sequences: \\(x=(x_{1},\\ldots,x_{l})\\) and \\(y=(y_{1},\\ldots,y_{l^{\\prime}})\\) respectively. The model includes an embedding layer \\(f_{I}\\) that converts..."
  },
  {
    "section_id": "heading",
    "confidence": 0.3,
    "original_preview": "**(a)** Mean compute steps used by models with ACT versus models with a fixed number of compute steps (\\(M\\)). ACT maintains a low and stable number of average compute steps even as the maximum limit (\\(M_{\\max}\\)) increases. **(b)** Accuracy comparison. The ACT model achieves performance comparable to the fixed-compute model while utilizing substantially fewer computational steps on average. **(c)** Inference-time scalability. Models trained with a specific \\(M_{\\max}\\) can generalize to higher computational limits during inference, leading to improved accuracy. For example, a model trained w",
    "analysis_preview": "对意群内容的基本描述：**(a)** Mean compute steps used by models with ACT versus models with a fixed number of compute steps (\\(M\\)). ACT maintains a low and stable number of average compute steps even as the maximum limit ..."
  },
  {
    "section_id": "heading",
    "confidence": 0.3,
    "original_preview": "## 3 Results\n\nThis section begins by describing the ARC-AGI, Sudoku, and Maze benchmarks, followed by an overview of the baseline models and their results. Figure 6-(a,b,c) presents a visual representation of the three benchmark tasks, which are selected to evaluate various reasoning abilities in AI models.",
    "analysis_preview": "对意群内容的基本描述：## 3 Results\n\nThis section begins by describing the ARC-AGI, Sudoku, and Maze benchmarks, followed by an overview of the baseline models and their results. Figure 6-(a,b,c) presents a visual represent..."
  },
  {
    "section_id": "heading",
    "confidence": 0.3,
    "original_preview": "### Benchmarks",
    "analysis_preview": "对意群内容的基本描述：### Benchmarks..."
  },
  {
    "section_id": "heading",
    "confidence": 0.3,
    "original_preview": "1 ARC-AGI Challenge The ARC-AGI benchmark evaluates general fluid intelligence through IQ-test-like puzzles that require inductive reasoning [27]. The initial version, ARC-AGI-1, presents challenges as input-output grid pairs that force AI systems to extract and generalize abstract rules from just a few examples. Each task provides a few input-output example pairs (usually 2-3) and a test input. An AI model has two attempts to produce the correct output grid. Although some believe that mastering ARC-AGI would signal true artificial general intelligence, its primary purpose is to expose the cur",
    "analysis_preview": "对意群内容的基本描述：1 ARC-AGI Challenge The ARC-AGI benchmark evaluates general fluid intelligence through IQ-test-like puzzles that require inductive reasoning [27]. The initial version, ARC-AGI-1, presents challenges a..."
  },
  {
    "section_id": "heading",
    "confidence": 0.3,
    "original_preview": "Lehnert et al.",
    "analysis_preview": "对意群内容的基本描述：Lehnert et al...."
  },
  {
    "section_id": "heading",
    "confidence": 0.3,
    "original_preview": "### Visualization of intermediate timesteps Although HRM demonstrates strong performance on complex reasoning tasks, it raises an intriguing question: what underlying reasoning algorithms does the HRM neural network actually implement? Addressing this question is important for enhancing model interpretability and developing a deeper understanding of the HRM solution space. In the Maze task, HRM appears to initially explore several potential paths simultaneously, subsequently eliminating blocked or inefficient routes, then constructing a preliminary solution outline followed by multiple refinem",
    "analysis_preview": "对意群内容的基本描述：### Visualization of intermediate timesteps Although HRM demonstrates strong performance on complex reasoning tasks, it raises an intriguing question: what underlying reasoning algorithms does the HRM..."
  },
  {
    "section_id": "heading",
    "confidence": 0.3,
    "original_preview": "While a definitive answer lies beyond our current scope, we begin our investigation by analyzing state trajectories and their corresponding solution evolution. In Sudoku, the strategy resembles a depth-first search approach, where the model appears to explore potential solutions and backtracks when it hits dead ends. Unlike Sudoku, which involves frequent backtracking, the ARC solution path follows a more consistent progression similar to hill-climbing optimization.",
    "analysis_preview": "对意群内容的基本描述：While a definitive answer lies beyond our current scope, we begin our investigation by analyzing state trajectories and their corresponding solution evolution. In Sudoku, the strategy resembles a dept..."
  },
  {
    "section_id": "heading",
    "confidence": 0.3,
    "original_preview": "More specifically, at each timestep \\(i\\) and given the low-level and high-level state pair (\\(z^{i}_{L}\\) and \\(z^{i}_{H}\\)) we perform a preliminary forward pass through the H-module to obtain \\(\\bar{z}^{i}=f_{H}(z^{i}_{H},z^{i}_{L}; \\theta_{H})\\) and its corresponding decoded prediction \\(\\bar{y}^{i}=f_{O}(\\bar{z}^{i}; \\theta_{O})\\). The prediction \\(\\bar{y}^{i}\\) is then visualized in Figure 7.",
    "analysis_preview": "对意群内容的基本描述：More specifically, at each timestep \\(i\\) and given the low-level and high-level state pair (\\(z^{i}_{L}\\) and \\(z^{i}_{H}\\)) we perform a preliminary forward pass through the H-module to obtain \\(\\ba..."
  },
  {
    "section_id": "heading",
    "confidence": 0.3,
    "original_preview": "## 4 Brain Correspondence A key principle from systems neuroscience is that a brain region's functional repertoire--its ability to handle diverse and complex tasks--is closely linked to the dimensionality of its neural representations [75, 76]. Higher-order cortical areas, responsible for complex reasoning and decision-making, must handle a wide variety of tasks, demanding more flexible and context-dependent processing [77]. In dynamical systems, this flexibility is often realized through higher-dimensional state-space trajectories, which allow for a richer repertoire of potential computations",
    "analysis_preview": "对意群内容的基本描述：## 4 Brain Correspondence A key principle from systems neuroscience is that a brain region's functional repertoire--its ability to handle diverse and complex tasks--is closely linked to the dimensiona..."
  },
  {
    "section_id": "heading",
    "confidence": 0.3,
    "original_preview": "grey shading indicates changes from the previous timestep.",
    "analysis_preview": "对意群内容的基本描述：grey shading indicates changes from the previous timestep...."
  },
  {
    "section_id": "heading",
    "confidence": 0.3,
    "original_preview": "**Bottom:** ARC-AGI-2 Task—left: provided example input-output pair; right: intermediate steps solving the test input.",
    "analysis_preview": "对意群内容的基本描述：**Bottom:** ARC-AGI-2 Task—left: provided example input-output pair; right: intermediate steps solving the test input...."
  },
  {
    "section_id": "heading",
    "confidence": 0.3,
    "original_preview": "The results show a striking parallel to the biological findings.",
    "analysis_preview": "对意群内容的基本描述：The results show a striking parallel to the biological findings...."
  },
  {
    "section_id": "heading",
    "confidence": 0.3,
    "original_preview": "## 5 Related Work",
    "analysis_preview": "对意群内容的基本描述：## 5 Related Work..."
  },
  {
    "section_id": "heading",
    "confidence": 0.3,
    "original_preview": "1 Reasoning and algorithm learning Given the central role of reasoning problems and their close relation to algorithms, researchers have long explored neural architectures that enable algorithm learning from training instances. This line of work includes Neural Turing Machines (NTM)[83], the Differentiable Neural Computer (DNC)[84], and Neural GPUs[85]-all of which construct iterative neural architectures that mimic computational hardware for algorithm execution, and are trained to learn algorithms from data. Another notable work in this area is Recurrent Relational Networks (RRN)[62], which e",
    "analysis_preview": "对意群内容的基本描述：1 Reasoning and algorithm learning Given the central role of reasoning problems and their close relation to algorithms, researchers have long explored neural architectures that enable algorithm learni..."
  },
  {
    "section_id": "heading",
    "confidence": 0.3,
    "original_preview": "Geiping et al. Shen et al.",
    "analysis_preview": "对意群内容的基本描述：Geiping et al. Shen et al...."
  },
  {
    "section_id": "heading",
    "confidence": 0.3,
    "original_preview": "We also mention adaptive halting mechanisms designed to allocate additional computational resources to more challenging problems. This includes the Adaptive Computation Time (ACT) for RNNs[88] and follow-up research like PonderNet[89], which aims to improve the stability of this allocation process.",
    "analysis_preview": "对意群内容的基本描述：We also mention adaptive halting mechanisms designed to allocate additional computational resources to more challenging problems. This includes the Adaptive Computation Time (ACT) for RNNs[88] and fol..."
  },
  {
    "section_id": "heading",
    "confidence": 0.3,
    "original_preview": "2 Brain-inspired reasoning architectures Developing a model with the reasoning power of the brain has long been a goal in brain-inspired computing. This design enables an architecture to perform a range of cognitive tasks, from memory recall to simple reasoning puzzles. However, its reasoning relies on hand-designed algorithms, which may limit its ability to learn new tasks. These models often require hand-made rules to be set up for solving a specific reasoning task. In essence, while prior models are restricted to simple reasoning problems, HRM is designed to solve complex tasks that are har",
    "analysis_preview": "对意群内容的基本描述：2 Brain-inspired reasoning architectures Developing a model with the reasoning power of the brain has long been a goal in brain-inspired computing. This design enables an architecture to perform a ran..."
  },
  {
    "section_id": "heading",
    "confidence": 0.3,
    "original_preview": "Spaun[90] is one notable example, which uses spiking neural networks to create distinct modules corresponding to brain regions like the visual cortex and prefrontal cortex. Another significant model is the Tolman-Eichenbaum Machine (TEM)[91], which is inspired by the hippocampal-entorhinal system's role in spatial and relational memory tasks. TEM proposes that medial entorhinal cells create a basis for structural knowledge, while hippocampal cells link this basis to sensory information. Another approach involves neural sampling models [92], which view the neural signaling process as inference ",
    "analysis_preview": "对意群内容的基本描述：Spaun[90] is one notable example, which uses spiking neural networks to create distinct modules corresponding to brain regions like the visual cortex and prefrontal cortex. Another significant model i..."
  },
  {
    "section_id": "heading",
    "confidence": 0.3,
    "original_preview": "This allows TEM to generalize and explains the emergence of various cell types like grid, border, and place cells.",
    "analysis_preview": "对意群内容的基本描述：This allows TEM to generalize and explains the emergence of various cell types like grid, border, and place cells...."
  },
  {
    "section_id": "heading",
    "confidence": 0.3,
    "original_preview": "## 6 Discussions **Turing-completeness of HRM**: Like earlier neural reasoning algorithms including the Universal Transformer [95], HRM is computationally universal when given sufficient memory and time constraints. Given that earlier neural algorithm reasoners were trained as recurrent neural networks, they suffer from premature convergence and memory intensive BPTT. By resolving these two challenges and being equipped with adaptive computation, HRM could be trained on long reasoning processes, solve complex puzzles requiring intensive depth-first search and backtracking, and move closer to p",
    "analysis_preview": "对意群内容的基本描述：## 6 Discussions **Turing-completeness of HRM**: Like earlier neural reasoning algorithms including the Universal Transformer [95], HRM is computationally universal when given sufficient memory and ti..."
  },
  {
    "section_id": "heading",
    "confidence": 0.3,
    "original_preview": "In other words, it falls into the category of models that can simulate any Turing machine, overcoming the computational limitations of standard Transformers discussed previously in the introduction. Therefore, in practice, their effective computational depth remains limited, though still deeper than that of a standard Transformer. However, substituting the attention mechanism alone does not change the fact that Transformers are still fixed-depth, and require CoT as a compensatory mechanism.",
    "analysis_preview": "对意群内容的基本描述：In other words, it falls into the category of models that can simulate any Turing machine, overcoming the computational limitations of standard Transformers discussed previously in the introduction. T..."
  },
  {
    "section_id": "heading",
    "confidence": 0.3,
    "original_preview": "## 7 Conclusion This work introduces the Hierarchical Reasoning Model, a brain-inspired architecture that leverages hierarchical structure and multi-timescale processing to achieve substantial computational depth without sacrificing training stability or efficiency. With only 27M parameters and training on just 1000 examples, HRM effectively solves challenging reasoning problems such as ARC, Sudoku, and complex maze navigation-tasks that typically pose significant difficulties for contemporary LLM and chain-of-thought models. Although the brain relies heavily on hierarchical structures to enab",
    "analysis_preview": "对意群内容的基本描述：## 7 Conclusion This work introduces the Hierarchical Reasoning Model, a brain-inspired architecture that leverages hierarchical structure and multi-timescale processing to achieve substantial computa..."
  },
  {
    "section_id": "heading",
    "confidence": 0.3,
    "original_preview": "**Acknowledgements** We thank Mingli Yuan, Ahmed Murtadha Hasan Mahyoub and Hengshuai Yao for their insightful discussions and valuable feedback throughout the course of this work.",
    "analysis_preview": "对意群内容的基本描述：**Acknowledgements** We thank Mingli Yuan, Ahmed Murtadha Hasan Mahyoub and Hengshuai Yao for their insightful discussions and valuable feedback throughout the course of this work...."
  },
  {
    "section_id": "heading",
    "confidence": 0.3,
    "original_preview": "## References * [1] Ian Goodfellow, Yoshua Bengio, and Aaron Courville. * [2] Kaiming He, X. Zhang, Shaoqing Ren, and Jian Sun. * [3] Lena Strobl. * [4] Tom Bylander. * [5] William Merrill and Ashish Sabharwal. * [6] David Chiang. * [7] Lucas Lehnert, Sainbayar Sukhbaatar, DiJia Su, Qinqing Zheng, Paul McVay, Michael Rabbat, and Yuandong Tian. * [8] Wilfried Bounsi, Borja Ibarz, Andrew Dudzik, Jessica B. Hamrick, Larisa Markeeva, Alex Vitvitskyi, Razvan Pascanu, and Petar Velivckovic'c. * [9] William Merrill and Ashish Sabharwal. * [10] Jason Wei, Yi Tay, et al. * [11] William Merrill and Ashi",
    "analysis_preview": "对意群内容的基本描述：## References * [1] Ian Goodfellow, Yoshua Bengio, and Aaron Courville. * [2] Kaiming He, X. Zhang, Shaoqing Ren, and Jian Sun. * [3] Lena Strobl. * [4] Tom Bylander. * [5] William Merrill and Ashish ..."
  },
  {
    "section_id": "heading",
    "confidence": 0.3,
    "original_preview": "_Deep Learning_. deeplearningbook. Deep residual learning for image recognition. _2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 770-778, 2015. Complexity results for planning. In _Proceedings of the 12th International Joint Conference on Artificial Intelligence - Volume 1_, IJCAI'91, page 274-279, San Francisco, CA, USA, 1991. In _Neural Information Processing Systems_, 2023. _Transactions on Machine Learning Research_, 2025. Beyond a*: Better planning with transformers via search dynamics bootstrapping. In _First Conference on Language Modeling_, 2024. _Transa",
    "analysis_preview": "对意群内容的基本描述：_Deep Learning_. deeplearningbook. Deep residual learning for image recognition. _2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 770-778, 2015. Complexity results for p..."
  },
  {
    "section_id": "heading",
    "confidence": 0.3,
    "original_preview": "MIT Press, 2016. http://www. Morgan Kaufmann Publishers Inc. ISBN 1558601600. _ArXiv_, abs/2406. 09308, 2024. 1162/tacl_a_00562. arXiv preprint arXiv:2201. In _ICLR_, 2024. _ArXiv_, abs/2402. 08939, 2024. _arXiv preprint arXiv:2211. 04325_, 2022. _arXiv preprint arXiv:2412. 07423_, 2024. [2024] Evelina Fedorenko, Steven T Piantadosi, and Edward AF Gibson. _Nature_, 630(8017):575-586, 2024. [2024] Hongyu Wang, Shuming Ma, Li Dong, Shaohan Huang, Dongdong Zhang, and Furu Wei. ISSN 0959-4388. doi: https://doi. [2023] Roxana Zeraati, Yan-Liang Shi, Nicholas A Steinmetz, Marc A Gieselmann, Alexande",
    "analysis_preview": "对意群内容的基本描述：MIT Press, 2016. http://www. Morgan Kaufmann Publishers Inc. ISBN 1558601600. _ArXiv_, abs/2406. 09308, 2024. 1162/tacl_a_00562. arXiv preprint arXiv:2201. In _ICLR_, 2024. _ArXiv_, abs/2402. 08939, 2..."
  },
  {
    "section_id": "heading",
    "confidence": 0.3,
    "original_preview": "Average-hard attention transformers are constant-depth uniform threshold circuits, 2023. A logic for expressing log-precision transformers. Transformers in DLOGTIME-uniform TC\\({}^{0}\\). Transformers meet neural algorithmic reasoners. The parallelism tradeoff: Limitations of log-precision transformers. The expressive power of transformers with chain of thought. Roformer: Enhanced transformer with rotary position embedding. Glu variants improve transformer. Universal transformers, 2018. Transformers are ssms: Generalized models and efficient algorithms through structured state space duality.",
    "analysis_preview": "对意群内容的基本描述：Average-hard attention transformers are constant-depth uniform threshold circuits, 2023. A logic for expressing log-precision transformers. Transformers in DLOGTIME-uniform TC\\({}^{0}\\). Transformers ..."
  },
  {
    "section_id": "heading",
    "confidence": 0.3,
    "original_preview": "Will we run out of data?",
    "analysis_preview": "对意群内容的基本描述：Will we run out of data?..."
  },
  {
    "section_id": "heading",
    "confidence": 0.3,
    "original_preview": "Zico Kolter. Zico Kolter. Mozer, and M.",
    "analysis_preview": "对意群内容的基本描述：Zico Kolter. Zico Kolter. Mozer, and M...."
  },
  {
    "section_id": "heading",
    "confidence": 0.3,
    "original_preview": "* [58] JAX Developers. dev/en/latest/_autosummary/jax.",
    "analysis_preview": "对意群内容的基本描述：* [58] JAX Developers. dev/en/latest/_autosummary/jax...."
  },
  {
    "section_id": "heading",
    "confidence": 0.3,
    "original_preview": "initializers. initializers. sourceforge. Perreault.",
    "analysis_preview": "对意群内容的基本描述：initializers. initializers. sourceforge. Perreault...."
  },
  {
    "section_id": "heading",
    "confidence": 0.3,
    "original_preview": "lecun_normal_. lecun_normal.",
    "analysis_preview": "对意群内容的基本描述：lecun_normal_. lecun_normal...."
  },
  {
    "section_id": "heading",
    "confidence": 0.3,
    "original_preview": "Can convolutional neural networks crack sudoku puzzles? com/Kyubyong/sudoku, 2018. Tdoku: A fast sudoku solver and generator. io/tdoku/, 2025. Sudoku-bench: Evaluating creative reasoning with sudoku variants.",
    "analysis_preview": "对意群内容的基本描述：Can convolutional neural networks crack sudoku puzzles? com/Kyubyong/sudoku, 2018. Tdoku: A fast sudoku solver and generator. io/tdoku/, 2025. Sudoku-bench: Evaluating creative reasoning with sudoku v..."
  }
]