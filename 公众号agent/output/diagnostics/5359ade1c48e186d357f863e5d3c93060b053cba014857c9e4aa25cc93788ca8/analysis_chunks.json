[
  {
    "chunk_id": "ff91b35d-c4d6-4bdc-bdaf-4f4999937e99",
    "section_type": "heading",
    "start": 302,
    "end": 2244,
    "topic": "分层循环模型HRM实现高效深度推理",
    "importance": 1.0,
    "complexity": 1,
    "content_preview": "###### Abstract Reasoning, the process of devising and executing complex goal-oriented action sequences, remains a critical challenge in AI. Inspired by the hierarchical and multi-timescale processing in the human brain, we propose the Hierarchical Reasoning Model (HRM), a novel recurrent architecture that attains significant computational depth while maintaining both training stability and efficiency. HRM executes sequential reasoning tasks in a single forward pass without explicit supervision of the intermediate process, through two interdependent recurrent modules: a high-level module responsible for slow, abstract planning, and a low-level module handling rapid, detailed computations. With only 27 million parameters, HRM achieves exceptional performance on complex reasoning tasks using"
  },
  {
    "chunk_id": "0cccd82e-7bf8-4290-a5ac-f4f2f9e5cfce",
    "section_type": "heading",
    "start": 2245,
    "end": 2426,
    "topic": "分层递归模型HRM小样本推理",
    "importance": 0.8425613275613275,
    "complexity": 1,
    "content_preview": "Current large language models (LLMs) primarily employ Chain-of-Thought (CoT) techniques, which suffer from brittle task decomposition, extensive data requirements, and high latency."
  },
  {
    "chunk_id": "b69476b4-4f2c-4341-8692-24935949c1fa",
    "section_type": "introduction",
    "start": 2432,
    "end": 7903,
    "topic": "分层循环模型HRM实现小样本深度推理",
    "importance": 1.0,
    "complexity": 3,
    "content_preview": "Introduction Deep learning, as its name suggests, emerged from the idea of stacking more layers to achieve increased representation power and improved performance [1, 2]. However, despite the remarkable success of large language models, their core architecture is paradoxically shallow [3]. This imposes a fundamental constraint on their most sought-after capability: reasoning. The fixed depth of standard Transformers places them in computational complexity classes such as \\(AC^{0}\\) or \\(TC^{0}\\)[4], preventing them from solving problems that require polynomial time [5, 6]. LLMs are not Turing-complete and thus they cannot, at least in a purely end-to-end manner, execute complex algorithmic reasoning that is necessary for deliberate planning or symbolic manipulation tasks [7, 8]. For exampl"
  },
  {
    "chunk_id": "aa771aee-dec5-4d10-8327-909dad9d32ac",
    "section_type": "introduction",
    "start": 7904,
    "end": 7984,
    "topic": "分层递归模型HRM小样本推理",
    "importance": 0.693989898989899,
    "complexity": 1,
    "content_preview": "Footnote 1: Simply increasing the model width does not improve performance here."
  },
  {
    "chunk_id": "9f195d83-1c0d-4ffc-a3a7-89d2ef000200",
    "section_type": "introduction",
    "start": 7985,
    "end": 8280,
    "topic": "分层递归模型HRM小样本高效推理",
    "importance": 0.8193795093795093,
    "complexity": 1,
    "content_preview": "A more efficient approach is needed to minimize these data requirements [14]. 3%**, which substantially surpasses leading CoT-based models like o3-mini-high (34. 5%) and Claude 3. 7 8K context (21. 2%), despite their considerably larger parameter sizes and context lengths, as shown in Figure 1."
  },
  {
    "chunk_id": "7ddcfab5-c21d-41e3-a247-7f0ca90be17d",
    "section_type": "heading",
    "start": 8286,
    "end": 23356,
    "topic": "分层循环模型HRM实现深度推理",
    "importance": 1.0,
    "complexity": 5,
    "content_preview": "## 2 Hierarchical Reasoning Model We present the HRM, inspired by three fundamental principles of neural computation observed in the brain: * [leftmargin=*] * **Hierarchical processing:** The brain processes information across a hierarchy of cortical areas. Higher-level areas integrate information over longer timescales and form abstract representations, while lower-level areas handle more immediate, detailed sensory and motor processing [20, 22, 21]. Figure 2: **The necessity of depth for complex reasoning. Left:** On _Sudoku-Extreme Full_, which require extensive tree-search and backtracking, increasing a Transformer’s width yields no performance gain, while increasing depth is critical. **Right:** Standard architectures saturates, failing to benefit from increased depth. HRM overcomes t"
  },
  {
    "chunk_id": "de8907e3-08d3-4546-b20a-b7854108f3f1",
    "section_type": "heading",
    "start": 23357,
    "end": 23496,
    "topic": "分层递归模型HRM的深度推理创新",
    "importance": 0.7897835497835498,
    "complexity": 1,
    "content_preview": ", slow theta waves, 4-8 Hz and fast gamma waves, 30-100 Hz) [30, 31]. Based Figure 3: Comparison of forward residuals and PCA trajectories."
  },
  {
    "chunk_id": "f3eb2f5f-b9b6-499b-aad9-c39e21bf9e55",
    "section_type": "heading",
    "start": 23497,
    "end": 25816,
    "topic": "分层循环模型HRM的深度推理机制",
    "importance": 0.9008225108225107,
    "complexity": 1,
    "content_preview": "A halting mechanism (detailed later in this section) determines whether the model should terminate, in which case \\(\\hat{y}\\) will be used as the final prediction, or continue with an additional forward pass. Let \\(M\\) denote the total number of segments executed before termination. A Q-head uses the final state of the H-module to predict the Q-values \\(\\hat{Q}^{m}=(\\hat{Q}^{m}_{\\text{halt}},\\hat{Q}^{m}_{\\text{continue}})\\) of the \"halt\" and \"continue\" actions: \\[\\hat{Q}^{m}=\\sigma(\\theta_{Q}^{\\top}z_{H}^{mNT})\\,,\\] where \\(\\sigma\\) denotes the sigmoid function applied element-wise. The halt or continue action is chosen using a randomized strategy as detailed next. Let \\(M_{\\max}\\) denote the maximum number of segments (a fixed hyperparameter) and \\(M_{\\min}\\) denote the minimum number of "
  },
  {
    "chunk_id": "675bde6b-6e3b-4790-90f7-8a1a263777ef",
    "section_type": "heading",
    "start": 25862,
    "end": 26397,
    "topic": "分层递归模型HRM的稳定高效推理",
    "importance": 0.9615367965367966,
    "complexity": 1,
    "content_preview": "2 Stability of Q-learning in ACT The deep Q-learning that underpins our ACT mechanism is known to be prone to instability, often requiring stabilization techniques such as replay buffers and target networks [48], which are absent in our design. Our approach, however, achieves stability through the intrinsic properties of our model and training procedure. [49] shows that Q-learning can achieve convergence if network parameters are bounded, weight decay is incorporated during training, and post-normalization layers are implemented."
  },
  {
    "chunk_id": "039c6d5b-1acf-4d64-98cf-2c5fe6425840",
    "section_type": "heading",
    "start": 26398,
    "end": 26439,
    "topic": "分层递归模型HRM的深度推理",
    "importance": 0.5654040404040405,
    "complexity": 2,
    "content_preview": "Recent theoretical work by Gallici et al."
  },
  {
    "chunk_id": "28d7d9e1-b474-4d08-9118-59ed73e46356",
    "section_type": "heading",
    "start": 26440,
    "end": 26739,
    "topic": "分层递归模型HRM的深度推理优化",
    "importance": 0.8521572871572873,
    "complexity": 2,
    "content_preview": "Our model satisfies these conditions through its Post-Norm architecture that employs RMSNorm (a layer normalization variant) and the AdamW optimizer. AdamW has been shown to solve an \\(L_{\\infty}\\)-constrained optimization problem, ensuring that model parameters remain bounded by \\(1/\\lambda\\)[50]."
  },
  {
    "chunk_id": "6bde1915-1042-4030-9bf5-45d569a838db",
    "section_type": "heading",
    "start": 26751,
    "end": 28179,
    "topic": "分层循环模型HRM的深度推理优化",
    "importance": 0.9354329004329003,
    "complexity": 1,
    "content_preview": "3 Architectural details We employ a sequence-to-sequence architecture for HRM. For small-sample experiments, we replace softmax with stablemax [51] to improve generalization performance. Both the low-level and high-level recurrent modules \\(f_{L}\\) and \\(f_{H}\\) are implemented using encoder-only Transformer [52] blocks with identical architectures and dimensions. These modules take multiple inputs, and we use straightforward element-wise addition to combine them, though more sophisticated merging techniques such as gating mechanisms could potentially improve performance and is left for future work. For all Transformer blocks in this work--including those in the baseline models--we incorporate the enhancements found in modern LLMs (based on Llama [53] architectures). These improvements inc"
  },
  {
    "chunk_id": "cd690d0e-158c-4835-82ca-37d4ff80c7aa",
    "section_type": "heading",
    "start": 28180,
    "end": 29008,
    "topic": "分层递归模型HRM实现小样本深度推理",
    "importance": 0.948044733044733,
    "complexity": 1,
    "content_preview": "Both input and output are represented as token sequences: \\(x=(x_{1},\\ldots,x_{l})\\) and \\(y=(y_{1},\\ldots,y_{l^{\\prime}})\\) respectively. The model includes an embedding layer \\(f_{I}\\) that converts discrete tokens into vector representations, and an output head \\(f_{O}(z; \\theta_{O})=\\text{softmax}(\\theta_{O}z)\\) that transforms hidden states into token probability distributions \\(\\hat{y}\\). The sequence-to-sequence loss is averaged over all tokens, \\(\\textsc{Loss}(\\hat{y},y)=\\frac{1}{l^{\\prime}}\\sum_{i=1}^{l^{\\prime}}\\log p(y_{ i})\\), where \\(p(y_{i})\\) is the probability that distribution \\(\\hat{y}_{i}\\) assigns to token \\(y_{i}\\). The initial hidden states \\(z^{0}\\) are initialized by sampling from a truncated normal distribution with standard deviation of 1, truncation of 2, and kep"
  },
  {
    "chunk_id": "a9ccdafd-d177-4f7a-a13e-47edcacd5b22",
    "section_type": "heading",
    "start": 29009,
    "end": 29707,
    "topic": "分层循环模型HRM的深度推理高效计算",
    "importance": 1.0,
    "complexity": 1,
    "content_preview": "**(a)** Mean compute steps used by models with ACT versus models with a fixed number of compute steps (\\(M\\)). ACT maintains a low and stable number of average compute steps even as the maximum limit (\\(M_{\\max}\\)) increases. **(b)** Accuracy comparison. The ACT model achieves performance comparable to the fixed-compute model while utilizing substantially fewer computational steps on average. **(c)** Inference-time scalability. Models trained with a specific \\(M_{\\max}\\) can generalize to higher computational limits during inference, leading to improved accuracy. For example, a model trained with \\(M_{\\max}=8\\) continues to see accuracy gains when run with \\(M_{\\max}=16\\) during inference."
  },
  {
    "chunk_id": "c2ba62a8-eba6-49b2-a616-6a8b29d18b04",
    "section_type": "heading",
    "start": 29722,
    "end": 30032,
    "topic": "分层循环模型HRM的少样本深度推理",
    "importance": 1.0,
    "complexity": 1,
    "content_preview": "## 3 Results\n\nThis section begins by describing the ARC-AGI, Sudoku, and Maze benchmarks, followed by an overview of the baseline models and their results. Figure 6-(a,b,c) presents a visual representation of the three benchmark tasks, which are selected to evaluate various reasoning abilities in AI models."
  },
  {
    "chunk_id": "ffe111f3-cdab-4507-b2ca-05a8bf7b0986",
    "section_type": "heading",
    "start": 30032,
    "end": 30048,
    "topic": "\"分层递归模型HRM的深度推理研究\"",
    "importance": 0.5604040404040405,
    "complexity": 1,
    "content_preview": "### Benchmarks"
  },
  {
    "chunk_id": "f55797c7-c61f-42ee-ad4e-3691d75e3270",
    "section_type": "heading",
    "start": 30048,
    "end": 36565,
    "topic": "分层循环模型HRM解决复杂推理任务",
    "importance": 1.0,
    "complexity": 2,
    "content_preview": "1 ARC-AGI Challenge The ARC-AGI benchmark evaluates general fluid intelligence through IQ-test-like puzzles that require inductive reasoning [27]. The initial version, ARC-AGI-1, presents challenges as input-output grid pairs that force AI systems to extract and generalize abstract rules from just a few examples. Each task provides a few input-output example pairs (usually 2-3) and a test input. An AI model has two attempts to produce the correct output grid. Although some believe that mastering ARC-AGI would signal true artificial general intelligence, its primary purpose is to expose the current roadblocks in AGI progress. In fact, both conventional deep learning methods and CoT techniques have faced significant challenges with ARC-AGI-1, primarily because it requires the ability to gene"
  },
  {
    "chunk_id": "73e42676-9c39-48a4-b21f-2c4a318dee0b",
    "section_type": "heading",
    "start": 36566,
    "end": 36580,
    "topic": "分层递归模型HRM深度推理",
    "importance": 0.5404040404040404,
    "complexity": 1,
    "content_preview": "Lehnert et al."
  },
  {
    "chunk_id": "48b14cea-d15c-4ff3-b08a-53166adce1ce",
    "section_type": "heading",
    "start": 40029,
    "end": 41030,
    "topic": "分层循环模型HRM的深度推理机制",
    "importance": 1.0,
    "complexity": 3,
    "content_preview": "### Visualization of intermediate timesteps Although HRM demonstrates strong performance on complex reasoning tasks, it raises an intriguing question: what underlying reasoning algorithms does the HRM neural network actually implement? Addressing this question is important for enhancing model interpretability and developing a deeper understanding of the HRM solution space. In the Maze task, HRM appears to initially explore several potential paths simultaneously, subsequently eliminating blocked or inefficient routes, then constructing a preliminary solution outline followed by multiple refinement iterations. HRM uses a different approach for ARC tasks, making incremental adjustments to the board and iteratively improving it until reaching a solution. Importantly, the model shows that it ca"
  },
  {
    "chunk_id": "1401c9b2-7b92-4119-8c0a-4c69a59d5000",
    "section_type": "heading",
    "start": 41031,
    "end": 41501,
    "topic": "分层递归模型HRM的深度推理研究",
    "importance": 0.7778715728715728,
    "complexity": 2,
    "content_preview": "While a definitive answer lies beyond our current scope, we begin our investigation by analyzing state trajectories and their corresponding solution evolution. In Sudoku, the strategy resembles a depth-first search approach, where the model appears to explore potential solutions and backtracks when it hits dead ends. Unlike Sudoku, which involves frequent backtracking, the ARC solution path follows a more consistent progression similar to hill-climbing optimization."
  },
  {
    "chunk_id": "32fc3ceb-22ed-48b3-98a6-049f547b32fa",
    "section_type": "heading",
    "start": 41502,
    "end": 41903,
    "topic": "分层递归模型HRM的深度推理研究",
    "importance": 0.7350937950937951,
    "complexity": 1,
    "content_preview": "More specifically, at each timestep \\(i\\) and given the low-level and high-level state pair (\\(z^{i}_{L}\\) and \\(z^{i}_{H}\\)) we perform a preliminary forward pass through the H-module to obtain \\(\\bar{z}^{i}=f_{H}(z^{i}_{H},z^{i}_{L}; \\theta_{H})\\) and its corresponding decoded prediction \\(\\bar{y}^{i}=f_{O}(\\bar{z}^{i}; \\theta_{O})\\). The prediction \\(\\bar{y}^{i}\\) is then visualized in Figure 7."
  },
  {
    "chunk_id": "68607623-04e0-4108-ae50-56612c18a4e4",
    "section_type": "heading",
    "start": 41907,
    "end": 49381,
    "topic": "分层循环模型HRM的少样本深度推理",
    "importance": 1.0,
    "complexity": 4,
    "content_preview": "## 4 Brain Correspondence A key principle from systems neuroscience is that a brain region's functional repertoire--its ability to handle diverse and complex tasks--is closely linked to the dimensionality of its neural representations [75, 76]. Higher-order cortical areas, responsible for complex reasoning and decision-making, must handle a wide variety of tasks, demanding more flexible and context-dependent processing [77]. In dynamical systems, this flexibility is often realized through higher-dimensional state-space trajectories, which allow for a richer repertoire of potential computations [78]. This principle gives rise to an observable _dimensionality hierarchy_, where a region's position in the processing hierarchy correlates with its _effective dimensionality_. To quantify this phe"
  },
  {
    "chunk_id": "83aa4cc0-2262-4d64-96fe-bc7e3bc816c8",
    "section_type": "heading",
    "start": 49382,
    "end": 49440,
    "topic": "分层递归模型HRM的深度推理研究",
    "importance": 0.5654040404040405,
    "complexity": 1,
    "content_preview": "grey shading indicates changes from the previous timestep."
  },
  {
    "chunk_id": "1c74796f-e8ba-4441-80a0-65ced77fab86",
    "section_type": "heading",
    "start": 49441,
    "end": 49559,
    "topic": "分层递归模型HRM小样本推理",
    "importance": 0.5796897546897547,
    "complexity": 1,
    "content_preview": "**Bottom:** ARC-AGI-2 Task—left: provided example input-output pair; right: intermediate steps solving the test input."
  },
  {
    "chunk_id": "d41ee866-3b24-4d2b-bc7a-5928bba63ede",
    "section_type": "heading",
    "start": 49560,
    "end": 49624,
    "topic": "分层递归模型HRM小样本推理",
    "importance": 0.8046897546897547,
    "complexity": 1,
    "content_preview": "The results show a striking parallel to the biological findings."
  },
  {
    "chunk_id": "96377695-9c21-4e76-b838-9cec8a747731",
    "section_type": "heading",
    "start": 49651,
    "end": 49670,
    "topic": "分层递归模型HRM的深度推理创新",
    "importance": 0.5654040404040405,
    "complexity": 1,
    "content_preview": "## 5 Related Work"
  },
  {
    "chunk_id": "4b9e2c45-2848-4436-991d-207ea3992622",
    "section_type": "heading",
    "start": 49670,
    "end": 51354,
    "topic": "分层循环模型HRM的深度推理创新",
    "importance": 1.0,
    "complexity": 3,
    "content_preview": "1 Reasoning and algorithm learning Given the central role of reasoning problems and their close relation to algorithms, researchers have long explored neural architectures that enable algorithm learning from training instances. This line of work includes Neural Turing Machines (NTM)[83], the Differentiable Neural Computer (DNC)[84], and Neural GPUs[85]-all of which construct iterative neural architectures that mimic computational hardware for algorithm execution, and are trained to learn algorithms from data. Another notable work in this area is Recurrent Relational Networks (RRN)[62], which executes algorithms on graph representations through graph neural networks. Recent studies have integrated algorithm learning approaches with Transformer-based architectures. Universal Transformers ext"
  },
  {
    "chunk_id": "f2fe3c1e-300d-4a9f-b92c-155c69dcb356",
    "section_type": "heading",
    "start": 51355,
    "end": 51381,
    "topic": "分层递归模型HRM小样本推理",
    "importance": 0.5654040404040405,
    "complexity": 1,
    "content_preview": "Geiping et al. Shen et al."
  },
  {
    "chunk_id": "4052288e-1c31-4bb9-bb2f-cfacea8e7afd",
    "section_type": "heading",
    "start": 51382,
    "end": 51681,
    "topic": "分层循环模型HRM的少样本深度推理",
    "importance": 0.874069264069264,
    "complexity": 1,
    "content_preview": "We also mention adaptive halting mechanisms designed to allocate additional computational resources to more challenging problems. This includes the Adaptive Computation Time (ACT) for RNNs[88] and follow-up research like PonderNet[89], which aims to improve the stability of this allocation process."
  },
  {
    "chunk_id": "2f111008-ba2c-4128-87c1-bc6dda5490ff",
    "section_type": "heading",
    "start": 51695,
    "end": 52529,
    "topic": "分层递归模型HRM实现深度推理",
    "importance": 1.0,
    "complexity": 2,
    "content_preview": "2 Brain-inspired reasoning architectures Developing a model with the reasoning power of the brain has long been a goal in brain-inspired computing. This design enables an architecture to perform a range of cognitive tasks, from memory recall to simple reasoning puzzles. However, its reasoning relies on hand-designed algorithms, which may limit its ability to learn new tasks. These models often require hand-made rules to be set up for solving a specific reasoning task. In essence, while prior models are restricted to simple reasoning problems, HRM is designed to solve complex tasks that are hard for even advanced LLMs, without pre-training or task-specific manual design. Since HRM focuses on reasoning, full attention is applied for simplicity. Incorporating hierarchical memory into HRM coul"
  },
  {
    "chunk_id": "cab69162-8f81-4adf-9682-e794dfda148d",
    "section_type": "heading",
    "start": 52530,
    "end": 53718,
    "topic": "分层循环模型HRM的深度推理研究",
    "importance": 1.0,
    "complexity": 2,
    "content_preview": "Spaun[90] is one notable example, which uses spiking neural networks to create distinct modules corresponding to brain regions like the visual cortex and prefrontal cortex. Another significant model is the Tolman-Eichenbaum Machine (TEM)[91], which is inspired by the hippocampal-entorhinal system's role in spatial and relational memory tasks. TEM proposes that medial entorhinal cells create a basis for structural knowledge, while hippocampal cells link this basis to sensory information. Another approach involves neural sampling models [92], which view the neural signaling process as inference over a distribution, functioning similarly to a Boltzmann machine. **Hierarchical memory**: The hierarchical multi-timescale structure also plays an important role in how the brain processes memory. M"
  },
  {
    "chunk_id": "110b2326-95b2-4642-81e3-db0749f84f77",
    "section_type": "heading",
    "start": 53719,
    "end": 53833,
    "topic": "分层递归模型HRM高效推理",
    "importance": 0.7493795093795095,
    "complexity": 1,
    "content_preview": "This allows TEM to generalize and explains the emergence of various cell types like grid, border, and place cells."
  },
  {
    "chunk_id": "bd7a7ad0-58be-4e41-a57d-ffd0880525c2",
    "section_type": "heading",
    "start": 53848,
    "end": 56198,
    "topic": "分层循环模型HRM实现高效深度推理",
    "importance": 1.0,
    "complexity": 4,
    "content_preview": "## 6 Discussions **Turing-completeness of HRM**: Like earlier neural reasoning algorithms including the Universal Transformer [95], HRM is computationally universal when given sufficient memory and time constraints. Given that earlier neural algorithm reasoners were trained as recurrent neural networks, they suffer from premature convergence and memory intensive BPTT. By resolving these two challenges and being equipped with adaptive computation, HRM could be trained on long reasoning processes, solve complex puzzles requiring intensive depth-first search and backtracking, and move closer to practical Turing-completeness. **Reinforcement learning with chain-of-thought**: Beyond fine-tuning using human-annotated CoT, reinforcement learning (RL) represents another widely adopted training met"
  },
  {
    "chunk_id": "b86c9503-e0ac-41d2-9bbd-0e994bfa2153",
    "section_type": "heading",
    "start": 56199,
    "end": 56694,
    "topic": "分层递归模型HRM的深度推理突破",
    "importance": 1.0,
    "complexity": 1,
    "content_preview": "In other words, it falls into the category of models that can simulate any Turing machine, overcoming the computational limitations of standard Transformers discussed previously in the introduction. Therefore, in practice, their effective computational depth remains limited, though still deeper than that of a standard Transformer. However, substituting the attention mechanism alone does not change the fact that Transformers are still fixed-depth, and require CoT as a compensatory mechanism."
  },
  {
    "chunk_id": "0a8e2ff4-9f2a-4676-a79c-5377cfe21e1f",
    "section_type": "heading",
    "start": 56700,
    "end": 57810,
    "topic": "分层循环模型实现高效深度推理",
    "importance": 1.0,
    "complexity": 1,
    "content_preview": "## 7 Conclusion This work introduces the Hierarchical Reasoning Model, a brain-inspired architecture that leverages hierarchical structure and multi-timescale processing to achieve substantial computational depth without sacrificing training stability or efficiency. With only 27M parameters and training on just 1000 examples, HRM effectively solves challenging reasoning problems such as ARC, Sudoku, and complex maze navigation-tasks that typically pose significant difficulties for contemporary LLM and chain-of-thought models. Although the brain relies heavily on hierarchical structures to enable most cognitive processes, these concepts have largely remained confined to academic literature rather than being translated into practical applications. The prevailing AI approach continues to favo"
  },
  {
    "chunk_id": "113ce265-1a2a-434f-8604-3956a4fd6c61",
    "section_type": "heading",
    "start": 57811,
    "end": 57991,
    "topic": "分层递归模型HRM的深度推理创新",
    "importance": 0.8004978354978356,
    "complexity": 1,
    "content_preview": "**Acknowledgements** We thank Mingli Yuan, Ahmed Murtadha Hasan Mahyoub and Hengshuai Yao for their insightful discussions and valuable feedback throughout the course of this work."
  },
  {
    "chunk_id": "ea463158-fb6d-482a-b850-ba8da6131367",
    "section_type": "heading",
    "start": 57996,
    "end": 64822,
    "topic": "分层递归模型HRM实现小样本深度推理",
    "importance": 0.7993795093795094,
    "complexity": 1,
    "content_preview": "## References * [1] Ian Goodfellow, Yoshua Bengio, and Aaron Courville. * [2] Kaiming He, X. Zhang, Shaoqing Ren, and Jian Sun. * [3] Lena Strobl. * [4] Tom Bylander. * [5] William Merrill and Ashish Sabharwal. * [6] David Chiang. * [7] Lucas Lehnert, Sainbayar Sukhbaatar, DiJia Su, Qinqing Zheng, Paul McVay, Michael Rabbat, and Yuandong Tian. * [8] Wilfried Bounsi, Borja Ibarz, Andrew Dudzik, Jessica B. Hamrick, Larisa Markeeva, Alex Vitvitskyi, Razvan Pascanu, and Petar Velivckovic'c. * [9] William Merrill and Ashish Sabharwal. * [10] Jason Wei, Yi Tay, et al. * [11] William Merrill and Ashish Sabharwal. * [12] Xinyun Chen, Ryan A. Chi, Xuezhi Wang, and Denny Zhou. * [13] Rongwu Xu, Zehan Qi, and Wei Xu. * [14] Pablo Villalobos, Anson Ho, Jaime Sevilla, Tamay Besiroglu, Lennart Heim, and"
  },
  {
    "chunk_id": "f8b3e367-b791-4829-9a86-9db097e64ec9",
    "section_type": "heading",
    "start": 64823,
    "end": 72177,
    "topic": "分层递归模型HRM实现小样本深度推理",
    "importance": 1.0,
    "complexity": 4,
    "content_preview": "_Deep Learning_. deeplearningbook. Deep residual learning for image recognition. _2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 770-778, 2015. Complexity results for planning. In _Proceedings of the 12th International Joint Conference on Artificial Intelligence - Volume 1_, IJCAI'91, page 274-279, San Francisco, CA, USA, 1991. In _Neural Information Processing Systems_, 2023. _Transactions on Machine Learning Research_, 2025. Beyond a*: Better planning with transformers via search dynamics bootstrapping. In _First Conference on Language Modeling_, 2024. _Transactions of the Association for Computational Linguistics_, 11:531-545, 2023. Chain-of-thought prompting elicits reasoning in large language models, 2022. Premise order matters in reasoning with large l"
  },
  {
    "chunk_id": "0c604b60-dceb-4491-8fe6-a7eb4e715374",
    "section_type": "heading",
    "start": 72178,
    "end": 75561,
    "topic": "分层循环模型HRM实现小样本深度推理",
    "importance": 0.9012914862914863,
    "complexity": 1,
    "content_preview": "MIT Press, 2016. http://www. Morgan Kaufmann Publishers Inc. ISBN 1558601600. _ArXiv_, abs/2406. 09308, 2024. 1162/tacl_a_00562. arXiv preprint arXiv:2201. In _ICLR_, 2024. _ArXiv_, abs/2402. 08939, 2024. _arXiv preprint arXiv:2211. 04325_, 2022. _arXiv preprint arXiv:2412. 07423_, 2024. [2024] Evelina Fedorenko, Steven T Piantadosi, and Edward AF Gibson. _Nature_, 630(8017):575-586, 2024. [2024] Hongyu Wang, Shuming Ma, Li Dong, Shaohan Huang, Dongdong Zhang, and Furu Wei. ISSN 0959-4388. doi: https://doi. [2023] Roxana Zeraati, Yan-Liang Shi, Nicholas A Steinmetz, Marc A Gieselmann, Alexander Thiele, Tirin Moore, Anna Levina, and Tatiana A Engel. _Nature communications_, 14(1):1858, 2023. [2024] Klara Kaleb, Barbara Feulner, Juan Gallego, and Claudia Clopath. arXiv preprint arXiv:1911. ["
  },
  {
    "chunk_id": "0c1140cb-1922-499e-96c5-5892b3b61fc9",
    "section_type": "heading",
    "start": 75562,
    "end": 76159,
    "topic": "分层递归模型HRM的深度推理研究",
    "importance": 0.8801875901875904,
    "complexity": 3,
    "content_preview": "Average-hard attention transformers are constant-depth uniform threshold circuits, 2023. A logic for expressing log-precision transformers. Transformers in DLOGTIME-uniform TC\\({}^{0}\\). Transformers meet neural algorithmic reasoners. The parallelism tradeoff: Limitations of log-precision transformers. The expressive power of transformers with chain of thought. Roformer: Enhanced transformer with rotary position embedding. Glu variants improve transformer. Universal transformers, 2018. Transformers are ssms: Generalized models and efficient algorithms through structured state space duality."
  },
  {
    "chunk_id": "70d70737-2236-42af-8247-3cf72158fe5b",
    "section_type": "heading",
    "start": 76160,
    "end": 76184,
    "topic": "分层循环模型HRM小样本推理",
    "importance": 0.6343795093795094,
    "complexity": 1,
    "content_preview": "Will we run out of data?"
  },
  {
    "chunk_id": "a6393335-267b-43bb-8493-a0c60065fe54",
    "section_type": "heading",
    "start": 76185,
    "end": 76224,
    "topic": "分层递归模型HRM的深度推理创新",
    "importance": 0.5854040404040405,
    "complexity": 1,
    "content_preview": "Zico Kolter. Zico Kolter. Mozer, and M."
  },
  {
    "chunk_id": "be5692bf-b3d5-44ee-9cc5-8d2a97c72583",
    "section_type": "heading",
    "start": 76225,
    "end": 76279,
    "topic": "分层递归模型HRM实现小样本深度推理",
    "importance": 0.5546897546897547,
    "complexity": 1,
    "content_preview": "* [58] JAX Developers. dev/en/latest/_autosummary/jax."
  },
  {
    "chunk_id": "b1b46c1c-797f-4f66-b3b0-dd75c0a5f932",
    "section_type": "heading",
    "start": 76280,
    "end": 76331,
    "topic": "分层递归模型HRM的深度推理创新",
    "importance": 0.6308080808080808,
    "complexity": 1,
    "content_preview": "initializers. initializers. sourceforge. Perreault."
  },
  {
    "chunk_id": "abd2956e-1070-49aa-a90d-6b49f4ff628d",
    "section_type": "heading",
    "start": 76332,
    "end": 76360,
    "topic": "分层递归模型HRM的深度推理",
    "importance": 0.5654040404040405,
    "complexity": 1,
    "content_preview": "lecun_normal_. lecun_normal."
  },
  {
    "chunk_id": "dcb15066-5983-4756-804d-48e73222bf0b",
    "section_type": "heading",
    "start": 76361,
    "end": 76569,
    "topic": "分层递归模型HRM的深度推理",
    "importance": 0.7614430014430016,
    "complexity": 2,
    "content_preview": "Can convolutional neural networks crack sudoku puzzles? com/Kyubyong/sudoku, 2018. Tdoku: A fast sudoku solver and generator. io/tdoku/, 2025. Sudoku-bench: Evaluating creative reasoning with sudoku variants."
  }
]