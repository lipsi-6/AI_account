{
  "timestamp": "2025-08-19T08:24:09.895155+00:00",
  "model": "deepseek-chat",
  "temperature": 0.2,
  "max_tokens": 48,
  "system_prompt": null,
  "use_insight_model": false,
  "messages": [
    {
      "role": "user",
      "content": "你是一位学术编辑。请基于如下论文上下文与片段内容，生成一个高度概括的中文主题短语（8-16字），不含标点，不要解释：\n论文标题: 测试论文\n研究领域: Artificial Intelligence\n关键贡献: Proposal of HRM, a hierarchical recurrent model for deep reasoning tasks, Demonstration of HRM's ability to solve complex tasks with minimal training data (1000 examples), Elimination of dependency on pre-training or CoT techniques\n片段:\nIntroduction Deep learning, as its name suggests, emerged from the idea of stacking more layers to achieve increased representation power and improved performance [1, 2]. However, despite the remarkable success of large language models, their core architecture is paradoxically shallow [3]. This imposes a fundamental constraint on their most sought-after capability: reasoning. The fixed depth of standard Transformers places them in computational complexity classes such as \\(AC^{0}\\) or \\(TC^{0}\\)[4], preventing them from solving problems that require polynomial time [5, 6]. LLMs are not Turing-complete and thus they cannot, at least in a purely end-to-end manner, execute complex algorithmic reasoning that is necessary for deliberate planning or symbolic manipulation tasks [7, 8]. For example, our results on the Sudoku task show that increasing Transformer model depth _can_ improve performance,1 but performance remains far from optimal even with very deep models (see Figure 2), which supports the conjectured limitations of the LLM scaling paradigm [9]. The LLMs literature has relied largely on Chain-of-Thought (CoT) prompting for reasoning [10]. CoT externalizes reasoning into token-"
    }
  ],
  "additional_params": {}
}