{
  "timestamp": "2025-08-19T08:25:00.772500+00:00",
  "model": "deepseek-chat",
  "temperature": 0.2,
  "max_tokens": 48,
  "system_prompt": null,
  "use_insight_model": false,
  "messages": [
    {
      "role": "user",
      "content": "你是一位学术编辑。请基于如下论文上下文与片段内容，生成一个高度概括的中文主题短语（8-16字），不含标点，不要解释：\n论文标题: 测试论文\n研究领域: Artificial Intelligence\n关键贡献: Proposal of HRM, a hierarchical recurrent model for deep reasoning tasks, Demonstration of HRM's ability to solve complex tasks with minimal training data (1000 examples), Elimination of dependency on pre-training or CoT techniques\n片段:\n**(a)** Mean compute steps used by models with ACT versus models with a fixed number of compute steps (\\(M\\)). ACT maintains a low and stable number of average compute steps even as the maximum limit (\\(M_{\\max}\\)) increases. **(b)** Accuracy comparison. The ACT model achieves performance comparable to the fixed-compute model while utilizing substantially fewer computational steps on average. **(c)** Inference-time scalability. Models trained with a specific \\(M_{\\max}\\) can generalize to higher computational limits during inference, leading to improved accuracy. For example, a model trained with \\(M_{\\max}=8\\) continues to see accuracy gains when run with \\(M_{\\max}=16\\) during inference."
    }
  ],
  "additional_params": {}
}