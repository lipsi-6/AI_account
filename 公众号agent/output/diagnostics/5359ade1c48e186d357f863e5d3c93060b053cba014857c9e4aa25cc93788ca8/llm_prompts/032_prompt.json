{
  "timestamp": "2025-08-19T08:26:12.596950+00:00",
  "model": "deepseek-chat",
  "temperature": 0.2,
  "max_tokens": 48,
  "system_prompt": null,
  "use_insight_model": false,
  "messages": [
    {
      "role": "user",
      "content": "你是一位学术编辑。请基于如下论文上下文与片段内容，生成一个高度概括的中文主题短语（8-16字），不含标点，不要解释：\n论文标题: 测试论文\n研究领域: Artificial Intelligence\n关键贡献: Proposal of HRM, a hierarchical recurrent model for deep reasoning tasks, Demonstration of HRM's ability to solve complex tasks with minimal training data (1000 examples), Elimination of dependency on pre-training or CoT techniques\n片段:\nSpaun[90] is one notable example, which uses spiking neural networks to create distinct modules corresponding to brain regions like the visual cortex and prefrontal cortex. Another significant model is the Tolman-Eichenbaum Machine (TEM)[91], which is inspired by the hippocampal-entorhinal system's role in spatial and relational memory tasks. TEM proposes that medial entorhinal cells create a basis for structural knowledge, while hippocampal cells link this basis to sensory information. Another approach involves neural sampling models [92], which view the neural signaling process as inference over a distribution, functioning similarly to a Boltzmann machine. **Hierarchical memory**: The hierarchical multi-timescale structure also plays an important role in how the brain processes memory. Models such as Hierarchical Sequential Models [93] and Clockwork RNN [94] use multiple recurrent modules that operate at varying time scales to more effectively capture long-range dependencies within sequences, thereby mitigating the forgetting issue in RNNs. Similar mechanisms have also been adopted in linear attention methods for memorizing long contexts (see the Discussions section)."
    }
  ],
  "additional_params": {}
}