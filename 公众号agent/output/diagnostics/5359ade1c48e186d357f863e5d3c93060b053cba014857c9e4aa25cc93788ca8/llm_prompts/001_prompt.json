{
  "timestamp": "2025-08-19T08:23:08.366438+00:00",
  "model": "deepseek-chat",
  "temperature": 0.3,
  "max_tokens": null,
  "system_prompt": null,
  "use_insight_model": true,
  "messages": [
    {
      "role": "user",
      "content": "您是一位世界顶级的学术研究专家，具有深厚的跨学科知识背景。请对以下学术论文进行全局深度分析，并严格按照指定 JSON 架构输出。\n\n论文信息：\n- 标题: 测试论文\n- 作者: \n\n论文内容：\n# Hierarchical Reasoning Model\n\n Guan Wang\\({}^{1,\\dagger}\\), Jin Li\\({}^{1}\\), Yuhao Sun\\({}^{1}\\), Xing Chen\\({}^{1}\\), Changling Liu\\({}^{1}\\),\n\nYue Wu\\({}^{1}\\), Meng Lu\\({}^{1,\\dagger}\\), Sen Song\\({}^{2,\\dagger}\\), Yasin Abbasi Yadkori\\({}^{1,\\dagger}\\)\n\n\\({}^{1}\\)Sapient Intelligence, Singapore\n\n###### Abstract\n\nReasoning, the process of devising and executing complex goal-oriented action sequences, remains a critical challenge in AI. Current large language models (LLMs) primarily employ Chain-of-Thought (CoT) techniques, which suffer from brittle task decomposition, extensive data requirements, and high latency. Inspired by the hierarchical and multi-timescale processing in the human brain, we propose the Hierarchical Reasoning Model (HRM), a novel recurrent architecture that attains significant computational depth while maintaining both training stability and efficiency. HRM executes sequential reasoning tasks in a single forward pass without explicit supervision of the intermediate process, through two interdependent recurrent modules: a high-level module responsible for slow, abstract planning, and a low-level module handling rapid, detailed computations. With only 27 million parameters, HRM achieves exceptional performance on complex reasoning tasks using only 1000 training samples. The model operates without pre-training or CoT data, yet achieves nearly perfect performance on challenging tasks including complex Sudoku puzzles and optimal path finding in large mazes. Furthermore, HRM outperforms much larger models with significantly longer context windows on the Abstraction and Reasoning Corpus (ARC), a key benchmark for measuring artificial general intelligence capabilities. These results underscore HRM's potential as a transformative advancement toward universal computation and general-purpose reasoning systems.\n\n\n\nFigure 1: **Left:** HRM is inspired by hierarchical processing and temporal separation in the brain. It has two recurrent networks operating at different timescales to collaboratively solve tasks. **Right:** With only about 1000 training examples, the HRM (\\(\\sim\\)27M parameters) surpasses state-of-the-art CoT models on inductive benchmarks (ARC-AGI) and challenging symbolic tree-search puzzles (_Sudoku-Extreme_, _Maze-Hard_) where CoT models failed completely. The HRM was randomly initialized, and it solved the tasks directly from inputs without chain of thoughts.\n\nIntroduction\n\nDeep learning, as its name suggests, emerged from the idea of stacking more layers to achieve increased representation power and improved performance [1, 2]. However, despite the remarkable success of large language models, their core architecture is paradoxically shallow [3]. This imposes a fundamental constraint on their most sought-after capability: reasoning. The fixed depth of standard Transformers places them in computational complexity classes such as \\(AC^{0}\\) or \\(TC^{0}\\)[4], preventing them from solving problems that require polynomial time [5, 6]. LLMs are not Turing-complete and thus they cannot, at least in a purely end-to-end manner, execute complex algorithmic reasoning that is necessary for deliberate planning or symbolic manipulation tasks [7, 8]. For example, our results on the Sudoku task show that increasing Transformer model depth _can_ improve performance,1 but performance remains far from optimal even with very deep models (see Figure 2), which supports the conjectured limitations of the LLM scaling paradigm [9].\n\nFootnote 1: Simply increasing the model width does not improve performance here.\n\nThe LLMs literature has relied largely on Chain-of-Thought (CoT) prompting for reasoning [10]. CoT externalizes reasoning into token-level language by breaking down complex tasks into simpler intermediate steps, sequentially generating text using a shallow model [11]. However, CoT for reasoning is a crutch, not a satisfactory solution. It relies on brittle, human-defined decompositions where a single misstep or a misorder of the steps can derail the reasoning process entirely [12, 13]. This dependency on explicit linguistic steps tethers reasoning to patterns at the token level. As a result, CoT reasoning often requires significant amount of training data and generates a large number of tokens for complex reasoning tasks, resulting in slow response times. A more efficient approach is needed to minimize these data requirements [14].\n\nTowards this goal, we explore \"latent reasoning\", where the model conducts computations within its internal hidden state space [15, 16]. This aligns with the understanding that language is a tool for human communication, not the substrate of thought itself [17]; the brain sustains lengthy, coherent chains of reasoning with remarkable efficiency in a latent space, without constant translation back to language. However, the power of latent reasoning is still fundamentally constrained by a model's _effective computational depth_. Naively stacking layers is notoriously difficult due to vanishing gradients, which plague training stability and effectiveness [1, 18]. Recurrent architectures, a natural alternative for sequential tasks, often suffer from early convergence, rendering subsequent computational steps inert, and rely on the biologically implausible, computationally expensive and memory intensive Backpropagation Through Time (BPTT) for training [19].\n\nThe human brain provides a compelling blueprint for achieving the effective computational depth that contemporary artificial models lack. It organizes computation hierarchically across cortical regions operating at different timescales, enabling deep, multi-stage reasoning [20, 21, 22]. Recurrent feedback loops iteratively refine internal representations, allowing slow, higher-level areas to guide, and fast, lower-level circuits to execute--subordinate processing while preserving global coherence [23, 24, 25]. Notably, the brain achieves such depth without incurring the prohibitive credit-assignment costs that typically hamper recurrent networks from backpropagation through time [19, 26].\n\nInspired by this hierarchical and multi-timescale biological architecture, we propose the Hierarchical Reasoning Model (HRM). HRM is designed to significantly increase the effective computational depth. It features two coupled recurrent modules: a high-level (H) module for abstract, deliberate reasoning, and a low-level (L) module for fast, detailed computations. This structure\n\navoids the rapid convergence of standard recurrent models through a process we term \"hierarchical convergence.\" The slow-updating H-module advances only after the fast-updating L-module has completed multiple computational steps and reached a local equilibrium, at which point the L-module is reset to begin a new computational phase.\n\nFurthermore, we propose a one-step gradient approximation for training HRM, which offers improved efficiency and eliminates the requirement for BPTT. This design maintains a constant memory footprint (\\(O(1)\\) compared to BPTT's \\(O(T)\\) for \\(T\\) timesteps) throughout the backpropagation process, making it scalable and more biologically plausible.\n\nLeveraging its enhanced effective depth, HRM excels at tasks that demand extensive search and backtracking. **Using only 1,000 input-output examples, without pre-training or CoT supervision,** HRM learns to solve problems that are intractable for even the most advanced LLMs. For example, it achieves near-perfect accuracy in complex Sudoku puzzles (_Sudoku-Extreme Full_) and optimal pathfinding in 30x30 mazes, where state-of-the-art CoT methods completely fail (0% accuracy). In the Abstraction and Reasoning Corpus (ARC) AGI Challenge [27, 28, 29] - a benchmark of inductive reasoning - HRM, trained from scratch with only the official dataset (~1000 examples), with only 27M parameters and a 30x30 grid context (900 tokens), achieves a performance of **40.3%**, which substantially surpasses leading CoT-based models like o3-min\n\n输出要求（极其重要）：\n- 严格输出 UTF-8 单个 JSON 对象（无额外文字、无 markdown 代码块），键名与类型必须完全匹配下述架构；字符串请避免换行过多。\n- 架构键名（全部必填）：\n  {\n    \"abstract_summary\": string,                      // 摘要层面的全局总结\n    \"research_domain\": string,                       // 研究领域（如：Natural Language Processing）\n    \"key_contributions\": string[],                   // 主要学术贡献（3-8 条）\n    \"methodology_overview\": string,                  // 方法论/技术路线概述\n    \"main_findings\": string[],                       // 关键实验或理论发现（3-8 条）\n    \"limitations\": string[],                         // 局限性（2-6 条）\n    \"future_directions\": string[],                   // 未来工作方向（2-6 条）\n    \"technical_complexity\": number,                  // 技术复杂度评分 ∈ [0,1]\n    \"novelty_score\": number,                         // 新颖性评分 ∈ [0,1]\n    \"practical_relevance\": number,                   // 实用相关性评分 ∈ [0,1]\n    \"related_concepts\": string[],                    // 相关概念/术语（5-15 个，短语）\n    \"terminology_glossary\": {                        // 术语词汇表：术语 -> 简要定义\n      \"<term>\": \"<definition>\",\n      \"...\": \"...\"\n    },\n    \"mathematical_concepts\": string[]                // 涉及的数学概念/对象（如：KL divergence, convexity, spectral norm）\n  }\n\n请直接输出满足上述键名与类型约束的 JSON 对象。"
    }
  ],
  "additional_params": {}
}