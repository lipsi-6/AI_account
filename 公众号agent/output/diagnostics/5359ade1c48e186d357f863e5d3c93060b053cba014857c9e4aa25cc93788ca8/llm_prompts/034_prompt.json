{
  "timestamp": "2025-08-19T08:26:20.602885+00:00",
  "model": "deepseek-chat",
  "temperature": 0.2,
  "max_tokens": 48,
  "system_prompt": null,
  "use_insight_model": false,
  "messages": [
    {
      "role": "user",
      "content": "你是一位学术编辑。请基于如下论文上下文与片段内容，生成一个高度概括的中文主题短语（8-16字），不含标点，不要解释：\n论文标题: 测试论文\n研究领域: Artificial Intelligence\n关键贡献: Proposal of HRM, a hierarchical recurrent model for deep reasoning tasks, Demonstration of HRM's ability to solve complex tasks with minimal training data (1000 examples), Elimination of dependency on pre-training or CoT techniques\n片段:\n## 6 Discussions **Turing-completeness of HRM**: Like earlier neural reasoning algorithms including the Universal Transformer [95], HRM is computationally universal when given sufficient memory and time constraints. Given that earlier neural algorithm reasoners were trained as recurrent neural networks, they suffer from premature convergence and memory intensive BPTT. By resolving these two challenges and being equipped with adaptive computation, HRM could be trained on long reasoning processes, solve complex puzzles requiring intensive depth-first search and backtracking, and move closer to practical Turing-completeness. **Reinforcement learning with chain-of-thought**: Beyond fine-tuning using human-annotated CoT, reinforcement learning (RL) represents another widely adopted training methodology. However, recent evidence suggests that RL primarily unlocks existing CoT-like capabilities rather than discovering fundamentally new reasoning mechanisms [96, 97, 98, 99]. Additionally, CoT-training with RL is known for its instability and data inefficiency, often requiring extensive exploration and careful reward design. In contrast, HRM takes feedback from dense gradient-based supervis"
    }
  ],
  "additional_params": {}
}