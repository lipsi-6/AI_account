#!/usr/bin/env python3
"""
ç®€åŒ–çš„PDFæµ‹è¯•è„šæœ¬
ç›´æ¥æµ‹è¯•PDFå¤„ç†å’Œåˆ†æåŠŸèƒ½ï¼Œä¸æ¶‰åŠè®ºæ–‡å‘ç°
"""

import asyncio
import sys
import os
import hashlib
import json
from datetime import datetime, timezone
from pathlib import Path

# æ·»åŠ é¡¹ç›®è·¯å¾„åˆ°sys.path
sys.path.insert(0, str(Path(__file__).parent))

from modules.config_manager import ConfigManager
from modules.logger import setup_logging, get_logger
from modules.llm_provider import LLMProviderService
from modules.embedding_provider import EmbeddingProviderService
from modules.memory_system import HybridMemorySystem
from modules.ingestion_pipeline import IngestionPipeline
from modules.analysis_core import DeepAnalysisCore, AnalysisRequest
from modules.synthesis_engine import SynthesisEngine
import aiofiles
import shutil
from dataclasses import asdict

async def test_pdf_processing():
    """æµ‹è¯•PDFå¤„ç†åŠŸèƒ½"""
    print("ğŸ§ª å¼€å§‹æµ‹è¯•PDFå¤„ç†åŠŸèƒ½...")
    
    # æ£€æŸ¥æµ‹è¯•PDFæ˜¯å¦å­˜åœ¨
    pdf_path = Path("test_simple.pdf")
    if not pdf_path.exists():
        print("âŒ æµ‹è¯•PDFæ–‡ä»¶ä¸å­˜åœ¨ï¼")
        return False
    
    print(f"âœ“ æ‰¾åˆ°æµ‹è¯•PDF: {pdf_path}")
    
    try:
        # 1. åˆå§‹åŒ–é…ç½®ç®¡ç†å™¨
        config = ConfigManager()
        await config.initialize("config.yaml")
        print("âœ“ é…ç½®ç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆ")
        
        # 2. è®¾ç½®æ—¥å¿—ç³»ç»Ÿ
        await setup_logging(config)
        logger = get_logger("pdf_test")
        print("âœ“ æ—¥å¿—ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
        
        # 3. åˆå§‹åŒ–LLMæä¾›å•†æœåŠ¡
        llm_provider = LLMProviderService(config)
        print("âœ“ LLMæœåŠ¡åˆå§‹åŒ–å®Œæˆ")
        
        # 4. åˆå§‹åŒ–åµŒå…¥æä¾›å•†æœåŠ¡
        embedding_provider = EmbeddingProviderService(config)
        print("âœ“ åµŒå…¥æœåŠ¡åˆå§‹åŒ–å®Œæˆ")
        
        # 5. åˆå§‹åŒ–è®°å¿†ç³»ç»Ÿ
        memory_system = HybridMemorySystem(config, embedding_provider)
        await memory_system.initialize()
        print("âœ“ è®°å¿†ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
        
        # 6. åˆå§‹åŒ–æ¶ˆåŒ–ç®¡é“
        ingestion_pipeline = IngestionPipeline()
        await ingestion_pipeline.initialize(config)
        print("âœ“ æ¶ˆåŒ–ç®¡é“åˆå§‹åŒ–å®Œæˆ")
        
        # 7. åˆå§‹åŒ–åˆ†ææ ¸å¿ƒ
        analysis_core = DeepAnalysisCore(
            config,
            llm_provider,
            memory_system,
            embedding_provider,
        )
        await analysis_core.initialize()
        print("âœ“ åˆ†ææ ¸å¿ƒåˆå§‹åŒ–å®Œæˆ")
        
        # 8. åˆå§‹åŒ–åˆæˆå¼•æ“
        synthesis_engine = SynthesisEngine(config, llm_provider, memory_system)
        await synthesis_engine.initialize()
        print("âœ“ åˆæˆå¼•æ“åˆå§‹åŒ–å®Œæˆ")
        
        print("\nğŸ“„ å¼€å§‹å¤„ç†PDF...")
        
        # å¤„ç†PDF
        processed_paper = await ingestion_pipeline.process_paper(str(pdf_path.absolute()))
        
        if not processed_paper:
            print("âŒ PDFå¤„ç†å¤±è´¥")
            return False
            
        print(f"âœ“ PDFè§£æå®Œæˆï¼Œæå– {len(processed_paper.sections)} ä¸ªç« èŠ‚")

        # ====== ä¸­é—´ç»“æœæŒä¹…åŒ–ï¼ˆæŒ‰PDFå“ˆå¸Œå‘½åï¼‰======
        file_hash = (
            getattr(getattr(processed_paper, 'metadata', None), 'file_hash', None)
            or hashlib.sha256(pdf_path.read_bytes()).hexdigest()
        )
        diag_root = Path("output") / "diagnostics" / file_hash
        diag_root.mkdir(parents=True, exist_ok=True)

        async def save_text(rel: str, text: str) -> None:
            p = diag_root / rel
            async with aiofiles.open(p, 'w', encoding='utf-8') as f:
                await f.write(text or "")
            print(f"   â†³ å†™å…¥ {p}")

        async def save_json(rel: str, obj) -> None:
            p = diag_root / rel
            async with aiofiles.open(p, 'w', encoding='utf-8') as f:
                await f.write(json.dumps(obj, ensure_ascii=False, indent=2))
            print(f"   â†³ å†™å…¥ {p}")

        # æ‘„å–é˜¶æ®µç»“æœ
        print("ğŸ’¾ ä¿å­˜æ‘„å–é˜¶æ®µï¼ˆingestionï¼‰ä¸­é—´ç»“æœ...")
        await save_text("ingestion_full_text.md", processed_paper.full_text)
        await save_json(
            "ingestion_sections.json",
            [
                {
                    "section_id": s.section_id,
                    "title": s.title,
                    "level": s.level,
                    "content_preview": (s.content or "")[:800]
                }
                for s in (processed_paper.sections or [])
            ],
        )
        await save_json(
            "ingestion_images.json",
            [
                {
                    "image_id": i.image_id,
                    "page": i.page_number,
                    "bbox": i.bbox,
                    "path": i.image_path,
                    "size": i.file_size,
                }
                for i in (processed_paper.images or [])
            ],
        )
        await save_json("ingestion_stats.json", processed_paper.processing_stats)
        await save_json(
            "ingestion_parse_results.json",
            [
                {
                    "parser": r.parser_name,
                    "success": r.success,
                    "quality": getattr(getattr(r, 'quality', None), 'value', str(getattr(r, 'quality', None))),
                    "content_length": len(r.content or ""),
                    "sections": len(r.sections or []),
                    "confidence": r.confidence_score,
                    "error": r.error_message,
                }
                for r in (processed_paper.parse_results or [])
            ],
        )

        # è®°å½• LLM çš„è¾“å…¥æç¤ºè¯ï¼ˆçŒ´å­è¡¥ä¸ llm_provider.generateï¼‰
        print("ğŸ§¾ å¯ç”¨æç¤ºè¯è®°å½•å™¨ï¼šå°†ä¿å­˜æ‰€æœ‰ LLM è¾“å…¥æç¤ºåˆ° diagnostics ç›®å½•")
        llm_prompts_dir = diag_root / "llm_prompts"
        llm_prompts_dir.mkdir(exist_ok=True)
        _llm_call_counter = {"n": 0}
        _original_generate = llm_provider.generate

        async def _wrapped_generate(
            messages,
            model=None,
            temperature=None,
            max_tokens=None,
            system_prompt=None,
            use_insight_model=False,
            **kwargs,
        ):
            try:
                _llm_call_counter["n"] += 1
                idx = _llm_call_counter["n"]
                payload = {
                    "timestamp": datetime.now(timezone.utc).isoformat(),
                    "model": model
                    or (
                        getattr(config.llm_settings, "insight_model", None)
                        if use_insight_model
                        else getattr(config.llm_settings, "primary_model", None)
                    ),
                    "temperature": temperature,
                    "max_tokens": max_tokens,
                    "system_prompt": system_prompt,
                    "use_insight_model": use_insight_model,
                    "messages": messages,
                    "additional_params": kwargs,
                }
                async with aiofiles.open(
                    llm_prompts_dir / f"{idx:03d}_prompt.json", "w", encoding="utf-8"
                ) as f:
                    await f.write(json.dumps(payload, ensure_ascii=False, indent=2))
            except Exception as _e:
                try:
                    print(f"âš ï¸ LLM æç¤ºè¯è®°å½•å¤±è´¥: {_e}")
                except Exception:
                    pass
            return await _original_generate(
                messages=messages,
                model=model,
                temperature=temperature,
                max_tokens=max_tokens,
                system_prompt=system_prompt,
                use_insight_model=use_insight_model,
                **kwargs,
            )

        llm_provider.generate = _wrapped_generate
        
        # è¿›è¡Œæ·±åº¦åˆ†æ
        print("ğŸ” å¼€å§‹æ·±åº¦åˆ†æ...")
        request = AnalysisRequest(
            paper_content=processed_paper.full_text,
            paper_title=processed_paper.metadata.title or "æµ‹è¯•è®ºæ–‡",
            paper_authors=processed_paper.metadata.authors or [],
            analysis_depth=2,  # é™ä½åˆ†ææ·±åº¦ä»¥åŠ å¿«æµ‹è¯•
            include_mathematical_details=True,
            include_historical_context=False,  # ç®€åŒ–åˆ†æ
            custom_focus_areas=None,
            output_language="zh-CN",
        )
        analysis_result = await analysis_core.analyze_paper(request)
        print(f"âœ“ åˆ†æå®Œæˆï¼Œç”Ÿæˆ {len(analysis_result.chunk_explanations)} ä¸ªæ„ç¾¤è§£é‡Š")

        # åˆ†æé˜¶æ®µä¸­é—´ç»“æœ
        print("ğŸ’¾ ä¿å­˜åˆ†æé˜¶æ®µï¼ˆanalysisï¼‰ä¸­é—´ç»“æœ...")
        gm = analysis_result.global_map
        gm_payload = {
            "paper_id": gm.paper_id,
            "title": gm.title,
            "authors": gm.authors,
            "abstract_summary": gm.abstract_summary,
            "research_domain": gm.research_domain,
            "key_contributions": gm.key_contributions,
            "methodology_overview": gm.methodology_overview,
            "main_findings": gm.main_findings,
            "limitations": gm.limitations,
            "future_directions": gm.future_directions,
            "technical_complexity": gm.technical_complexity,
            "novelty_score": gm.novelty_score,
            "practical_relevance": gm.practical_relevance,
            "related_concepts": gm.related_concepts,
            "terminology_glossary": gm.terminology_glossary,
            "mathematical_concepts": gm.mathematical_concepts,
            "created_at": gm.created_at.isoformat() if getattr(gm, 'created_at', None) else None,
        }
        await save_json("analysis_global_map.json", gm_payload)
        await save_json(
            "analysis_chunks.json",
            [
                {
                    "chunk_id": c.chunk_id,
                    "section_type": c.section_type,
                    "start": c.start_position,
                    "end": c.end_position,
                    "topic": c.semantic_topic,
                    "importance": c.importance_score,
                    "complexity": c.complexity_level,
                    "content_preview": (c.content or "")[:800],
                }
                for c in (analysis_result.semantic_chunks or [])
            ],
        )
        await save_json(
            "analysis_chunk_explanations.json",
            [
                {
                    "chunk_id": e.chunk_id,
                    "confidence": e.confidence_score,
                    "key_insights": e.key_insights,
                    "terminology": e.terminology_explained,
                    "math": e.mathematical_explanations,
                    "llm_model": e.llm_model_used,
                    "explanation": e.explanation,
                }
                for e in (analysis_result.chunk_explanations or [])
            ],
        )
        await save_text("analysis_overall_summary.md", analysis_result.overall_summary or "")
        await save_json("analysis_research_insights.json", analysis_result.research_insights)
        await save_json("analysis_practical_implications.json", analysis_result.practical_implications)
        await save_json("analysis_quality_metrics.json", analysis_result.quality_metrics)
        
        # ç”Ÿæˆæœ€ç»ˆæ–‡ç« 
        print("ğŸ“ å¼€å§‹åˆæˆæ–‡ç« ...")
        from modules.synthesis_engine import (
            AnalysisPair,
            GlobalAnalysisMap,
            PaperMetadata,
            FigureReference,
        )
        
        # æ„å»ºåˆ†æå¯¹
        chunk_map = {c.chunk_id: c for c in analysis_result.semantic_chunks}
        analysis_pairs = []
        for exp in analysis_result.chunk_explanations:
            chunk = chunk_map.get(exp.chunk_id)
            if chunk:
                pair = AnalysisPair(
                    original=chunk.content,
                    analysis=exp.explanation,
                    section_id=chunk.section_type,
                    confidence_score=exp.confidence_score,
                )
                analysis_pairs.append(pair)
        
        # æ„å»ºå…¨å±€åˆ†ææ˜ å°„
        global_map_src = analysis_result.global_map
        global_analysis = GlobalAnalysisMap(
            summary=analysis_result.overall_summary,
            key_insights=analysis_result.research_insights[:5],
            technical_terms=global_map_src.terminology_glossary,
            methodology_analysis=global_map_src.methodology_overview,
            significance_assessment=", ".join(analysis_result.practical_implications[:3])
        )
        
        # æ„å»ºè®ºæ–‡å…ƒæ•°æ®
        paper_metadata = PaperMetadata(
            title=processed_paper.metadata.title or "æµ‹è¯•è®ºæ–‡",
            authors=processed_paper.metadata.authors or [],
            institutions=None,
            publication_date=None,
            arxiv_id=None,
            doi=None,
            abstract=processed_paper.abstract,
        )
        
        # åˆæˆé˜¶æ®µè¾“å…¥ä¿å­˜
        print("ğŸ’¾ ä¿å­˜åˆæˆé˜¶æ®µï¼ˆsynthesisï¼‰è¾“å…¥...")
        await save_json(
            "synthesis_analysis_pairs.json",
            [
                {
                    "section_id": p.section_id,
                    "confidence": p.confidence_score,
                    "original_preview": (p.original or "")[:600],
                    "analysis_preview": (p.analysis or "")[:600],
                }
                for p in analysis_pairs
            ],
        )
        await save_json("synthesis_global_analysis.json", asdict(global_analysis))
        await save_json("synthesis_paper_metadata.json", asdict(paper_metadata))

        # åˆæˆæ–‡ç« 
        article = await synthesis_engine.synthesize_article(
            analysis_pairs=analysis_pairs,
            global_analysis=global_analysis,
            paper_metadata=paper_metadata,
            figures=[],
        )
        
        print(f"âœ… æµ‹è¯•å®Œæˆï¼æ–‡ç« å·²ç”Ÿæˆ: {article.title}")
        if hasattr(article, 'file_path') and article.file_path:
            print(f"ğŸ“„ è¾“å‡ºæ–‡ä»¶: {article.file_path}")
            # å¤åˆ¶ä¸€ä»½åˆ°è¯Šæ–­ç›®å½•ï¼Œä¾¿äºå•ç‚¹æŸ¥çœ‹
            try:
                target_md = diag_root / "final_article.md"
                src = Path(article.file_path)
                if src.exists():
                    async with aiofiles.open(src, 'r', encoding='utf-8') as rf:
                        content = await rf.read()
                    await save_text("final_article.md", content)
                # è®°å½•åŸå§‹è·¯å¾„
                await save_text("final_article_path.txt", article.file_path)
            except Exception as _e:
                logger.warning(f"å¤åˆ¶æœ€ç»ˆæ–‡ç« åˆ°è¯Šæ–­ç›®å½•å¤±è´¥: {_e}")

        # æ¸…ç† data/temp_downloads ä¸­çš„ä¸´æ—¶æ–‡ä»¶ï¼ˆæœ¬æ¬¡æµ‹è¯•ä¸å†éœ€è¦ï¼‰
        try:
            temp_dir = Path(config.paths.temp_downloads_folder)
            if temp_dir.exists():
                for item in temp_dir.iterdir():
                    try:
                        if item.is_file():
                            item.unlink()
                        else:
                            shutil.rmtree(item, ignore_errors=True)
                    except Exception as _e:
                        logger.warning(f"æ¸…ç†ä¸´æ—¶æ–‡ä»¶å¤±è´¥: {item} -> {_e}")
                print("âœ“ å·²æ¸…ç† data/temp_downloads ç›®å½•")
        except Exception as _e:
            print(f"âš ï¸ æ¸…ç†ä¸´æ—¶ç›®å½•å¤±è´¥: {_e}")

        # æ¸…ç†è€æ—§æ—¥å¿—ï¼ˆä»…ä¿ç•™å½“å‰ä¸»æ—¥å¿—æ–‡ä»¶ï¼‰
        try:
            log_file = Path(config.logging.file_path)
            logs_dir = log_file.parent
            if logs_dir.exists():
                for lf in logs_dir.glob("*.log.*"):
                    try:
                        lf.unlink()
                    except Exception:
                        pass
            print(f"ğŸ—‘ï¸ å·²æ¸…ç†æ—§æ—¥å¿—ï¼Œå½“å‰æ—¥å¿—: {log_file}")
        except Exception as _e:
            print(f"âš ï¸ æ—¥å¿—æ¸…ç†å¤±è´¥: {_e}")
        
        return True
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    # æ£€æŸ¥ç¯å¢ƒå˜é‡
    if not os.getenv("OPENAI_API_KEY") and not os.getenv("DEEPSEEK_API_KEY"):
        print("âš ï¸ è­¦å‘Š: æœªè®¾ç½®APIå¯†é’¥ç¯å¢ƒå˜é‡ (OPENAI_API_KEY æˆ– DEEPSEEK_API_KEY)")
        print("è¯·è®¾ç½®ä½ çš„DeepSeek APIå¯†é’¥:")
        print("export OPENAI_API_KEY='ä½ çš„deepseek_api_key'")
        
    # è¿è¡Œæµ‹è¯•
    success = asyncio.run(test_pdf_processing())
    
    if success:
        print("\nğŸ‰ PDFå¤„ç†æµ‹è¯•æˆåŠŸï¼")
        sys.exit(0)
    else:
        print("\nğŸ’¥ PDFå¤„ç†æµ‹è¯•å¤±è´¥ï¼")
        sys.exit(1)
