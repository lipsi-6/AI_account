{"paper_id": "420c43a510576ebeecd5aca722620763", "source_url": "/workspaces/AI_account/公众号agent/test.pdf", "metadata": {"title": null, "authors": [], "abstract": null, "keywords": [], "doi": null, "arxiv_id": null, "publication_date": null, "journal": null, "conference": null, "pages": null, "volume": null, "issue": null, "url": "/workspaces/AI_account/公众号agent/test.pdf", "file_hash": "5359ade1c48e186d357f863e5d3c93060b053cba014857c9e4aa25cc93788ca8", "file_size": 0, "page_count": 0, "language": "en", "creation_date": null, "modification_date": null, "producer": null, "creator": null}, "sections": [{"section_id": "section_0", "title": "Hierarchical Reasoning Model", "content": "Guan Wang\\({}^{1,\\dagger}\\), Jin Li\\({}^{1}\\), Yuhao Sun\\({}^{1}\\), Xing Chen\\({}^{1}\\), Changling Liu\\({}^{1}\\),\nYue Wu\\({}^{1}\\), Meng Lu\\({}^{1,\\dagger}\\), Sen Song\\({}^{2,\\dagger}\\), Yasin Abbasi Yadkori\\({}^{1,\\dagger}\\)\n\\({}^{1}\\)Sapient Intelligence, Singapore", "level": 1, "page_numbers": [], "subsections": [], "references": [], "equations": [], "images": [], "tables": []}, {"section_id": "section_1", "title": "Abstract", "content": "Reasoning, the process of devising and executing complex goal-oriented action sequences, remains a critical challenge in AI. Current large language models (LLMs) primarily employ Chain-of-Thought (CoT) techniques, which suffer from brittle task decomposition, extensive data requirements, and high latency. Inspired by the hierarchical and multi-timescale processing in the human brain, we propose the Hierarchical Reasoning Model (HRM), a novel recurrent architecture that attains significant computational depth while maintaining both training stability and efficiency. HRM executes sequential reasoning tasks in a single forward pass without explicit supervision of the intermediate process, through two interdependent recurrent modules: a high-level module responsible for slow, abstract planning, and a low-level module handling rapid, detailed computations. With only 27 million parameters, HRM achieves exceptional performance on complex reasoning tasks using only 1000 training samples. The model operates without pre-training or CoT data, yet achieves nearly perfect performance on challenging tasks including complex Sudoku puzzles and optimal path finding in large mazes. Furthermore, HRM outperforms much larger models with significantly longer context windows on the Abstraction and Reasoning Corpus (ARC), a key benchmark for measuring artificial general intelligence capabilities. These results underscore HRM's potential as a transformative advancement toward universal computation and general-purpose reasoning systems.\nFigure 1: **Left:** HRM is inspired by hierarchical processing and temporal separation in the brain. It has two recurrent networks operating at different timescales to collaboratively solve tasks. **Right:** With only about 1000 training examples, the HRM (\\(\\sim\\)27M parameters) surpasses state-of-the-art CoT models on inductive benchmarks (ARC-AGI) and challenging symbolic tree-search puzzles (_Sudoku-Extreme_, _Maze-Hard_) where CoT models failed completely. The HRM was randomly initialized, and it solved the tasks directly from inputs without chain of thoughts.\nIntroduction\nDeep learning, as its name suggests, emerged from the idea of stacking more layers to achieve increased representation power and improved performance [1, 2]. However, despite the remarkable success of large language models, their core architecture is paradoxically shallow [3]. This imposes a fundamental constraint on their most sought-after capability: reasoning. The fixed depth of standard Transformers places them in computational complexity classes such as \\(AC^{0}\\) or \\(TC^{0}\\)[4], preventing them from solving problems that require polynomial time [5, 6]. LLMs are not Turing-complete and thus they cannot, at least in a purely end-to-end manner, execute complex algorithmic reasoning that is necessary for deliberate planning or symbolic manipulation tasks [7, 8]. For example, our results on the Sudoku task show that increasing Transformer model depth _can_ improve performance,1 but performance remains far from optimal even with very deep models (see Figure 2), which supports the conjectured limitations of the LLM scaling paradigm [9].\nFootnote 1: Simply increasing the model width does not improve performance here.\nThe LLMs literature has relied largely on Chain-of-Thought (CoT) prompting for reasoning [10]. CoT externalizes reasoning into token-level language by breaking down complex tasks into simpler intermediate steps, sequentially generating text using a shallow model [11]. However, CoT for reasoning is a crutch, not a satisfactory solution. It relies on brittle, human-defined decompositions where a single misstep or a misorder of the steps can derail the reasoning process entirely [12, 13]. This dependency on explicit linguistic steps tethers reasoning to patterns at the token level. As a result, CoT reasoning often requires significant amount of training data and generates a large number of tokens for complex reasoning tasks, resulting in slow response times. A more efficient approach is needed to minimize these data requirements [14].\nTowards this goal, we explore \"latent reasoning\", where the model conducts computations within its internal hidden state space [15, 16]. This aligns with the understanding that language is a tool for human communication, not the substrate of thought itself [17]; the brain sustains lengthy, coherent chains of reasoning with remarkable efficiency in a latent space, without constant translation back to language. However, the power of latent reasoning is still fundamentally constrained by a model's _effective computational depth_. Naively stacking layers is notoriously difficult due to vanishing gradients, which plague training stability and effectiveness [1, 18]. Recurrent architectures, a natural alternative for sequential tasks, often suffer from early convergence, rendering subsequent computational steps inert, and rely on the biologically implausible, computationally expensive and memory intensive Backpropagation Through Time (BPTT) for training [19].\nThe human brain provides a compelling blueprint for achieving the effective computational depth that contemporary artificial models lack. It organizes computation hierarchically across cortical regions operating at different timescales, enabling deep, multi-stage reasoning [20, 21, 22]. Recurrent feedback loops iteratively refine internal representations, allowing slow, higher-level areas to guide, and fast, lower-level circuits to execute--subordinate processing while preserving global coherence [23, 24, 25]. Notably, the brain achieves such depth without incurring the prohibitive credit-assignment costs that typically hamper recurrent networks from backpropagation through time [19, 26].\nInspired by this hierarchical and multi-timescale biological architecture, we propose the Hierarchical Reasoning Model (HRM). HRM is designed to significantly increase the effective computational depth. It features two coupled recurrent modules: a high-level (H) module for abstract, deliberate reasoning, and a low-level (L) module for fast, detailed computations. This structure\navoids the rapid convergence of standard recurrent models through a process we term \"hierarchical convergence.\" The slow-updating H-module advances only after the fast-updating L-module has completed multiple computational steps and reached a local equilibrium, at which point the L-module is reset to begin a new computational phase.\nFurthermore, we propose a one-step gradient approximation for training HRM, which offers improved efficiency and eliminates the requirement for BPTT. This design maintains a constant memory footprint (\\(O(1)\\) compared to BPTT's \\(O(T)\\) for \\(T\\) timesteps) throughout the backpropagation process, making it scalable and more biologically plausible.\nLeveraging its enhanced effective depth, HRM excels at tasks that demand extensive search and backtracking. **Using only 1,000 input-output examples, without pre-training or CoT supervision,** HRM learns to solve problems that are intractable for even the most advanced LLMs. For example, it achieves near-perfect accuracy in complex Sudoku puzzles (_Sudoku-Extreme Full_) and optimal pathfinding in 30x30 mazes, where state-of-the-art CoT methods completely fail (0% accuracy). In the Abstraction and Reasoning Corpus (ARC) AGI Challenge [27, 28, 29] - a benchmark of inductive reasoning - HRM, trained from scratch with only the official dataset (~1000 examples), with only 27M parameters and a 30x30 grid context (900 tokens), achieves a performance of **40.3%**, which substantially surpasses leading CoT-based models like o3-mini-high (34.5%) and Claude 3.7 8K context (21.2%), despite their considerably larger parameter sizes and context lengths, as shown in Figure 1. This represents a promising direction toward the development of next-generation AI reasoning systems with universal computational capabilities.", "level": 1, "page_numbers": [], "subsections": [], "references": [], "equations": [], "images": [], "tables": []}, {"section_id": "section_2", "title": "2 Hierarchical Reasoning Model", "content": "We present the HRM, inspired by three fundamental principles of neural computation observed in the brain:\n* [leftmargin=*]\n* **Hierarchical processing:** The brain processes information across a hierarchy of cortical areas. Higher-level areas integrate information over longer timescales and form abstract representations, while lower-level areas handle more immediate, detailed sensory and motor processing [20, 22, 21].\nFigure 2: **The necessity of depth for complex reasoning. Left:** On _Sudoku-Extreme Full_, which require extensive tree-search and backtracking, increasing a Transformer’s width yields no performance gain, while increasing depth is critical. **Right:** Standard architectures saturates, failing to benefit from increased depth. HRM overcomes this fundamental limitation, effectively using its computational depth to achieve near-perfect accuracy.\n* **Temporal Separation:** These hierarchical levels in the brain operate at distinct intrinsic timescales, reflected in neural rhythms (e.g., slow theta waves, 4-8 Hz and fast gamma waves, 30-100 Hz) [30, 31]. This separation allows for stable, high-level guidance of rapid, low-level computations [32, 33].\n* **Recurrent Connectivity:** The brain features extensive recurrent connections. These feedback loops enable iterative refinement, yielding more accurate and context-sensitive representations at the cost of additional processing time. Additionally, the brain largely avoids the problematic deep credit assignment problem associated with BPTT19. Footnote 19: While inspired by temporal separation in the brain, our model’s “high-level” and “low-level” modules are conceptual abstractions and do not map directly to specific neural oscillation frequencies.\nThe HRM model consists of four learnable components: an input network \\(f_{I}(\\cdot;\\theta_{I})\\), a low-level recurrent module \\(f_{L}(\\cdot;\\theta_{L})\\), a high-level recurrent module \\(f_{H}(\\cdot;\\theta_{H})\\), and an output network \\(f_{O}(\\cdot;\\theta_{O})\\). The model's dynamics unfold over \\(N\\) high-level cycles of \\(T\\) low-level timesteps each2. We index the total timesteps of one forward pass by \\(i=1,\\ldots,N\\times T\\). The modules \\(f_{L}\\) and \\(f_{H}\\) each keep a hidden state--\\(z_{L}^{i}\\) for \\(f_{L}\\) and \\(z_{H}^{i}\\) for \\(f_{H}\\)--which are initialized with the vectors \\(z_{L}^{0}\\) and \\(z_{H}^{0}\\), respectively.\nFootnote 2: While inspired by temporal separation in the brain, our model’s “high-level” and “low-level” modules are conceptual abstractions and do not map directly to specific neural oscillation frequencies.\nThe HRM maps an input vector \\(x\\) to an output prediction vector \\(\\hat{y}\\) as follows. First, the input \\(x\\) is projected into a working representation \\(\\tilde{x}\\) by the input network:\n\\[\\tilde{x}=f_{I}(x;\\theta_{I})\\ .\\]\nAt each timestep \\(i\\), the L-module updates its state conditioned on its own previous state, the H-module's current state (which remains fixed throughout the cycle), and the input representation. The H-module only updates once per cycle (i.e., every \\(T\\) timesteps) using the L-module's final state at the end of that cycle:\n\\[z_{L}^{i} =f_{L}\\left(z_{L}^{i-1},z_{H}^{i-1},\\tilde{x};\\theta_{L}\\right)\\ ,\\] \\[z_{H}^{i} =\\begin{cases}f_{H}\\left(z_{H}^{i-1},z_{L}^{i-1};\\theta_{H}\\right) &\\text{if }i\\equiv 0\\,\\left(\\mathrm{mod}\\ T\\right),\\\\ z_{H}^{i-1}&\\text{otherwise}\\ \\ .\\end{cases}\\]\nFinally, after \\(N\\) full cycles, a prediction \\(\\hat{y}\\) is extracted from the hidden state of the H-module:\n\\[\\hat{y}=f_{O}(z_{H}^{NT};\\theta_{O})\\ .\\]\nThis entire \\(NT\\)-timestep process represents a single forward pass of the HRM. A halting mechanism (detailed later in this section) determines whether the model should terminate, in which case \\(\\hat{y}\\) will be used as the final prediction, or continue with an additional forward pass.\n**Hierarchical convergence** Although convergence is crucial for recurrent networks, standard RNNs are fundamentally limited by their tendency to converge too early. As the hidden state settles toward a fixed point, update magnitudes shrink, effectively stalling subsequent computation and capping the network's effective depth. To preserve computational power, we actually want convergence to proceed very slowly-but engineering that gradual approach is difficult, since pushing convergence too far edges the system toward instability.\nHRM is explicitly designed to counteract this premature convergence through a process we term _hierarchical convergence_. During each cycle, the L-module (an RNN) exhibits stable convergence to a _local equilibrium_. This equilibrium, however, depends on the high-level state \\(z_{H}\\) supplied during that cycle. After completing the \\(T\\) steps, the H-module incorporates the sub-computation's outcome (the final state \\(z_{L}\\)) and performs its own update. This \\(z_{H}\\) update establishes a fresh context for the L-module, essentially \"restarting\" its computational path and initiating a new convergence phase toward a different local equilibrium.\nThis process allows the HRM to perform a sequence of distinct, stable, nested computations, where the H-module directs the overall problem-solving strategy and the L-module executes the intensive search or refinement required for each step. Although a standard RNN may approach convergence within \\(T\\) iterations, the hierarchical convergence benefits from an enhanced effective depth of \\(NT\\) steps. As empirically shown in Figure 3, this mechanism allows HRM both to maintain high computational activity (forward residual) over many steps (in contrast to a standard RNN, whose activity rapidly decays) and to enjoy stable convergence. This translates into better performance at any computation depth, as illustrated in Figure 2.\n**Approximate gradient** Recurrent models typically use BPTT to compute gradients. However, BPTT requires storing the hidden states from the forward pass and then combining them with gradients during the backward pass, which demands \\(O(T)\\) memory for T timesteps. This heavy memory burden forces smaller batch sizes and leads to poor GPU utilization, especially for large-scale networks. Additionally, because retaining the full history trace through time is biologically implausible, it is unlikely that the brain implements BPTT [19].\nFortunately, if a recurrent neural network converges to a fixed point, we can avoid unrolling its state sequence by applying backpropagation in a single step at that equilibrium point. Moreover, such a mechanism could plausibly be implemented in the brain using only local learning rules [34, 35]. Based\nFigure 3: Comparison of forward residuals and PCA trajectories. HRM shows hierarchical convergence: the H-module steadily converges, while the L-module repeatedly converges within cycles before being reset by H, resulting in residual spikes. The recurrent neural network exhibits rapid convergence with residuals quickly approaching zero. In contrast, the deep neural network experiences vanishing gradients, with significant residuals primarily in the initial (input) and final layers.\non this finding, we propose a one-step approximation of the HRM gradient-using the gradient of the last state of each module and treating other states as constant. The gradient path is, therefore,\n\\[\\text{Output head}\\rightarrow\\text{final state of the H-module}\\rightarrow\\text{final state of the L-module}\\rightarrow\\text{input embedding}\\]\nThe above method needs \\(O(1)\\) memory, does not require unrolling through time, and can be easily implemented with an autograd framework such as PyTorch, as shown in Figure 4. Given that each module only needs to back-propagate errors through its most recent local synaptic activity, this approach aligns well with the perspective that cortical credit assignment relies on short-range, temporally local mechanisms rather than on a global replay of activity patterns.\nThe one-step gradient approximation is theoretically grounded in the mathematics of Deep Equilibrium Models (DEQ) [36] which employs the Implicit Function Theorem (IFT) to bypass BPTT, as detailed next. Consider an idealized HRM behavior where, during high-level cycle \\(k\\), the L-module repeatedly updates until its state \\(z_{L}\\) converges to a local fixed point \\(z_{H}^{\\star}\\). This fixed point, given the current high-level state \\(z_{H}^{k-1}\\), can be expressed as\n\\[z_{L}^{\\star}=f_{L}(z_{L}^{\\star},z_{H}^{k-1},\\tilde{x};\\theta_{L})\\ .\\]\nThe H-module then performs a single update using this converged L-state:\n\\[z_{H}^{k}=f_{H}(z_{H}^{k-1},z_{L}^{\\star};\\theta_{H})\\ .\\]\nWith a proper mapping \\(\\mathcal{F}\\), the updates to the high-level state can be written in a more compact form as \\(z_{H}^{k}=\\mathcal{F}(z_{H}^{k-1};\\tilde{x},\\theta)\\), where \\(\\theta=(\\theta_{I},\\theta_{L})\\), and the fixed-point can be written as \\(z_{H}^{\\star}=\\mathcal{F}(z_{H}^{\\star};\\tilde{x},\\theta)\\). Let \\(J_{\\mathcal{F}}=\\frac{\\partial\\mathcal{F}}{\\partial z_{H}}\\) be the Jacobian of \\(\\mathcal{F}\\), and assume that the matrix \\(I-J_{\\mathcal{F}}\\) is invertible at \\(z_{H}^{\\star}\\) and that the mapping \\(\\mathcal{F}\\) is continuously differentiable. The Implicit Function Theorem then allows us to calculate the exact gradient of fixed point \\(z_{H}^{\\star}\\) with respect to the parameters \\(\\theta\\) without explicit back-propagation:\n\\[\\frac{\\partial z_{H}^{\\star}}{\\partial\\theta}=\\left(I-J_{\\mathcal{F}}\\big{|}_ {z_{H}^{\\star}}\\right)^{-1}\\frac{\\partial\\mathcal{F}}{\\partial\\theta}\\bigg{|} _{z_{H}^{\\star}}\\ .\\] (1)\nCalculating the above gradient requires evaluating and inverting matrix \\((I-J_{\\mathcal{F}})\\) that can be computationally expensive. Given the Neumann series expansion,\n\\[(I-J_{\\mathcal{F}})^{-1}=I+J_{\\mathcal{F}}+J_{\\mathcal{F}}^{2}+J_{\\mathcal{F} }^{3}+\\ldots\\ ,\\]\nthe so-called _1-step gradient_[37] approximates the series by considering only its first term, i.e. \\((I-J_{\\mathcal{F}})^{-1}\\approx I\\), and leads to the following approximation of Equation (1):\n\\[\\frac{\\partial z_{H}^{\\star}}{\\partial\\theta_{H}}\\approx\\frac{ \\partial f_{H}}{\\partial\\theta_{H}},\\quad\\frac{\\partial z_{H}^{\\star}}{ \\partial\\theta_{L}}\\approx\\frac{\\partial f_{H}}{\\partial z_{L}^{\\star}}\\cdot \\frac{\\partial z_{L}^{\\star}}{\\partial\\theta_{L}},\\quad\\frac{\\partial z_{H}^ {\\star}}{\\partial\\theta_{I}}\\approx\\frac{\\partial f_{H}}{\\partial z_{L}^{ \\star}}\\cdot\\frac{\\partial z_{L}^{\\star}}{\\partial\\theta_{I}}\\ .\\] (2)\nFigure 4: **Top:** Diagram of HRM with approximate gradient. **Bottom:** Pseudocode of HRM with deep supervision training in PyTorch.\nThe gradients of the low-level fixed point, \\(\\frac{\\partial z_{L}^{*}}{\\partial\\theta_{L}}\\) and \\(\\frac{\\partial z_{L}^{*}}{\\partial\\theta_{I}}\\), can also be approximated using another application of the 1-step gradient:\n\\[\\frac{\\partial z_{L}^{*}}{\\partial\\theta_{L}}\\approx\\frac{\\partial f_{L}}{ \\partial\\theta_{L}},\\quad\\frac{\\partial z_{L}^{*}}{\\partial\\theta_{I}}\\approx \\frac{\\partial f_{L}}{\\partial\\theta_{I}}\\ .\\] (3)\nBy substituting Equation (3) back into Equation (2), we arrive at the final simplified gradients.\nBefore defining our loss function, we must first introduce two key elements of our proposed method: _deep supervision_ and _adaptive computational time_.\n**Deep supervision** Inspired by the principle that periodic neural oscillations regulate when learning occurs in the brain [38], we incorporate a deep supervision mechanism into HRM, as detailed next.\nGiven a data sample \\((x,y)\\), we run multiple forward passes of the HRM model, each of which we refer to as a _segment_. Let \\(M\\) denote the total number of segments executed before termination. For each segment \\(m\\in\\{1,\\dots,M\\}\\), let \\(z^{m}=(z_{H}^{mNT},z_{L}^{mNT})\\) represent the hidden state at the conclusion of segment \\(m\\), encompassing both high-level and low-level state components.\nAt each segment \\(m\\), we apply a deep supervision step as follows:", "level": 1, "page_numbers": [], "subsections": [], "references": [], "equations": [], "images": [], "tables": []}, {"section_id": "section_3", "title": "3. Update parameters: \\[\\theta\\leftarrow\\text{OptimizerStep}(\\theta,\\nabla_{\\theta}L^{m})\\]", "content": "The crucial aspect of this procedure is that the hidden state \\(z^{m}\\) is \"detached\" from the computation graph before being used as the input state for the next segment. Consequently, gradients from segment \\(m+1\\) do not propagate back through segment \\(m\\), effectively creating a 1-step approximation of the gradient of the recursive deep supervision process [39, 40]. This approach provides more frequent feedback to the H-module and serves as a regularization mechanism, demonstrating superior empirical performance and enhanced stability in deep equilibrium models when compared to more complex, Jacobian-based regularization techniques [39, 41]. Figure 4 shows pseudocode of deep supervision training.\n**Adaptive computational time (ACT)** The brain dynamically alternates between automatic thinking (\"System 1\") and deliberate reasoning (\"System 2\") [42]. Neuroscientific evidence shows that these cognitive modes share overlapping neural circuits, particularly within regions such as the prefrontal cortex and the default mode network [43, 44]. This indicates that the brain dynamically modulates the \"runtime\" of these circuits according to task complexity and potential rewards [45, 46].\nInspired by the above mechanism, we incorporate an adaptive halting strategy into HRM that enables \"thinking, fast and slow\". This integration leverages deep supervision and uses the Q-learning\nalgorithm [47] to adaptively determine the number of segments. A Q-head uses the final state of the H-module to predict the Q-values \\(\\hat{Q}^{m}=(\\hat{Q}^{m}_{\\text{halt}},\\hat{Q}^{m}_{\\text{continue}})\\) of the \"halt\" and \"continue\" actions:\n\\[\\hat{Q}^{m}=\\sigma(\\theta_{Q}^{\\top}z_{H}^{mNT})\\,,\\]\nwhere \\(\\sigma\\) denotes the sigmoid function applied element-wise. The halt or continue action is chosen using a randomized strategy as detailed next. Let \\(M_{\\max}\\) denote the maximum number of segments (a fixed hyperparameter) and \\(M_{\\min}\\) denote the minimum number of segments (a random variable). The value of \\(M_{\\min}\\) is determined stochastically: with probability \\(\\varepsilon\\), it is sampled uniformly from the set \\(\\{2,\\cdots,M_{\\max}\\}\\) (to encourage longer thinking), and with probability \\(1-\\varepsilon\\), it is set to 1. The halt action is selected under two conditions: when the segment count surpasses the maximum threshold \\(M_{\\max}\\), or when the estimated halt value \\(\\hat{Q}_{\\text{halt}}\\) exceeds the estimated continue value \\(\\hat{Q}_{\\text{continue}}\\) and the segment count has reached at least the minimum threshold \\(M_{\\min}\\).\nThe Q-head is updated through a Q-learning algorithm, which is defined on the following episodic Markov Decision Process (MDP). The state of the MDP at segment \\(m\\) is \\(z^{m}\\), and the action space is \\(\\{\\text{halt},\\text{continue}\\}\\). Choosing the action \"halt\" terminates the episode and returns a binary reward indicating prediction correctness, i.e., \\(\\mathbf{1}\\{\\hat{y}^{m}=y\\}\\). Choosing \"continue\" yields a reward of 0 and the state transitions to \\(z^{m+1}\\). Thus, the Q-learning targets for the two actions \\(\\hat{G}^{m}=(\\hat{G}^{m}_{\\text{halt}},\\hat{G}^{m}_{\\text{continue}})\\) are given by\n\\[\\hat{G}^{m}_{\\text{halt}} =\\mathbf{1}\\{\\hat{y}^{m}=y\\}\\,,\\] \\[\\hat{G}^{m}_{\\text{continue}} =\\begin{cases}\\hat{Q}^{m+1}_{\\text{halt}},&\\text{if }m\\geq N_{ \\max}\\,,\\\\ \\max(\\hat{Q}^{m+1}_{\\text{halt}},\\hat{Q}^{m+1}_{\\text{continue}})\\,,&\\text{ otherwise }.\\end{cases}\\]\nWe can now define the loss function of our learning procedure. The overall loss for each supervision segment combines both the Q-head loss and the sequence-to-sequence loss:\n\\[L^{m}_{\\text{ACT}}=\\textsc{Loss}(\\hat{y}^{m},y)+\\textsc{BinaryCrossEntropy}( \\hat{Q}^{m},\\hat{G}^{m})\\ .\\]\nMinimizing the above loss enables both accurate predictions and nearly optimal stopping decisions.\nSelecting the \"halt\" action ends the supervision loop. In practice, sequences are processed in batches, which can be easily handled by substituting any halted sample in the batch with a fresh sample from the dataloader.\nFigure 5 presents a performance comparison between two HRM variants: one incorporating ACT and another employing a fixed computational step count equivalent to ACT's \\(M_{\\max}\\) parameter. It shows that ACT effectively adapts its computational resources based on task complexity, achieving significant computational savings with minimal impact on performance.\n**Inference-time scaling** An effective neural model should exploit additional computational resources during inference to enhance performance. As illustrated in Figure 5-(c), HRM seamlessly achieves inference-time scaling by simply increasing the computational limit parameter, \\(M_{\\max}\\) without requiring further training or architectural modifications.\nAdditional compute is especially effective for tasks that demand deeper reasoning. On Sudoku--a problem that often requires long-term planning--HRM exhibits strong inference-time scaling. On the other hand, we find that extra computational resources yield minimal gains in ARC-AGI challenge, as solutions generally require only a few transformations.", "level": 1, "page_numbers": [], "subsections": [], "references": [], "equations": [], "images": [], "tables": []}, {"section_id": "section_4", "title": "3.2.2 Stability of Q-learning in ACT", "content": "The deep Q-learning that underpins our ACT mechanism is known to be prone to instability, often requiring stabilization techniques such as replay buffers and target networks [48], which are absent in our design. Our approach, however, achieves stability through the intrinsic properties of our model and training procedure. Recent theoretical work by Gallici et al. [49] shows that Q-learning can achieve convergence if network parameters are bounded, weight decay is incorporated during training, and post-normalization layers are implemented. Our model satisfies these conditions through its Post-Norm architecture that employs RMSNorm (a layer normalization variant) and the AdamW optimizer. AdamW has been shown to solve an \\(L_{\\infty}\\)-constrained optimization problem, ensuring that model parameters remain bounded by \\(1/\\lambda\\)[50].", "level": 1, "page_numbers": [], "subsections": [], "references": [], "equations": [], "images": [], "tables": []}, {"section_id": "section_5", "title": "3.2.3 Architectural details", "content": "We employ a sequence-to-sequence architecture for HRM. Both input and output are represented as token sequences: \\(x=(x_{1},\\ldots,x_{l})\\) and \\(y=(y_{1},\\ldots,y_{l^{\\prime}})\\) respectively. The model includes an embedding layer \\(f_{I}\\) that converts discrete tokens into vector representations, and an output head \\(f_{O}(z;\\theta_{O})=\\text{softmax}(\\theta_{O}z)\\) that transforms hidden states into token probability distributions \\(\\hat{y}\\). For small-sample experiments, we replace softmax with stablemax [51] to improve generalization performance. The sequence-to-sequence loss is averaged over all tokens, \\(\\textsc{Loss}(\\hat{y},y)=\\frac{1}{l^{\\prime}}\\sum_{i=1}^{l^{\\prime}}\\log p(y_{ i})\\), where \\(p(y_{i})\\) is the probability that distribution \\(\\hat{y}_{i}\\) assigns to token \\(y_{i}\\). The initial hidden states \\(z^{0}\\) are initialized by sampling from a truncated normal distribution with standard deviation of 1, truncation of 2, and kept fixed throughout training.\nBoth the low-level and high-level recurrent modules \\(f_{L}\\) and \\(f_{H}\\) are implemented using encoder-only Transformer [52] blocks with identical architectures and dimensions. These modules take multiple inputs, and we use straightforward element-wise addition to combine them, though more sophisticated merging techniques such as gating mechanisms could potentially improve performance and is left for future work. For all Transformer blocks in this work--including those in the baseline models--we incorporate the enhancements found in modern LLMs (based on Llama [53] architectures). These improvements include Rotary Positional Encoding [54], Gated Linear Units [55], RMSNorm [56], and the removal of bias terms from linear layers.\nFurthermore, both HRM and recurrent Transformer models implement a Post-Norm architecture\nFigure 5: **Effectiveness of Adaptive Computation Time (ACT)** on the _Sudoku-Extreme-Full_. **(a)** Mean compute steps used by models with ACT versus models with a fixed number of compute steps (\\(M\\)). ACT maintains a low and stable number of average compute steps even as the maximum limit (\\(M_{\\max}\\)) increases. **(b)** Accuracy comparison. The ACT model achieves performance comparable to the fixed-compute model while utilizing substantially fewer computational steps on average. **(c)** Inference-time scalability. Models trained with a specific \\(M_{\\max}\\) can generalize to higher computational limits during inference, leading to improved accuracy. For example, a model trained with \\(M_{\\max}=8\\) continues to see accuracy gains when run with \\(M_{\\max}=16\\) during inference.\nwith weights initialized via truncated LeCun Normal initialization [57, 58, 59], while the scale and bias parameters are excluded from RMSNorm. All parameters are optimized using the Adam-atan2 optimizer [60], a scale-invariant variant of Adam [61], combined with a constant learning rate that includes linear warm-up.", "level": 1, "page_numbers": [], "subsections": [], "references": [], "equations": [], "images": [], "tables": []}, {"section_id": "section_6", "title": "3 Results", "content": "This section begins by describing the ARC-AGI, Sudoku, and Maze benchmarks, followed by an overview of the baseline models and their results. Figure 6-(a,b,c) presents a visual representation of the three benchmark tasks, which are selected to evaluate various reasoning abilities in AI models.", "level": 1, "page_numbers": [], "subsections": [], "references": [], "equations": [], "images": [], "tables": []}, {"section_id": "section_7", "title": "3.1.1 ARC-AGI Challenge", "content": "The ARC-AGI benchmark evaluates general fluid intelligence through IQ-test-like puzzles that require inductive reasoning [27]. The initial version, ARC-AGI-1, presents challenges as input-output grid pairs that force AI systems to extract and generalize abstract rules from just a few examples. Each task provides a few input-output example pairs (usually 2-3) and a test input. An AI model has two attempts to produce the correct output grid. Although some believe that mastering ARC-AGI would signal true artificial general intelligence, its primary purpose is to expose the current roadblocks in AGI progress. In fact, both conventional deep learning methods and CoT techniques have faced significant challenges with ARC-AGI-1, primarily because it requires the ability to generalize to entirely new tasks [28].\nAddressing the limitations identified in ARC-AGI-1, ARC-AGI-2 significantly expands the benchmark by providing a more comprehensive and carefully refined collection of tasks. These new tasks emphasize deeper compositional reasoning, multi-step logic, contextual rule application, and symbolic abstraction. Human calibration studies show these tasks are challenging but doable for people, while being much harder for current AI systems, offering a clearer measure of general reasoning abilities [29].\nFigure 6: **Left: Visualization of benchmark tasks. Right: Difficulty of _Sudoku-Extreme_ examples.**\nSudoku-ExtremeSudoku is a 9\\(\\times\\)9 logic puzzle, requiring each row, column, and 3\\(\\times\\)3 block to contain the digits 1-9 exactly once. A prediction is considered correct if it exactly matches the puzzle's unique solution. Sudoku's complex logical structure makes it a popular benchmark for evaluating logical reasoning in machine learning [62, 63, 64].\nThe most frequently used Sudoku dataset in research, namely the Kaggle dataset [65], can be fully solved using elementary single-digit techniques [66]. The minimal 17-clue puzzles [62], another widely-used collection, might seem more challenging due to its small number of clues. However, this perception is misleading--since 17 represents the minimum number of clues required to guarantee a unique Sudoku solution, these hints need to be highly orthogonal to each other. This orthogonal arrangement leads to many direct, easily-resolved solution paths [67].\nWe introduce _Sudoku-Extreme_, a more challenging dataset that is compiled from the aforementioned easy datasets as well as puzzles recognized by the Sudoku community as exceptionally difficult for human players:\n* Easy puzzles compiled from Kaggle, 17-clue, plus unbiased samples from the Sudoku puzzle distribution [67]: totaling \\(1\\,149\\,158\\) puzzles.\n* Challenging puzzles compiled from Magictour 1465, Forum-Hard and Forum-Extreme subsets: totaling \\(3\\,104\\,157\\) puzzles.\nThe compiled data then undergo a strict 90/10 train-test split, ensuring that the test set puzzles cannot be derived through equivalent transformations of any training samples. _Sudoku-Extreme_ is a down-sampled subset of this data containing 1000 training examples. We use _Sudoku-Extreme_ in our main experiments (Figure 1), which focuses on small-sample learning scenarios. To guarantee convergence and control overfitting effects in our analysis experiments (Figures 2, 3 and 5), we use the complete training data, _Sudoku-Extreme-Full_, containing \\(3\\,831\\,994\\) examples.\nWe measure puzzle difficulty by counting the number of search backtracks (\"guesses\") required by a smart Sudoku solver program _tdoku_, which uses propositional logic to reduce the number of guesses [67]. Our _Sudoku-Extreme_ dataset exhibits a mean difficulty of \\(22\\) backtracks per puzzle, significantly higher than existing datasets, including recent handmade puzzles Sudoku-Bench [68] which average just \\(0.45\\) backtracks per puzzle. These subset complexity levels are shown in Figure 6-(d).\nFor ARC-AGI challenge, we start with all input-output example pairs in the training and the evaluation sets. The dataset is augmented by applying translations, rotations, flips, and color permutations to the puzzles. Each task example is prepended with a learnable special token that represents the puzzle it belongs to. At test time, we proceed as follows for each test input in the evaluation set: (1) Generate and solve 1000 augmented variants and, for each, apply the inverse-augmentation transform to obtain a prediction. (2) Choose the two most popular predictions as the final outputs.3 All results are reported on the evaluation set.\nFootnote 3: The ARC-AGI allows two attempts for each test input.\nWe augment Sudoku puzzles by applying band and digit permutations, while data augmentation is disabled for Maze tasks. Both tasks undergo only a single inference pass.\nFor ARC-AGI, the scores of the CoT models are taken from the official leaderboard [29], while for Sudoku and Maze, the scores are obtained by evaluating through the corresponding API.\nIn Figure 1, the baselines are grouped based on whether they are pre-trained and use CoT, or neither. The \"Direct pred\" baseline means using \"direct prediction without CoT and pre-training\", which retains the exact training setup of HRM but swaps in a Transformer architecture. Interestingly, on ARC-AGI-1, \"Direct pred\" matches the performance of Liao and Gu [73], who built a carefully designed, domain-specific equivariant network for learning the ARC-AGI task from scratch, without pre-training. By substituting the Transformer architecture with HRM's hierarchical framework and implementing ACT, we achieve more than a twofold performance improvement.\nOn the _Sudoku-Extreme_ and _Maze-Hard_ benchmarks, the performance gap between HRM and the baseline methods is significant, as the baselines almost never manage to solve the tasks. These benchmarks that demand lengthy reasoning traces are particularly difficult for CoT-based methods. With only 1000 training examples, the \"Direct pred\" baseline--which employs an 8-layer Transformer identical in size to HRM--fails entirely on these challenging reasoning problems. When trained on the larger _Sudoku-Extreme-Full_ dataset, however, \"Direct pred\" can solve some easy Sudoku puzzles and reaches \\(16.9\\%\\) accuracy (see Figure 2). Lehnert et al. [71] showed that a large vanilla Transformer model with 175M parameters, trained on 1 million examples across multiple trials, achieved only marginal success on 30x30 Maze tasks, with accuracy below \\(20\\%\\) using the \\(pass@64\\) evaluation metric.", "level": 1, "page_numbers": [], "subsections": [], "references": [], "equations": [], "images": [], "tables": []}, {"section_id": "section_8", "title": "Visualization of intermediate timesteps", "content": "Although HRM demonstrates strong performance on complex reasoning tasks, it raises an intriguing question: what underlying reasoning algorithms does the HRM neural network actually implement? Addressing this question is important for enhancing model interpretability and developing a deeper understanding of the HRM solution space.\nWhile a definitive answer lies beyond our current scope, we begin our investigation by analyzing state trajectories and their corresponding solution evolution. More specifically, at each timestep \\(i\\) and given the low-level and high-level state pair (\\(z^{i}_{L}\\) and \\(z^{i}_{H}\\)) we perform a preliminary forward pass through the H-module to obtain \\(\\bar{z}^{i}=f_{H}(z^{i}_{H},z^{i}_{L};\\theta_{H})\\) and its corresponding decoded prediction \\(\\bar{y}^{i}=f_{O}(\\bar{z}^{i};\\theta_{O})\\). The prediction \\(\\bar{y}^{i}\\) is then visualized in Figure 7.\nIn the Maze task, HRM appears to initially explore several potential paths simultaneously, subsequently eliminating blocked or inefficient routes, then constructing a preliminary solution outline followed by multiple refinement iterations. In Sudoku, the strategy resembles a depth-first search\napproach, where the model appears to explore potential solutions and backtracks when it hits dead ends. HRM uses a different approach for ARC tasks, making incremental adjustments to the board and iteratively improving it until reaching a solution. Unlike Sudoku, which involves frequent backtracking, the ARC solution path follows a more consistent progression similar to hill-climbing optimization. Importantly, the model shows that it can adapt to different reasoning approaches, likely choosing an effective strategy for each particular task. Further research is needed to gain more comprehensive insights into these solution strategies.", "level": 1, "page_numbers": [], "subsections": [], "references": [], "equations": [], "images": [], "tables": []}, {"section_id": "section_9", "title": "4 Brain Correspondence", "content": "A key principle from systems neuroscience is that a brain region's functional repertoire--its ability to handle diverse and complex tasks--is closely linked to the dimensionality of its neural representations [75, 76]. Higher-order cortical areas, responsible for complex reasoning and decision-making, must handle a wide variety of tasks, demanding more flexible and context-dependent processing [77]. In dynamical systems, this flexibility is often realized through higher-dimensional state-space trajectories, which allow for a richer repertoire of potential computations [78]. This principle gives rise to an observable _dimensionality hierarchy_, where a region's position in the processing hierarchy correlates with its _effective dimensionality_. To quantify this phenomenon, we can examine the\nFigure 7: **Visualization of intermediate predictions by HRM on benchmark tasks.****Top:**_MazeHard_—blue cells indicate the predicted path. **Middle:**_Sudoku-Extreme_—bold cells represent initial givens; red highlights cells violating Sudoku constraints; grey shading indicates changes from the previous timestep. **Bottom:** ARC-AGI-2 Task—left: provided example input-output pair; right: intermediate steps solving the test input.\nFigure 8: **Hierarchical Dimensionality Organization in the HRM and Mouse Cortex.****(a,b)** are adapted from Posani et al. 74. (a) Anatomical illustration of mouse cortical areas, color-coded by functional modules. (b) Correlation between Participation Ratio (PR), a measure of effective neural dimensionality, and hierarchical position across different mouse cortical areas. Higher positions in the hierarchy (e.g., MOs, ACAd) exhibit significantly higher PR values compared to lower sensory areas (e.g., SSp-n), with a Spearman correlation coefficient of \\(\\rho\\) = 0.79 (P = 0.0003). (c,d) **Trained HRM.** (c) PR scaling of the trained HRM with task diversity. The dimensionality of the high-level module (\\(z_{H}\\)) scales with the number of unique tasks (trajectories) included in the analysis, indicating an adaptive expansion of its representational capacity. In contrast, the low-level module’s (\\(z_{L}\\)) dimensionality remains stable. (d) PR values for the low-level (\\(z_{L}\\), PR = 30.22) and high-level (\\(z_{H}\\), PR = 89.95) modules of the _trained_ HRM, computed from neural activity during 100 unique Sudoku-solving trajectories. A clear dimensionality hierarchy is observed, with the high-level module operating in a substantially higher-dimensional space. (e,f) **Analysis of Untrained Network.** To verify that the dimensionality hierarchy is an emergent property of training, the same analyses were performed on an _untrained_ HRM with random weights. (e) In contrast to the trained model’s scaling in (c), the dimensionality of both modules in the untrained model remains low and stable, failing to scale with the number of tasks. (f) Similarly, contrasting with the clear separation in (d), the PR values for the untrained model’s modules (\\(z_{L}\\), PR = 42.09; \\(z_{H}\\), PR = 40.75) are low and nearly identical, showing no evidence of hierarchical separation. This confirms that the observed hierarchical organization of dimensionality is a learned property that emerges through training, not an artifact of the model’s architecture.\nParticipation Ratio (PR), which serves as a standard measure of the effective dimensionality of a high-dimensional representation [79]. The PR is calculated using the formula\n\\[\\text{PR}=\\frac{(\\sum_{i}\\lambda_{i})^{2}}{\\sum_{i}\\lambda_{i}^{2}}\\,,\\]\nwhere \\(\\{\\lambda_{i}\\}\\) are the eigenvalues of the covariance matrix of neural trajectories. Intuitively, a higher PR value signifies that variance is distributed more evenly across many dimensions, corresponding to a higher-dimensional representation. Conversely, a lower PR value indicates that variance is concentrated in only a few principal components, reflecting a more compact, lower-dimensional structure.\nThe dimensionality hierarchy can be observed, for example, in the mouse cortex, where the PR of population activity increases monotonically from low-level sensory areas to high-level associative areas, supporting this link between dimensionality and functional complexity [74] (Figure 8 (a,b)).\nWe evaluated whether HRM reproduces this neuroscientific principle by calculating the PR for both recurrent modules after training on the _Sudoku-Extreme Full_ dataset. The PR computation used the covariance matrix derived from neural states gathered across multiple Sudoku-solving trajectories. The results show a striking parallel to the biological findings. The low-level module's state (\\(z_{L}\\)) occupies a relatively small subspace with a participation ratio of 30.22, whereas the high-level module's state (\\(z_{H}\\)) operates in a substantially larger subspace with a participation ratio of 89.95, as shown in Figure 8(c). Furthermore, Figure 8(d) shows that increasing the number of unique tasks (trajectories) from 10 to 100 causes \\(z_{H}\\) dimensionality to scale up accordingly, while \\(z_{L}\\) dimensionality remains stable. These results suggest an _emergent_ separation of representational capacity between the modules that parallels their functional roles.\nTo confirm that this hierarchical organization is an emergent property of training, and not an artifact of the network's architecture, we performed a control analysis using an identical but untrained network with random weights.\nWe initialized an identical HRM architecture with random weights and, without any training, measured the PR of its modules as the network processed the same task-specific inputs given to the trained model.\nThe results, shown in Figure 8(e,f), reveal a stark contrast: the high-level and low-level modules of the untrained network exhibit no hierarchical separation, with their PR values remaining low and nearly indistinguishable from each other. This control analysis validates that the dimensionality hierarchy is an _emergent property_ that arises as the model learns to perform complex reasoning.\nThe high-to-low PR ratio in HRM (\\(z_{H}/z_{L}\\approx 2.98\\)) closely matches that measured in the mouse cortex (\\(\\approx 2.25\\)). In contrast, conventional deep networks often exhibit _neural collapse_, where last-layer features converge to a low-dimensional subspace [80, 81, 82]. HRM therefore departs from the collapse pattern and instead fosters a high-dimensional representation in its higher module. This is significant because such representations are considered crucial for cognitive flexibility and are a hallmark of higher-order brain regions like the prefrontal cortex (PFC), which is central to complex reasoning.\nThis structural parallel suggests the model has discovered a fundamental organizational principle. By learning to partition its representations into a high-capacity, high-dimensional subspace (\\(z_{H}\\)) and a more specialized, low-dimensional one (\\(z_{L}\\)), HRM autonomously discovers an organizational\nprinciple that is thought to be fundamental for achieving robust and flexible reasoning in biological systems. This provides a potential mechanistic explanation for the model's success on complex, long-horizon tasks that are intractable for models lacking such a differentiated internal structure. We emphasize, however, that this evidence is correlational. While a causal link could be tested via intervention (e.g., by constraining the H-module's dimensionality), such methods are difficult to interpret in deep learning due to potential confounding effects on the training process itself. Thus, the causal necessity of this emergent hierarchy remains an important question for future investigation.", "level": 1, "page_numbers": [], "subsections": [], "references": [], "equations": [], "images": [], "tables": []}, {"section_id": "section_10", "title": "5.0.1 Reasoning and algorithm learning", "content": "Given the central role of reasoning problems and their close relation to algorithms, researchers have long explored neural architectures that enable algorithm learning from training instances. This line of work includes Neural Turing Machines (NTM)[83], the Differentiable Neural Computer (DNC)[84], and Neural GPUs[85]-all of which construct iterative neural architectures that mimic computational hardware for algorithm execution, and are trained to learn algorithms from data. Another notable work in this area is Recurrent Relational Networks (RRN)[62], which executes algorithms on graph representations through graph neural networks.\nRecent studies have integrated algorithm learning approaches with Transformer-based architectures. Universal Transformers extend the standard Transformer model by introducing a recurrent loop over the layers and implementing an adaptive halting mechanism. Geiping et al.[86] demonstrate that looped Transformers can generalize to a larger number of recurrent steps during inference than what they were trained on. Shen et al.[16] propose adding continuous recurrent reasoning tokens to the Transformer. Finally, TransNAR[8] combine recurrent graph neural networks with language models.\nBuilding on the success of CoT-based reasoning, a line of work have introduced fine-tuning methods that use reasoning paths from search algorithms (like A*) as SFT targets[87, 71, 70].\nWe also mention adaptive halting mechanisms designed to allocate additional computational resources to more challenging problems. This includes the Adaptive Computation Time (ACT) for RNNs[88] and follow-up research like PonderNet[89], which aims to improve the stability of this allocation process.\nHRM further pushes the boundary of algorithm learning through a brain-inspired computational architecture that achieves exceptional data efficiency and model expressiveness, successfully discovering complex and diverse algorithms from just 1000 training examples.", "level": 1, "page_numbers": [], "subsections": [], "references": [], "equations": [], "images": [], "tables": []}, {"section_id": "section_11", "title": "5.0.2 Brain-inspired reasoning architectures", "content": "Developing a model with the reasoning power of the brain has long been a goal in brain-inspired computing. Spaun[90] is one notable example, which uses spiking neural networks to create distinct modules corresponding to brain regions like the visual cortex and prefrontal cortex. This design enables an architecture to perform a range of cognitive tasks, from memory recall to simple reasoning puzzles. However, its reasoning relies on hand-designed algorithms, which may limit its ability to learn new tasks. Another significant model is the Tolman-Eichenbaum Machine (TEM)[91], which is inspired by the hippocampal-entorhinal system's role in spatial and relational memory tasks. TEM proposes that medial entorhinal cells create a basis for structural knowledge, while hippocampal cells link this basis to sensory information. This allows TEM to generalize and explains the emergence of various cell types like grid, border, and\nplace cells. Another approach involves neural sampling models [92], which view the neural signaling process as inference over a distribution, functioning similarly to a Boltzmann machine. These models often require hand-made rules to be set up for solving a specific reasoning task. In essence, while prior models are restricted to simple reasoning problems, HRM is designed to solve complex tasks that are hard for even advanced LLMs, without pre-training or task-specific manual design.\n**Hierarchical memory**: The hierarchical multi-timescale structure also plays an important role in how the brain processes memory. Models such as Hierarchical Sequential Models [93] and Clockwork RNN [94] use multiple recurrent modules that operate at varying time scales to more effectively capture long-range dependencies within sequences, thereby mitigating the forgetting issue in RNNs.\nSimilar mechanisms have also been adopted in linear attention methods for memorizing long contexts (see the Discussions section). Since HRM focuses on reasoning, full attention is applied for simplicity. Incorporating hierarchical memory into HRM could be a promising future direction.", "level": 1, "page_numbers": [], "subsections": [], "references": [], "equations": [], "images": [], "tables": []}, {"section_id": "section_12", "title": "6 Discussions", "content": "**Turing-completeness of HRM**: Like earlier neural reasoning algorithms including the Universal Transformer [95], HRM is computationally universal when given sufficient memory and time constraints. In other words, it falls into the category of models that can simulate any Turing machine, overcoming the computational limitations of standard Transformers discussed previously in the introduction. Given that earlier neural algorithm reasoners were trained as recurrent neural networks, they suffer from premature convergence and memory intensive BPTT. Therefore, in practice, their effective computational depth remains limited, though still deeper than that of a standard Transformer. By resolving these two challenges and being equipped with adaptive computation, HRM could be trained on long reasoning processes, solve complex puzzles requiring intensive depth-first search and backtracking, and move closer to practical Turing-completeness.\n**Reinforcement learning with chain-of-thought**: Beyond fine-tuning using human-annotated CoT, reinforcement learning (RL) represents another widely adopted training methodology. However, recent evidence suggests that RL primarily unlocks existing CoT-like capabilities rather than discovering fundamentally new reasoning mechanisms [96, 97, 98, 99]. Additionally, CoT-training with RL is known for its instability and data inefficiency, often requiring extensive exploration and careful reward design. In contrast, HRM takes feedback from dense gradient-based supervision rather than relying on a sparse reward signal. Moreover, HRM operates naturally in a continuous space, which is biologically plausible and avoids allocating same computational resources to each token, even though tokens vary in their reasoning and planning complexity [16].\n**Linear attention**: Recurrence has been explored not only for its capability in universal computation, but also as a means to replace the attention mechanism in Transformers, which suffers from quadratic time and memory complexity [100]. Recurrent alternatives offer a more efficient design by processing input tokens sequentially and predicting the next token at each time step, similar to early RNN-based language models.\nSome linear-attention variants, such as Log-linear Attention [101], share an RNN-like state-update that can be interpreted as propagating multi-timescale summary statistics, thereby retaining long-range context without the quadratic memory growth of standard self-attention. However, substituting the attention mechanism alone does not change the fact that Transformers are still fixed-depth, and require CoT as a compensatory mechanism. Notably, linear attention can operate with a reduced\nkey-value cache over extended contexts, making them more suitable for deployment on resource-constrained edge devices.", "level": 1, "page_numbers": [], "subsections": [], "references": [], "equations": [], "images": [], "tables": []}, {"section_id": "section_13", "title": "7 Conclusion", "content": "This work introduces the Hierarchical Reasoning Model, a brain-inspired architecture that leverages hierarchical structure and multi-timescale processing to achieve substantial computational depth without sacrificing training stability or efficiency. With only 27M parameters and training on just 1000 examples, HRM effectively solves challenging reasoning problems such as ARC, Sudoku, and complex maze navigation-tasks that typically pose significant difficulties for contemporary LLM and chain-of-thought models.\nAlthough the brain relies heavily on hierarchical structures to enable most cognitive processes, these concepts have largely remained confined to academic literature rather than being translated into practical applications. The prevailing AI approach continues to favor non-hierarchical models. Our results challenge this established paradigm and suggest that the Hierarchical Reasoning Model represents a viable alternative to the currently dominant chain-of-thought reasoning methods, advancing toward a foundational framework capable of Turing-complete universal computation.\n**Acknowledgements** We thank Mingli Yuan, Ahmed Murtadha Hasan Mahyoub and Hengshuai Yao for their insightful discussions and valuable feedback throughout the course of this work.", "level": 1, "page_numbers": [], "subsections": [], "references": [], "equations": [], "images": [], "tables": []}, {"section_id": "section_14", "title": "References", "content": "* [1] Ian Goodfellow, Yoshua Bengio, and Aaron Courville. _Deep Learning_. MIT Press, 2016. http://www.deeplearningbook.org.\n* [2] Kaiming He, X. Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. _2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 770-778, 2015.\n* [3] Lena Strobl. Average-hard attention transformers are constant-depth uniform threshold circuits, 2023.\n* [4] Tom Bylander. Complexity results for planning. In _Proceedings of the 12th International Joint Conference on Artificial Intelligence - Volume 1_, IJCAI'91, page 274-279, San Francisco, CA, USA, 1991. Morgan Kaufmann Publishers Inc. ISBN 1558601600.\n* [5] William Merrill and Ashish Sabharwal. A logic for expressing log-precision transformers. In _Neural Information Processing Systems_, 2023.\n* [6] David Chiang. Transformers in DLOGTIME-uniform TC\\({}^{0}\\). _Transactions on Machine Learning Research_, 2025.\n* [7] Lucas Lehnert, Sainbayar Sukhbaatar, DiJia Su, Qinqing Zheng, Paul McVay, Michael Rabbat, and Yuandong Tian. Beyond a*: Better planning with transformers via search dynamics bootstrapping. In _First Conference on Language Modeling_, 2024.\n* [8] Wilfried Bounsi, Borja Ibarz, Andrew Dudzik, Jessica B. Hamrick, Larisa Markeeva, Alex Vitvitskyi, Razvan Pascanu, and Petar Velivckovic'c. Transformers meet neural algorithmic reasoners. _ArXiv_, abs/2406.09308, 2024.\n* [9] William Merrill and Ashish Sabharwal. The parallelism tradeoff: Limitations of log-precision transformers. _Transactions of the Association for Computational Linguistics_, 11:531-545, 2023. doi: 10.1162/tacl_a_00562.\n* [10] Jason Wei, Yi Tay, et al. Chain-of-thought prompting elicits reasoning in large language models, 2022. arXiv preprint arXiv:2201.11903.\n* [11] William Merrill and Ashish Sabharwal. The expressive power of transformers with chain of thought. In _ICLR_, 2024.\n* [12] Xinyun Chen, Ryan A. Chi, Xuezhi Wang, and Denny Zhou. Premise order matters in reasoning with large language models. _ArXiv_, abs/2402.08939, 2024.\n* [13] Rongwu Xu, Zehan Qi, and Wei Xu. Preemptive answer \"attacks\" on chain-of-thought reasoning. In _Annual Meeting of the Association for Computational Linguistics_, 2024.\n* [14] Pablo Villalobos, Anson Ho, Jaime Sevilla, Tamay Besiroglu, Lennart Heim, and Marius Hobbhahn. Will we run out of data? limits of llm scaling based on human-generated data. _arXiv preprint arXiv:2211.04325_, 2022.\n* [15] Xinghao Chen, Anhao Zhao, Heming Xia, Xuan Lu, Hanlin Wang, Yanjun Chen, Wei Zhang, Jian Wang, Wenjie Li, and Xiaoyu Shen. Reasoning beyond language: A comprehensive survey on latent chain-of-thought reasoning, 2025.\n* [16] Xuan Shen, Yizhou Wang, Xiangxi Shi, Yanzhi Wang, Pu Zhao, and Jiuxiang Gu. Training large language models to reason in a continuous latent space. _arXiv preprint arXiv:2412.07423_, 2024.\n* Fedorenko et al. [2024] Evelina Fedorenko, Steven T Piantadosi, and Edward AF Gibson. Language is primarily a tool for communication rather than thought. _Nature_, 630(8017):575-586, 2024.\n* Wang et al. [2024] Hongyu Wang, Shuming Ma, Li Dong, Shaohan Huang, Dongdong Zhang, and Furu Wei. Deepnet: Scaling transformers to 1,000 layers. _IEEE Transactions on Pattern Analysis and Machine Intelligence_, 2024.\n* Lillicrap and Santoro [2019] Timothy P Lillicrap and Adam Santoro. Backpropagation through time and the brain. _Current Opinion in Neurobiology_, 55:82-89, 2019. ISSN 0959-4388. doi: https://doi.org/10.1016/j.conb.2019.01.011.\n* Murray et al. [2014] John D Murray, Alberto Bernacchia, David J Freedman, Ranulfo Romo, Jonathan D Wallis, Xinying Cai, Camillo Padoa-Schioppa, Tatiana Pasternak, Hyojung Seo, Daeyeol Lee, et al. A hierarchy of intrinsic timescales across primate cortex. _Nature neuroscience_, 17(12):1661-1663, 2014.\n* Zeraati et al. [2023] Roxana Zeraati, Yan-Liang Shi, Nicholas A Steinmetz, Marc A Gieselmann, Alexander Thiele, Tirin Moore, Anna Levina, and Tatiana A Engel. Intrinsic timescales in the visual cortex change with selective attention and reflect spatial connectivity. _Nature communications_, 14(1):1858, 2023.\n* Huntenburg et al. [2018] Julia M Huntenburg, Pierre-Louis Bazin, and Daniel S Margulies. Large-scale gradients in human cortical organization. _Trends in cognitive sciences_, 22(1):21-31, 2018.\n* Lamme and Roelfsema [2000] Victor AF Lamme and Pieter R Roelfsema. The distinct modes of vision offered by feedforward and recurrent processing. _Trends in neurosciences_, 23(11):571-579, 2000.\n* Bastos et al. [2012] Andre M Bastos, W Martin Usrey, Rick A Adams, George R Mangun, Pascal Fries, and Karl J Friston. Canonical microcircuits for predictive coding. _Neuron_, 76(4):695-711, 2012.\n* Kaleb et al. [2024] Klara Kaleb, Barbara Feulner, Juan Gallego, and Claudia Clopath. Feedback control guides credit assignment in recurrent neural networks. _Advances in Neural Information Processing Systems_, 37:5122-5144, 2024.\n* Lillicrap et al. [2020] Timothy P Lillicrap, Adam Santoro, Luke Marris, Colin J Akerman, and Geoffrey Hinton. Backpropagation and the brain. _Nature Reviews Neuroscience_, 21(6):335-346, 2020.\n* Chollet [2019] Francois Chollet. On the measure of intelligence (abstraction and reasoning corpus), 2019. arXiv preprint arXiv:1911.01547.\n* Chollet et al. [2024] Francois Chollet, Mike Knoop, Gregory Kamradt, and Bryan Landers. Arc prize 2024: Technical report. _ArXiv_, abs/2412.04604, 2024.\n* Chollet et al. [2025] Francois Chollet, Mike Knoop, Gregory Kamradt, Bryan Landers, and Henry Pinkard. Arcagi-2: A new challenge for frontier ai reasoning systems. _arXiv preprint arXiv:2505.11831_, 2025.\n* Buzsaki [2000] Gyorgy Buzsaki. Gamma, alpha, delta, and theta oscillations govern cognitive processes. _International Journal of Psychophysiology_, 39:241-248, 2000.\n* Buzsaki [2006] Gyorgy Buzsaki. _Rhythms of the Brain_. Oxford university press, 2006.\n* Pahor and Jausovec [2014] Anja Pahor and Norbert Jausovec. Theta-gamma cross-frequency coupling relates to the level of human intelligence. _Intelligence_, 46:283-290, 2014.\n* Tort et al. [2009] Adriano BL Tort, Robert W Komorowski, Joseph R Manns, Nancy J Kopell, and Howard Eichenbaum. Theta-gamma coupling increases during the learning of item-context associations. _Proceedings of the National Academy of Sciences_, 106(49):20942-20947, 2009.\n* [34] Benjamin Scellier and Yoshua Bengio. Equilibrium propagation: Bridging the gap between energy-based models and backpropagation. _Frontiers in Computational Neuroscience_, 11, 2016.\n* [35] Guillaume Bellec, Franz Scherr, Anand Subramoney, Elias Hajek, Darjan Salaj, Robert Legenstein, and Wolfgang Maass. A solution to the learning dilemma for recurrent networks of spiking neurons. _Nature Communications_, 11, 07 2020. doi: 10.1038/s41467-020-17236-y.\n* [36] Shaojie Bai, J Zico Kolter, and Vladlen Koltun. Deep equilibrium models. In _Advances in Neural Information Processing Systems_, pages 690-701, 2019.\n* [37] Zhengyang Geng, Xinyu Zhang, Shaojie Bai, Yisen Wang, and Zhouchen Lin. On training implicit models. _ArXiv_, abs/2111.05177, 2021.\n* [38] Katarina Begus and Elizabeth Bonawitz. The rhythm of learning: Theta oscillations as an index of active learning in infancy. _Developmental Cognitive Neuroscience_, 45:100810, 2020. ISSN 1878-9293. doi: https://doi.org/10.1016/j.dcn.2020.100810.\n* [39] Shaojie Bai, Zhengyang Geng, Yash Savani, and J. Zico Kolter. Deep Equilibrium Optical Flow Estimation . In _2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 610-620, 2022.\n* [40] Zaccharie Ramzi, Florian Mannel, Shaojie Bai, Jean-Luc Starck, Philippe Ciuciu, and Thomas Moreau. Shine: Sharing the inverse estimate from the forward pass for bi-level optimization and implicit models. _ArXiv_, abs/2106.00553, 2021.\n* [41] Shaojie Bai, Vladlen Koltun, and J. Zico Kolter. Stabilizing equilibrium models by jacobian regularization. In _International Conference on Machine Learning_, 2021.\n* [42] Daniel Kahneman and P Egan. Thinking, fast and slow (farrar, straus and giroux, new york), 2011.\n* [43] Matthew D Lieberman. Social cognitive neuroscience: a review of core processes. _Annu. Rev. Psychol._, 58(1):259-289, 2007.\n* [44] Randy L Buckner, Jessica R Andrews-Hanna, and Daniel L Schacter. The brain's default network: anatomy, function, and relevance to disease. _Annals of the new York Academy of Sciences_, 1124(1):1-38, 2008.\n* [45] Marcus E Raichle. The brain's default mode network. _Annual review of neuroscience_, 38(1):433-447, 2015.\n* [46] Andrew Westbrook and Todd S Braver. Cognitive effort: A neuroeconomic approach. _Cognitive, Affective, & Behavioral Neuroscience_, 15:395-415, 2015.\n* [47] Richard S. Sutton and Andrew G. Barto. _Reinforcement Learning: An Introduction_. MIT Press, Cambridge, MA, 2018.\n* [48] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and Martin A. Riedmiller. Playing atari with deep reinforcement learning. _ArXiv_, abs/1312.5602, 2013.\n* [49] Matteo Gallici, Mattie Fellows, Benjamin Ellis, Bartomeu Pou, Ivan Masmitja, Jakob Nicolaus Foerster, and Mario Martin. Simplifying deep temporal difference learning, 2025.\n* [50] Shuo Xie and Zhiyuan Li. Implicit bias of adamw: L inf norm constrained optimization. _ArXiv_, abs/2404.04454, 2024.\n* [51] Lucas Prieto, Melih Barsbey, Pedro A. M. Mediano, and Tolga Birdal. Grokking at the edge of numerical stability. In _The Thirteenth International Conference on Learning Representations_, 2025.\n* [52] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In _Advances in neural information processing systems_, pages 5998-6008, 2017.\n* [53] Meta AI. Llama 3: State-of-the-art open weight language models. Technical report, Meta, 2024. URL https://ai.meta.com/llama/.\n* [54] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. _Neurocomputing_, 568:127063, 2024.\n* [55] Noam M. Shazeer. Glu variants improve transformer. _ArXiv_, abs/2002.05202, 2020.\n* [56] Biao Zhang and Rico Sennrich. Root mean square layer normalization. _ArXiv_, abs/1910.07467, 2019.\n* [57] Gunter Klambauer, Thomas Unterthiner, Andreas Mayr, and Sepp Hochreiter. Self-normalizing neural networks. In _Neural Information Processing Systems_, 2017.\n* [58] JAX Developers. _jax.nn.initializers.lecun_normal_. Google Research, 2025. URL https://docs.jax.dev/en/latest/_autosummary/jax.nn.initializers.lecun_normal.html. Accessed June 22, 2025.\n* [59] Yann LeCun, Leon Bottou, Genevieve B Orr, and Klaus-Robert Muller. Efficient backprop. In _Neural networks: Tricks of the trade_, pages 9-50. Springer, 2002.\n* [60] Katie E Everett, Lechao Xiao, Mitchell Wortsman, Alexander A Alemi, Roman Novak, Peter J Liu, Izzeddin Gur, Jascha Sohl-Dickstein, Leslie Pack Kaelbling, Jaehoon Lee, and Jeffrey Pennington. Scaling exponents across parameterizations and optimizers. In _Forty-first International Conference on Machine Learning_, 2024.\n* [61] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization, 2017.\n* [62] Rasmus Berg Palm, Ulrich Paquet, and Ole Winther. Recurrent relational networks. In _Neural Information Processing Systems_, 2017.\n* [63] Jieyi Long. Large language model guided tree-of-thought. _ArXiv_, abs/2305.08291, 2023.\n* [64] Yilun Du, Jiayuan Mao, and Josh Tenenbaum. Learning iterative reasoning through energy diffusion. _ArXiv_, abs/2406.11179, 2024.\n* [65] Kyubyong Park. Can convolutional neural networks crack sudoku puzzles? https://github.com/Kyubyong/sudoku, 2018.\n* [66] Single-digit techniques. https://hodoku.sourceforge.net/en/tech_singles.php. Accessed: 2025-06-16.\n* [67] Tom Dillion. Tdoku: A fast sudoku solver and generator. https://t-dillon.github.io/tdoku/, 2025.\n* [68] Jeffrey Seely, Yuki Imajuku, Tianyu Zhao, Edoardo Cetin, and Llion Jones. Sudoku-bench: Evaluating creative reasoning with sudoku variants. _arXiv preprint arXiv:2505.16135_, 2025.\n* [69] Luke Darlow, Ciaran Regan, Sebastian Risi, Jeffrey Seely, and Llion Jones. Continuous thought machines. _arXiv preprint arXiv:2505.05522_, 2025.\n* Su et al. [2025] DiJia Su, Sainbayar Sukhbaatar, Michael Rabbat, Yuandong Tian, and Qinqing Zheng. Dualformer: Controllable fast and slow thinking by learning with randomized reasoning traces, 2025.\n* Lehnert et al. [2024] Lucas Lehnert, Sainbayar Sukhbaatar, DiJia Su, Qinqing Zheng, Paul McVay, Michael Rabbat, and Yuandong Tian. Beyond a*: Better planning with transformers via search dynamics bootstrapping. In _First Conference on Language Modeling_, 2024.\n* Kapadia et al. [2013] Mubbasir Kapadia, Francisco Garcia, Cory D. Boatright, and Norman I. Badler. Dynamic search on the gpu. In _2013 IEEE/RSJ International Conference on Intelligent Robots and Systems_, pages 3332-3337, 2013. doi: 10.1109/IROS.2013.6696830.\n* Liao and Gu [2025] Isaac Liao and Albert Gu. Arc-agi without pretraining, 2025. URL https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/arc_agi_without_pretraining.html.\n* Posani et al. [2025] Lorenzo Posani, Shuqi Wang, Samuel P Muscinelli, Liam Paninski, and Stefano Fusi. Rarely categorical, always high-dimensional: how the neural code changes along the cortical hierarchy. _bioRxiv_, pages 2024-11, 2025.\n* Rigotti et al. [2013] Mattia Rigotti, Omri Barak, Melissa R. Warden, Xiao-Jing Wang, Nathaniel D. Daw, Earl K. Miller, and Stefano Fusi. The importance of mixed selectivity in complex cognitive tasks. _Nature_, 497:585-590, 2013. doi: 10.1038/nature12160.\n* Mante et al. [2013] Valerio Mante, David Sussillo, Krishna V. Shenoy, and William T. Newsome. Context-dependent computation by recurrent dynamics in prefrontal cortex. _Nature_, 503(7474):78-84, 2013. doi: 10.1038/nature12742.\n* Miller and Cohen [2001] Earl K. Miller and Jonathan D. Cohen. An integrative theory of prefrontal cortex function. _Annual Review of Neuroscience_, 24(1):167-202, 2001. doi: 10.1146/annurev.neuro.24.1.167.\n* Maass [2002] Wolfgang Maass. Real-time computing without stable states: a new framework for neural computation based on perturbations. _Neural Computation_, 14(11):2531-2560, 2002. doi: 10.1162/089976602760407955.\n* Altan et al. [2021] Ege Altan, Sara A. Solla, Lee E. Miller, and Eric J. Perreault. Estimating the dimensionality of the manifold underlying multi-electrode neural recordings. _PLoS Computational Biology_, 17(11):e1008591, 2021. doi: 10.1371/journal.pcbi.1008591.\n* Papyan et al. [2020] Vardan Papyan, X. Y. Han, and David L. Donoho. Prevalence of neural collapse during the terminal phase of deep learning training. _Proceedings of the National Academy of Sciences_, 117(40):24652-24663, 2020. doi: 10.1073/pnas.2015509117.\n* Fang et al. [2021] Cong Fang, Hangfeng He, Qi Long, and Weijie J. Su. Exploring deep neural networks via layer-peeled model: Minority collapse in imbalanced training. _Proceedings of the National Academy of Sciences_, 118(43):e2103091118, 2021. doi: 10.1073/pnas.2103091118.\n* Zhu et al. [2021] Zhihui Zhu, Tianyu Ding, Jinxin Zhou, Xiao Li, Chong You, Jeremias Sulam, and Qing Qu. A geometric analysis of neural collapse with unconstrained features. In _Advances in Neural Information Processing Systems_, volume 34 of _NeurIPS_, pages 29820-29834, 2021.\n* Graves et al. [2014] Alex Graves, Greg Wayne, and Ivo Danihelka. Neural turing machines, 2014.\n* Graves et al. [2016] Alex Graves, Greg Wayne, Malcolm Reynolds, Tim Harley, Ivo Danihelka, Agnieszka Grabska-Barwinska, Sergio Gomez Colmenarejo, Edward Grefenstette, Tiago Ramalho, John Agapiou, et al. Hybrid computing using a neural network with dynamic external memory. _Nature_, 538(7626):471-476, 2016.\n* [85] Lukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In _ICLR_, 2016.\n* [86] Jonas Geiping, Sean McLeish, Neel Jain, John Kirchenbauer, Siddharth Singh, Brian R. Bartoldson, Bhavya Kailkhura, Abhinav Bhatele, and Tom Goldstein. Scaling up test-time compute with latent reasoning: A recurrent depth approach, 2025.\n* [87] Tiedong Liu and Kian Hsiang Low. Goat: Fine-tuned llama outperforms gpt-4 on arithmetic tasks. _ArXiv_, abs/2305.14201, 2023.\n* [88] Alex Graves. Adaptive computation time for recurrent neural networks. _ArXiv_, abs/1603.08983, 2016.\n* [89] Andrea Banino, Jan Balaguer, and Charles Blundell. Pondernet: Learning to ponder. _ArXiv_, abs/2107.05407, 2021.\n* [90] Chris Eliasmith, Terrence C Stewart, Xuan Choo, Trevor Bekolay, Travis DeWolf, Yichuan Tang, and Daniel Rasmussen. A large-scale model of the functioning brain. _science_, 338 (6111):1202-1205, 2012.\n* [91] James CR Whittington, Timothy H Muller, Shirley Mark, Guifen Chen, Caswell Barry, Neil Burgess, and Timothy EJ Behrens. The tolman-eichenbaum machine: unifying space and relational memory through generalization in the hippocampal formation. _Cell_, 183(5):1249-1263, 2020.\n* [92] Lars Buesing, Johannes Bill, Bernhard Nessler, and Wolfgang Maass. Neural dynamics as sampling: a model for stochastic computation in recurrent networks of spiking neurons. _PLoS computational biology_, 7(11):e1002211, 2011.\n* [93] Salah Hihi and Yoshua Bengio. Hierarchical recurrent neural networks for long-term dependencies. In D. Touretzky, M.C. Mozer, and M. Hasselmo, editors, _Advances in Neural Information Processing Systems_, volume 8. MIT Press, 1995.\n* [94] Jan Koutnik, Klaus Greff, Faustino J. Gomez, and Jurgen Schmidhuber. A clockwork rnn. In _International Conference on Machine Learning_, 2014.\n* [95] Mostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob Uszkoreit, and Lukasz Kaiser. Universal transformers, 2018. arXiv preprint arXiv:1807.03819.\n* [96] Yiping Wang, Qing Yang, Zhiyuan Zeng, Liliang Ren, Lucas Liu, Baolin Peng, Hao Cheng, Xuehai He, Kuan Wang, Jianfeng Gao, Weizhu Chen, Shuohang Wang, Simon Shaolei Du, and Yelong Shen. Reinforcement learning for reasoning in large language models with one training example, 2025. URL https://arxiv.org/abs/2504.20571.\n* [97] Niklas Muennighoff. s1: Simple test-time scaling. _arXiv preprint arXiv:2502.23456_, 2025.\n* [98] Liang Wen, Yunke Cai, Fenrui Xiao, Xin He, Qi An, Zhenyu Duan, Yimin Du, Junchen Liu, Lifu Tang, Xiaowei Lv, Haosheng Zou, Yongchao Deng, Shousheng Jia, and Xiangzheng Zhang. Light-r1: Curriculum sft, dpo and rl for long cot from scratch and beyond, 2025.\n* [99] Xuefeng Li, Haoyang Zou, and Pengfei Liu. Limr: Less is more for rl scaling, 2025.\n* [100] Tri Dao and Albert Gu. Transformers are ssms: Generalized models and efficient algorithms through structured state space duality. _ArXiv_, abs/2405.21060, 2024.\n* [101] Han Guo, Songlin Yang, Tarushii Goel, Eric P Xing, Tri Dao, and Yoon Kim. Log-linear attention. _arXiv preprint arXiv:2506.04761_, 2025.", "level": 1, "page_numbers": [], "subsections": [], "references": [], "equations": [], "images": [], "tables": []}], "images": [{"image_id": "img_4_0", "page_number": 5, "bbox": [105.40464782714844, 193.83468627929688, 107.69548034667969, 241.41343688964844], "image_path": "data/temp_downloads/420c43a510576ebeecd5aca722620763_images/img_4_0.png", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 13, "height": 270, "format": "PNG", "file_size": 1085}, {"image_id": "img_4_1", "page_number": 5, "bbox": [107.69548034667969, 194.36334228515625, 109.98631286621094, 240.88479614257812], "image_path": "data/temp_downloads/420c43a510576ebeecd5aca722620763_images/img_4_1.png", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 13, "height": 264, "format": "PNG", "file_size": 986}, {"image_id": "img_4_2", "page_number": 5, "bbox": [261.35723876953125, 193.83468627929688, 263.6480712890625, 241.41343688964844], "image_path": "data/temp_downloads/420c43a510576ebeecd5aca722620763_images/img_4_2.png", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 13, "height": 270, "format": "PNG", "file_size": 954}, {"image_id": "img_4_3", "page_number": 5, "bbox": [417.3097839355469, 193.83468627929688, 419.6006164550781, 241.41343688964844], "image_path": "data/temp_downloads/420c43a510576ebeecd5aca722620763_images/img_4_3.png", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 13, "height": 270, "format": "PNG", "file_size": 1224}, {"image_id": "img_9_0", "page_number": 10, "bbox": [299.616455078125, 67.16817474365234, 378.818603515625, 68.36820983886719], "image_path": "data/temp_downloads/420c43a510576ebeecd5aca722620763_images/img_9_0.png", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 826}, {"image_id": "img_9_1", "page_number": 10, "bbox": [299.616455078125, 68.36820220947266, 378.818603515625, 69.5682373046875], "image_path": "data/temp_downloads/420c43a510576ebeecd5aca722620763_images/img_9_1.png", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 618}, {"image_id": "img_9_2", "page_number": 10, "bbox": [299.616455078125, 79.16849517822266, 378.818603515625, 80.3685302734375], "image_path": "data/temp_downloads/420c43a510576ebeecd5aca722620763_images/img_9_2.png", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 778}, {"image_id": "img_9_3", "page_number": 10, "bbox": [299.616455078125, 200.75363159179688, 378.818603515625, 201.95367431640625], "image_path": "data/temp_downloads/420c43a510576ebeecd5aca722620763_images/img_9_3.png", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 401}, {"image_id": "img_9_4", "page_number": 10, "bbox": [299.616455078125, 201.9536590576172, 378.818603515625, 203.15370178222656], "image_path": "data/temp_downloads/420c43a510576ebeecd5aca722620763_images/img_9_4.png", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 1361}, {"image_id": "img_9_5", "page_number": 10, "bbox": [299.616455078125, 203.1536865234375, 378.818603515625, 204.35372924804688], "image_path": "data/temp_downloads/420c43a510576ebeecd5aca722620763_images/img_9_5.png", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 453}, {"image_id": "img_9_6", "page_number": 10, "bbox": [299.616455078125, 204.35372924804688, 378.818603515625, 205.55377197265625], "image_path": "data/temp_downloads/420c43a510576ebeecd5aca722620763_images/img_9_6.png", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 1529}, {"image_id": "img_9_7", "page_number": 10, "bbox": [299.616455078125, 205.55377197265625, 378.818603515625, 206.75381469726562], "image_path": "data/temp_downloads/420c43a510576ebeecd5aca722620763_images/img_9_7.png", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 423}, {"image_id": "img_9_8", "page_number": 10, "bbox": [299.616455078125, 206.7537841796875, 378.818603515625, 207.95382690429688], "image_path": "data/temp_downloads/420c43a510576ebeecd5aca722620763_images/img_9_8.png", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 1384}, {"image_id": "img_9_9", "page_number": 10, "bbox": [299.616455078125, 207.95382690429688, 378.818603515625, 209.15386962890625], "image_path": "data/temp_downloads/420c43a510576ebeecd5aca722620763_images/img_9_9.png", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 415}, {"image_id": "img_9_10", "page_number": 10, "bbox": [299.616455078125, 209.1538543701172, 378.818603515625, 210.35389709472656], "image_path": "data/temp_downloads/420c43a510576ebeecd5aca722620763_images/img_9_10.png", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 1379}, {"image_id": "img_9_11", "page_number": 10, "bbox": [299.616455078125, 210.35389709472656, 378.818603515625, 211.55393981933594], "image_path": "data/temp_downloads/420c43a510576ebeecd5aca722620763_images/img_9_11.png", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 383}, {"image_id": "img_9_12", "page_number": 10, "bbox": [299.616455078125, 211.55392456054688, 378.818603515625, 212.75396728515625], "image_path": "data/temp_downloads/420c43a510576ebeecd5aca722620763_images/img_9_12.png", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 1367}, {"image_id": "img_9_13", "page_number": 10, "bbox": [299.616455078125, 80.36853790283203, 378.818603515625, 81.56857299804688], "image_path": "data/temp_downloads/420c43a510576ebeecd5aca722620763_images/img_9_13.png", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 821}, {"image_id": "img_9_14", "page_number": 10, "bbox": [299.616455078125, 212.7539520263672, 378.818603515625, 213.95399475097656], "image_path": "data/temp_downloads/420c43a510576ebeecd5aca722620763_images/img_9_14.png", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 415}, {"image_id": "img_9_15", "page_number": 10, "bbox": [299.616455078125, 213.95401000976562, 378.818603515625, 215.154052734375], "image_path": "data/temp_downloads/420c43a510576ebeecd5aca722620763_images/img_9_15.png", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 1403}, {"image_id": "img_9_16", "page_number": 10, "bbox": [299.616455078125, 215.15402221679688, 378.818603515625, 216.35406494140625], "image_path": "data/temp_downloads/420c43a510576ebeecd5aca722620763_images/img_9_16.png", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 420}, {"image_id": "img_9_17", "page_number": 10, "bbox": [299.616455078125, 216.35406494140625, 378.818603515625, 217.55410766601562], "image_path": "data/temp_downloads/420c43a510576ebeecd5aca722620763_images/img_9_17.png", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 1483}, {"image_id": "img_9_18", "page_number": 10, "bbox": [299.616455078125, 217.55409240722656, 378.818603515625, 218.75413513183594], "image_path": "data/temp_downloads/420c43a510576ebeecd5aca722620763_images/img_9_18.png", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 412}, {"image_id": "img_9_19", "page_number": 10, "bbox": [299.616455078125, 218.75411987304688, 378.818603515625, 219.95416259765625], "image_path": "data/temp_downloads/420c43a510576ebeecd5aca722620763_images/img_9_19.png", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 1425}, {"image_id": "img_9_20", "page_number": 10, "bbox": [299.616455078125, 219.9541473388672, 378.818603515625, 221.15419006347656], "image_path": "data/temp_downloads/420c43a510576ebeecd5aca722620763_images/img_9_20.png", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 432}, {"image_id": "img_9_21", "page_number": 10, "bbox": [299.616455078125, 221.15419006347656, 378.818603515625, 222.35423278808594], "image_path": "data/temp_downloads/420c43a510576ebeecd5aca722620763_images/img_9_21.png", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 1301}, {"image_id": "img_9_22", "page_number": 10, "bbox": [299.616455078125, 222.35421752929688, 378.818603515625, 223.55426025390625], "image_path": "data/temp_downloads/420c43a510576ebeecd5aca722620763_images/img_9_22.png", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 408}, {"image_id": "img_9_23", "page_number": 10, "bbox": [299.616455078125, 223.5542755126953, 378.818603515625, 224.20883178710938], "image_path": "data/temp_downloads/420c43a510576ebeecd5aca722620763_images/img_9_23.png", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 3, "format": "PNG", "file_size": 753}, {"image_id": "img_9_24", "page_number": 10, "bbox": [299.616455078125, 81.56856536865234, 378.818603515625, 82.76860046386719], "image_path": "data/temp_downloads/420c43a510576ebeecd5aca722620763_images/img_9_24.png", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 820}, {"image_id": "img_9_25", "page_number": 10, "bbox": [73.57389831542969, 67.22271728515625, 121.35700988769531, 75.18656921386719], "image_path": "data/temp_downloads/420c43a510576ebeecd5aca722620763_images/img_9_25.png", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 219, "height": 37, "format": "PNG", "file_size": 9937}, {"image_id": "img_9_26", "page_number": 10, "bbox": [73.57389831542969, 75.18656921386719, 121.35700988769531, 83.15042114257812], "image_path": "data/temp_downloads/420c43a510576ebeecd5aca722620763_images/img_9_26.png", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 219, "height": 37, "format": "PNG", "file_size": 9974}, {"image_id": "img_9_27", "page_number": 10, "bbox": [73.57389831542969, 83.15043640136719, 121.35700988769531, 91.11428833007812], "image_path": "data/temp_downloads/420c43a510576ebeecd5aca722620763_images/img_9_27.png", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 219, "height": 37, "format": "PNG", "file_size": 8809}, {"image_id": "img_9_28", "page_number": 10, "bbox": [73.57389831542969, 91.11427307128906, 121.35700988769531, 99.078125], "image_path": "data/temp_downloads/420c43a510576ebeecd5aca722620763_images/img_9_28.png", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 219, "height": 37, "format": "PNG", "file_size": 8582}, {"image_id": "img_9_29", "page_number": 10, "bbox": [73.57389831542969, 99.078125, 121.35700988769531, 107.04197692871094], "image_path": "data/temp_downloads/420c43a510576ebeecd5aca722620763_images/img_9_29.png", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 219, "height": 37, "format": "PNG", "file_size": 8139}, {"image_id": "img_9_30", "page_number": 10, "bbox": [73.57389831542969, 107.0419921875, 121.35700988769531, 114.89674377441406], "image_path": "data/temp_downloads/420c43a510576ebeecd5aca722620763_images/img_9_30.png", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 219, "height": 36, "format": "PNG", "file_size": 8875}, {"image_id": "img_9_31", "page_number": 10, "bbox": [133.03005981445312, 67.22271728515625, 181.0313720703125, 75.18656921386719], "image_path": "data/temp_downloads/420c43a510576ebeecd5aca722620763_images/img_9_31.png", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 220, "height": 37, "format": "PNG", "file_size": 1794}, {"image_id": "img_9_32", "page_number": 10, "bbox": [133.03005981445312, 75.18656921386719, 181.0313720703125, 83.15042114257812], "image_path": "data/temp_downloads/420c43a510576ebeecd5aca722620763_images/img_9_32.png", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 220, "height": 37, "format": "PNG", "file_size": 9350}, {"image_id": "img_9_33", "page_number": 10, "bbox": [133.03005981445312, 83.15043640136719, 181.0313720703125, 91.11428833007812], "image_path": "data/temp_downloads/420c43a510576ebeecd5aca722620763_images/img_9_33.png", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 220, "height": 37, "format": "PNG", "file_size": 8114}, {"image_id": "img_9_34", "page_number": 10, "bbox": [133.03005981445312, 91.11427307128906, 181.0313720703125, 99.078125], "image_path": "data/temp_downloads/420c43a510576ebeecd5aca722620763_images/img_9_34.png", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 220, "height": 37, "format": "PNG", "file_size": 10181}, {"image_id": "img_9_35", "page_number": 10, "bbox": [299.616455078125, 82.76860809326172, 378.818603515625, 83.96864318847656], "image_path": "data/temp_downloads/420c43a510576ebeecd5aca722620763_images/img_9_35.png", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 891}, {"image_id": "img_9_36", "page_number": 10, "bbox": [133.03005981445312, 99.078125, 181.0313720703125, 107.04197692871094], "image_path": "data/temp_downloads/420c43a510576ebeecd5aca722620763_images/img_9_36.png", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 220, "height": 37, "format": "PNG", "file_size": 9533}, {"image_id": "img_9_37", "page_number": 10, "bbox": [133.03005981445312, 107.0419921875, 181.0313720703125, 114.89674377441406], "image_path": "data/temp_downloads/420c43a510576ebeecd5aca722620763_images/img_9_37.png", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 220, "height": 36, "format": "PNG", "file_size": 1659}, {"image_id": "img_9_38", "page_number": 10, "bbox": [73.57389831542969, 121.66056823730469, 121.35700988769531, 129.62442016601562], "image_path": "data/temp_downloads/420c43a510576ebeecd5aca722620763_images/img_9_38.png", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 219, "height": 37, "format": "PNG", "file_size": 2179}, {"image_id": "img_9_39", "page_number": 10, "bbox": [73.57389831542969, 129.62440490722656, 121.35700988769531, 137.5882568359375], "image_path": "data/temp_downloads/420c43a510576ebeecd5aca722620763_images/img_9_39.png", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 219, "height": 37, "format": "PNG", "file_size": 8481}, {"image_id": "img_9_40", "page_number": 10, "bbox": [73.57389831542969, 137.58827209472656, 121.35700988769531, 145.5521240234375], "image_path": "data/temp_downloads/420c43a510576ebeecd5aca722620763_images/img_9_40.png", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 219, "height": 37, "format": "PNG", "file_size": 2011}, {"image_id": "img_9_41", "page_number": 10, "bbox": [73.57389831542969, 145.55213928222656, 121.35700988769531, 153.5159912109375], "image_path": "data/temp_downloads/420c43a510576ebeecd5aca722620763_images/img_9_41.png", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 219, "height": 37, "format": "PNG", "file_size": 14098}, {"image_id": "img_9_42", "page_number": 10, "bbox": [73.57389831542969, 153.51597595214844, 121.35700988769531, 161.47982788085938], "image_path": "data/temp_downloads/420c43a510576ebeecd5aca722620763_images/img_9_42.png", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 219, "height": 37, "format": "PNG", "file_size": 10140}, {"image_id": "img_9_43", "page_number": 10, "bbox": [73.57389831542969, 161.47984313964844, 121.35700988769531, 169.3345947265625], "image_path": "data/temp_downloads/420c43a510576ebeecd5aca722620763_images/img_9_43.png", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 219, "height": 36, "format": "PNG", "file_size": 7653}, {"image_id": "img_9_44", "page_number": 10, "bbox": [133.03005981445312, 121.66056823730469, 181.0313720703125, 129.62442016601562], "image_path": "data/temp_downloads/420c43a510576ebeecd5aca722620763_images/img_9_44.png", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 220, "height": 37, "format": "PNG", "file_size": 1858}, {"image_id": "img_9_45", "page_number": 10, "bbox": [133.03005981445312, 129.62440490722656, 181.0313720703125, 137.5882568359375], "image_path": "data/temp_downloads/420c43a510576ebeecd5aca722620763_images/img_9_45.png", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 220, "height": 37, "format": "PNG", "file_size": 10685}, {"image_id": "img_9_46", "page_number": 10, "bbox": [299.616455078125, 83.96863555908203, 378.818603515625, 85.16867065429688], "image_path": "data/temp_downloads/420c43a510576ebeecd5aca722620763_images/img_9_46.png", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 611}, {"image_id": "img_9_47", "page_number": 10, "bbox": [133.03005981445312, 137.58827209472656, 181.0313720703125, 145.5521240234375], "image_path": "data/temp_downloads/420c43a510576ebeecd5aca722620763_images/img_9_47.png", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 220, "height": 37, "format": "PNG", "file_size": 10767}, {"image_id": "img_9_48", "page_number": 10, "bbox": [133.03005981445312, 145.55213928222656, 181.0313720703125, 153.5159912109375], "image_path": "data/temp_downloads/420c43a510576ebeecd5aca722620763_images/img_9_48.png", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 220, "height": 37, "format": "PNG", "file_size": 10776}, {"image_id": "img_9_49", "page_number": 10, "bbox": [133.03005981445312, 153.51597595214844, 181.0313720703125, 161.47982788085938], "image_path": "data/temp_downloads/420c43a510576ebeecd5aca722620763_images/img_9_49.png", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 220, "height": 37, "format": "PNG", "file_size": 2133}, {"image_id": "img_9_50", "page_number": 10, "bbox": [133.03005981445312, 161.47984313964844, 181.0313720703125, 169.3345947265625], "image_path": "data/temp_downloads/420c43a510576ebeecd5aca722620763_images/img_9_50.png", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 220, "height": 36, "format": "PNG", "file_size": 1971}, {"image_id": "img_9_51", "page_number": 10, "bbox": [73.57389831542969, 176.86207580566406, 121.35700988769531, 184.825927734375], "image_path": "data/temp_downloads/420c43a510576ebeecd5aca722620763_images/img_9_51.png", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 219, "height": 37, "format": "PNG", "file_size": 12851}, {"image_id": "img_9_52", "page_number": 10, "bbox": [73.57389831542969, 184.82591247558594, 121.35700988769531, 192.78976440429688], "image_path": "data/temp_downloads/420c43a510576ebeecd5aca722620763_images/img_9_52.png", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 219, "height": 37, "format": "PNG", "file_size": 8897}, {"image_id": "img_9_53", "page_number": 10, "bbox": [73.57389831542969, 192.78977966308594, 121.35700988769531, 200.75363159179688], "image_path": "data/temp_downloads/420c43a510576ebeecd5aca722620763_images/img_9_53.png", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 219, "height": 37, "format": "PNG", "file_size": 13152}, {"image_id": "img_9_54", "page_number": 10, "bbox": [73.57389831542969, 200.75363159179688, 121.35700988769531, 208.7174835205078], "image_path": "data/temp_downloads/420c43a510576ebeecd5aca722620763_images/img_9_54.png", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 219, "height": 37, "format": "PNG", "file_size": 11724}, {"image_id": "img_9_55", "page_number": 10, "bbox": [73.57389831542969, 208.7174835205078, 121.35700988769531, 216.68133544921875], "image_path": "data/temp_downloads/420c43a510576ebeecd5aca722620763_images/img_9_55.png", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 219, "height": 37, "format": "PNG", "file_size": 9699}, {"image_id": "img_9_56", "page_number": 10, "bbox": [73.57389831542969, 216.6813507080078, 121.35700988769531, 224.64520263671875], "image_path": "data/temp_downloads/420c43a510576ebeecd5aca722620763_images/img_9_56.png", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 219, "height": 37, "format": "PNG", "file_size": 10934}, {"image_id": "img_9_57", "page_number": 10, "bbox": [299.616455078125, 85.16866302490234, 378.818603515625, 86.36869812011719], "image_path": "data/temp_downloads/420c43a510576ebeecd5aca722620763_images/img_9_57.png", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 873}, {"image_id": "img_9_58", "page_number": 10, "bbox": [133.03005981445312, 176.86207580566406, 181.0313720703125, 184.825927734375], "image_path": "data/temp_downloads/420c43a510576ebeecd5aca722620763_images/img_9_58.png", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 220, "height": 37, "format": "PNG", "file_size": 494}, {"image_id": "img_9_59", "page_number": 10, "bbox": [133.03005981445312, 184.82591247558594, 181.0313720703125, 192.78976440429688], "image_path": "data/temp_downloads/420c43a510576ebeecd5aca722620763_images/img_9_59.png", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 220, "height": 37, "format": "PNG", "file_size": 727}, {"image_id": "img_9_60", "page_number": 10, "bbox": [133.03005981445312, 192.78977966308594, 181.0313720703125, 200.75363159179688], "image_path": "data/temp_downloads/420c43a510576ebeecd5aca722620763_images/img_9_60.png", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 220, "height": 37, "format": "PNG", "file_size": 1601}, {"image_id": "img_9_61", "page_number": 10, "bbox": [133.03005981445312, 200.75363159179688, 181.0313720703125, 208.7174835205078], "image_path": "data/temp_downloads/420c43a510576ebeecd5aca722620763_images/img_9_61.png", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 220, "height": 37, "format": "PNG", "file_size": 1134}, {"image_id": "img_9_62", "page_number": 10, "bbox": [133.03005981445312, 208.7174835205078, 181.0313720703125, 216.68133544921875], "image_path": "data/temp_downloads/420c43a510576ebeecd5aca722620763_images/img_9_62.png", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 220, "height": 37, "format": "PNG", "file_size": 313}, {"image_id": "img_9_63", "page_number": 10, "bbox": [133.03005981445312, 216.6813507080078, 181.0313720703125, 224.20883178710938], "image_path": "data/temp_downloads/420c43a510576ebeecd5aca722620763_images/img_9_63.png", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 220, "height": 35, "format": "PNG", "file_size": 397}, {"image_id": "img_9_64", "page_number": 10, "bbox": [405.8193359375, 64.65902709960938, 534.0592041015625, 226.82708740234375], "image_path": "data/temp_downloads/420c43a510576ebeecd5aca722620763_images/img_9_64.png", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 431, "height": 545, "format": "PNG", "file_size": 34955}, {"image_id": "img_9_65", "page_number": 10, "bbox": [299.616455078125, 86.36869049072266, 378.818603515625, 87.5687255859375], "image_path": "data/temp_downloads/420c43a510576ebeecd5aca722620763_images/img_9_65.png", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 614}, {"image_id": "img_9_66", "page_number": 10, "bbox": [299.616455078125, 87.56873321533203, 378.818603515625, 88.76876831054688], "image_path": "data/temp_downloads/420c43a510576ebeecd5aca722620763_images/img_9_66.png", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 932}, {"image_id": "img_9_67", "page_number": 10, "bbox": [299.616455078125, 88.76876068115234, 378.818603515625, 89.96879577636719], "image_path": "data/temp_downloads/420c43a510576ebeecd5aca722620763_images/img_9_67.png", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 602}, {"image_id": "img_9_68", "page_number": 10, "bbox": [299.616455078125, 89.96878814697266, 378.818603515625, 91.1688232421875], "image_path": "data/temp_downloads/420c43a510576ebeecd5aca722620763_images/img_9_68.png", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 920}, {"image_id": "img_9_69", "page_number": 10, "bbox": [299.616455078125, 69.56822967529297, 378.818603515625, 70.76826477050781], "image_path": "data/temp_downloads/420c43a510576ebeecd5aca722620763_images/img_9_69.png", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 865}, {"image_id": "img_9_70", "page_number": 10, "bbox": [299.616455078125, 91.16883087158203, 378.818603515625, 92.36886596679688], "image_path": "data/temp_downloads/420c43a510576ebeecd5aca722620763_images/img_9_70.png", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 413}, {"image_id": "img_9_71", "page_number": 10, "bbox": [299.616455078125, 92.36885833740234, 378.818603515625, 93.56889343261719], "image_path": "data/temp_downloads/420c43a510576ebeecd5aca722620763_images/img_9_71.png", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 946}, {"image_id": "img_9_72", "page_number": 10, "bbox": [299.616455078125, 93.56888580322266, 378.818603515625, 94.7689208984375], "image_path": "data/temp_downloads/420c43a510576ebeecd5aca722620763_images/img_9_72.png", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 608}, {"image_id": "img_9_73", "page_number": 10, "bbox": [299.616455078125, 94.76892852783203, 378.818603515625, 95.96896362304688], "image_path": "data/temp_downloads/420c43a510576ebeecd5aca722620763_images/img_9_73.png", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 975}, {"image_id": "img_9_74", "page_number": 10, "bbox": [299.616455078125, 95.96895599365234, 378.818603515625, 97.16899108886719], "image_path": "data/temp_downloads/420c43a510576ebeecd5aca722620763_images/img_9_74.png", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 402}, {"image_id": "img_9_75", "page_number": 10, "bbox": [299.616455078125, 97.16899871826172, 378.818603515625, 98.36903381347656], "image_path": "data/temp_downloads/420c43a510576ebeecd5aca722620763_images/img_9_75.png", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 1024}, {"image_id": "img_9_76", "page_number": 10, "bbox": [299.616455078125, 98.36901092529297, 378.818603515625, 99.56904602050781], "image_path": "data/temp_downloads/420c43a510576ebeecd5aca722620763_images/img_9_76.png", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 393}, {"image_id": "img_9_77", "page_number": 10, "bbox": [299.616455078125, 99.56905364990234, 378.818603515625, 100.76908874511719], "image_path": "data/temp_downloads/420c43a510576ebeecd5aca722620763_images/img_9_77.png", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 878}, {"image_id": "img_9_78", "page_number": 10, "bbox": [299.616455078125, 100.76909637451172, 378.818603515625, 101.96913146972656], "image_path": "data/temp_downloads/420c43a510576ebeecd5aca722620763_images/img_9_78.png", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 403}, {"image_id": "img_9_79", "page_number": 10, "bbox": [299.616455078125, 101.96912384033203, 378.818603515625, 103.16915893554688], "image_path": "data/temp_downloads/420c43a510576ebeecd5aca722620763_images/img_9_79.png", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 1064}, {"image_id": "img_9_80", "page_number": 10, "bbox": [299.616455078125, 70.7682876586914, 378.818603515625, 71.96832275390625], "image_path": "data/temp_downloads/420c43a510576ebeecd5aca722620763_images/img_9_80.png", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 555}, {"image_id": "img_9_81", "page_number": 10, "bbox": [299.616455078125, 103.16915130615234, 378.818603515625, 104.36918640136719], "image_path": "data/temp_downloads/420c43a510576ebeecd5aca722620763_images/img_9_81.png", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 408}, {"image_id": "img_9_82", "page_number": 10, "bbox": [299.616455078125, 104.36919403076172, 378.818603515625, 105.56922912597656], "image_path": "data/temp_downloads/420c43a510576ebeecd5aca722620763_images/img_9_82.png", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 1051}, {"image_id": "img_9_83", "page_number": 10, "bbox": [299.616455078125, 105.56922149658203, 378.818603515625, 106.76925659179688], "image_path": "data/temp_downloads/420c43a510576ebeecd5aca722620763_images/img_9_83.png", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 403}, {"image_id": "img_9_84", "page_number": 10, "bbox": [299.616455078125, 106.76924896240234, 378.818603515625, 107.96928405761719], "image_path": "data/temp_downloads/420c43a510576ebeecd5aca722620763_images/img_9_84.png", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 1050}, {"image_id": "img_9_85", "page_number": 10, "bbox": [299.616455078125, 107.96929168701172, 378.818603515625, 109.16932678222656], "image_path": "data/temp_downloads/420c43a510576ebeecd5aca722620763_images/img_9_85.png", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 408}, {"image_id": "img_9_86", "page_number": 10, "bbox": [299.616455078125, 109.16931915283203, 378.818603515625, 110.36935424804688], "image_path": "data/temp_downloads/420c43a510576ebeecd5aca722620763_images/img_9_86.png", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 1053}, {"image_id": "img_9_87", "page_number": 10, "bbox": [299.616455078125, 110.3693618774414, 378.818603515625, 111.56939697265625], "image_path": "data/temp_downloads/420c43a510576ebeecd5aca722620763_images/img_9_87.png", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 383}, {"image_id": "img_9_88", "page_number": 10, "bbox": [299.616455078125, 111.56937408447266, 378.818603515625, 112.7694091796875], "image_path": "data/temp_downloads/420c43a510576ebeecd5aca722620763_images/img_9_88.png", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 1229}, {"image_id": "img_9_89", "page_number": 10, "bbox": [299.616455078125, 112.7694320678711, 378.818603515625, 113.96946716308594], "image_path": "data/temp_downloads/420c43a510576ebeecd5aca722620763_images/img_9_89.png", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 395}, {"image_id": "img_9_90", "page_number": 10, "bbox": [299.616455078125, 113.96944427490234, 378.818603515625, 115.16947937011719], "image_path": "data/temp_downloads/420c43a510576ebeecd5aca722620763_images/img_9_90.png", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 1171}, {"image_id": "img_9_91", "page_number": 10, "bbox": [299.616455078125, 71.96829986572266, 378.818603515625, 73.1683349609375], "image_path": "data/temp_downloads/420c43a510576ebeecd5aca722620763_images/img_9_91.png", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 891}, {"image_id": "img_9_92", "page_number": 10, "bbox": [299.616455078125, 115.16948699951172, 378.818603515625, 116.36952209472656], "image_path": "data/temp_downloads/420c43a510576ebeecd5aca722620763_images/img_9_92.png", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 406}, {"image_id": "img_9_93", "page_number": 10, "bbox": [299.616455078125, 116.36949920654297, 378.818603515625, 117.56953430175781], "image_path": "data/temp_downloads/420c43a510576ebeecd5aca722620763_images/img_9_93.png", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 1278}, {"image_id": "img_9_94", "page_number": 10, "bbox": [299.616455078125, 117.5695571899414, 378.818603515625, 118.76959228515625], "image_path": "data/temp_downloads/420c43a510576ebeecd5aca722620763_images/img_9_94.png", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 434}, {"image_id": "img_9_95", "page_number": 10, "bbox": [299.616455078125, 118.76958465576172, 378.818603515625, 119.96961975097656], "image_path": "data/temp_downloads/420c43a510576ebeecd5aca722620763_images/img_9_95.png", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 1313}, {"image_id": "img_9_96", "page_number": 10, "bbox": [299.616455078125, 119.96961212158203, 378.818603515625, 121.16964721679688], "image_path": "data/temp_downloads/420c43a510576ebeecd5aca722620763_images/img_9_96.png", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 416}, {"image_id": "img_9_97", "page_number": 10, "bbox": [299.616455078125, 121.1696548461914, 378.818603515625, 122.36968994140625], "image_path": "data/temp_downloads/420c43a510576ebeecd5aca722620763_images/img_9_97.png", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 1274}, {"image_id": "img_9_98", "page_number": 10, "bbox": [299.616455078125, 122.36968231201172, 378.818603515625, 123.56971740722656], "image_path": "data/temp_downloads/420c43a510576ebeecd5aca722620763_images/img_9_98.png", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 404}, {"image_id": "img_9_99", "page_number": 10, "bbox": [299.616455078125, 123.56970977783203, 378.818603515625, 124.76974487304688], "image_path": "data/temp_downloads/420c43a510576ebeecd5aca722620763_images/img_9_99.png", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 1244}, {"image_id": "img_9_100", "page_number": 10, "bbox": [299.616455078125, 124.76973724365234, 378.818603515625, 125.96977233886719], "image_path": "data/temp_downloads/420c43a510576ebeecd5aca722620763_images/img_9_100.png", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 389}, {"image_id": "img_9_101", "page_number": 10, "bbox": [299.616455078125, 125.96977996826172, 378.818603515625, 127.16981506347656], "image_path": "data/temp_downloads/420c43a510576ebeecd5aca722620763_images/img_9_101.png", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 1262}, {"image_id": "img_9_102", "page_number": 10, "bbox": [299.616455078125, 73.16834259033203, 378.818603515625, 74.36837768554688], "image_path": "data/temp_downloads/420c43a510576ebeecd5aca722620763_images/img_9_102.png", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 617}, {"image_id": "img_9_103", "page_number": 10, "bbox": [299.616455078125, 127.16980743408203, 378.818603515625, 128.36984252929688], "image_path": "data/temp_downloads/420c43a510576ebeecd5aca722620763_images/img_9_103.png", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 414}, {"image_id": "img_9_104", "page_number": 10, "bbox": [299.616455078125, 128.3698272705078, 378.818603515625, 129.5698699951172], "image_path": "data/temp_downloads/420c43a510576ebeecd5aca722620763_images/img_9_104.png", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 1335}, {"image_id": "img_9_105", "page_number": 10, "bbox": [299.616455078125, 129.56985473632812, 378.818603515625, 130.7698974609375], "image_path": "data/temp_downloads/420c43a510576ebeecd5aca722620763_images/img_9_105.png", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 422}, {"image_id": "img_9_106", "page_number": 10, "bbox": [299.616455078125, 130.76991271972656, 378.818603515625, 131.96995544433594], "image_path": "data/temp_downloads/420c43a510576ebeecd5aca722620763_images/img_9_106.png", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 1337}, {"image_id": "img_9_107", "page_number": 10, "bbox": [299.616455078125, 131.96994018554688, 378.818603515625, 133.16998291015625], "image_path": "data/temp_downloads/420c43a510576ebeecd5aca722620763_images/img_9_107.png", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 431}, {"image_id": "img_9_108", "page_number": 10, "bbox": [299.616455078125, 133.1699676513672, 378.818603515625, 134.37001037597656], "image_path": "data/temp_downloads/420c43a510576ebeecd5aca722620763_images/img_9_108.png", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 1460}, {"image_id": "img_9_109", "page_number": 10, "bbox": [299.616455078125, 134.37001037597656, 378.818603515625, 135.57005310058594], "image_path": "data/temp_downloads/420c43a510576ebeecd5aca722620763_images/img_9_109.png", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 424}, {"image_id": "img_9_110", "page_number": 10, "bbox": [299.616455078125, 135.57003784179688, 378.818603515625, 136.77008056640625], "image_path": "data/temp_downloads/420c43a510576ebeecd5aca722620763_images/img_9_110.png", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 1300}, {"image_id": "img_9_111", "page_number": 10, "bbox": [299.616455078125, 136.7700653076172, 378.818603515625, 137.97010803222656], "image_path": "data/temp_downloads/420c43a510576ebeecd5aca722620763_images/img_9_111.png", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 381}, {"image_id": "img_9_112", "page_number": 10, "bbox": [299.616455078125, 137.97012329101562, 378.818603515625, 138.57012939453125], "image_path": "data/temp_downloads/420c43a510576ebeecd5aca722620763_images/img_9_112.png", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 3, "format": "PNG", "file_size": 687}, {"image_id": "img_9_113", "page_number": 10, "bbox": [299.616455078125, 74.36837005615234, 378.818603515625, 75.56840515136719], "image_path": "data/temp_downloads/420c43a510576ebeecd5aca722620763_images/img_9_113.png", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 938}, {"image_id": "img_9_114", "page_number": 10, "bbox": [299.616455078125, 152.7523193359375, 378.818603515625, 153.95236206054688], "image_path": "data/temp_downloads/420c43a510576ebeecd5aca722620763_images/img_9_114.png", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 936}, {"image_id": "img_9_115", "page_number": 10, "bbox": [299.616455078125, 153.95236206054688, 378.818603515625, 155.15240478515625], "image_path": "data/temp_downloads/420c43a510576ebeecd5aca722620763_images/img_9_115.png", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 861}, {"image_id": "img_9_116", "page_number": 10, "bbox": [299.616455078125, 155.15237426757812, 378.818603515625, 156.3524169921875], "image_path": "data/temp_downloads/420c43a510576ebeecd5aca722620763_images/img_9_116.png", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 851}, {"image_id": "img_9_117", "page_number": 10, "bbox": [299.616455078125, 156.3524169921875, 378.818603515625, 157.55245971679688], "image_path": "data/temp_downloads/420c43a510576ebeecd5aca722620763_images/img_9_117.png", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 598}, {"image_id": "img_9_118", "page_number": 10, "bbox": [299.616455078125, 157.55245971679688, 378.818603515625, 158.75250244140625], "image_path": "data/temp_downloads/420c43a510576ebeecd5aca722620763_images/img_9_118.png", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 925}, {"image_id": "img_9_119", "page_number": 10, "bbox": [299.616455078125, 158.75250244140625, 378.818603515625, 159.95254516601562], "image_path": "data/temp_downloads/420c43a510576ebeecd5aca722620763_images/img_9_119.png", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 853}, {"image_id": "img_9_120", "page_number": 10, "bbox": [299.616455078125, 159.9525146484375, 378.818603515625, 161.15255737304688], "image_path": "data/temp_downloads/420c43a510576ebeecd5aca722620763_images/img_9_120.png", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 922}, {"image_id": "img_9_121", "page_number": 10, "bbox": [299.616455078125, 161.15255737304688, 378.818603515625, 162.35260009765625], "image_path": "data/temp_downloads/420c43a510576ebeecd5aca722620763_images/img_9_121.png", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 969}, {"image_id": "img_9_122", "page_number": 10, "bbox": [299.616455078125, 162.3525848388672, 378.818603515625, 163.55262756347656], "image_path": "data/temp_downloads/420c43a510576ebeecd5aca722620763_images/img_9_122.png", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 656}, {"image_id": "img_9_123", "page_number": 10, "bbox": [299.616455078125, 163.5526123046875, 378.818603515625, 164.75265502929688], "image_path": "data/temp_downloads/420c43a510576ebeecd5aca722620763_images/img_9_123.png", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 932}, {"image_id": "img_9_124", "page_number": 10, "bbox": [299.616455078125, 75.56841278076172, 378.818603515625, 76.76844787597656], "image_path": "data/temp_downloads/420c43a510576ebeecd5aca722620763_images/img_9_124.png", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 883}, {"image_id": "img_9_125", "page_number": 10, "bbox": [299.616455078125, 164.7526397705078, 378.818603515625, 165.9526824951172], "image_path": "data/temp_downloads/420c43a510576ebeecd5aca722620763_images/img_9_125.png", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 819}, {"image_id": "img_9_126", "page_number": 10, "bbox": [299.616455078125, 165.95269775390625, 378.818603515625, 167.15274047851562], "image_path": "data/temp_downloads/420c43a510576ebeecd5aca722620763_images/img_9_126.png", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 890}, {"image_id": "img_9_127", "page_number": 10, "bbox": [299.616455078125, 167.1527099609375, 378.818603515625, 168.35275268554688], "image_path": "data/temp_downloads/420c43a510576ebeecd5aca722620763_images/img_9_127.png", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 650}, {"image_id": "img_9_128", "page_number": 10, "bbox": [299.616455078125, 168.35275268554688, 378.818603515625, 169.55279541015625], "image_path": "data/temp_downloads/420c43a510576ebeecd5aca722620763_images/img_9_128.png", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 1011}, {"image_id": "img_9_129", "page_number": 10, "bbox": [299.616455078125, 169.55276489257812, 378.818603515625, 170.7528076171875], "image_path": "data/temp_downloads/420c43a510576ebeecd5aca722620763_images/img_9_129.png", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 624}, {"image_id": "img_9_130", "page_number": 10, "bbox": [299.616455078125, 170.7528076171875, 378.818603515625, 171.95285034179688], "image_path": "data/temp_downloads/420c43a510576ebeecd5aca722620763_images/img_9_130.png", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 1025}, {"image_id": "img_9_131", "page_number": 10, "bbox": [299.616455078125, 171.95285034179688, 378.818603515625, 173.15289306640625], "image_path": "data/temp_downloads/420c43a510576ebeecd5aca722620763_images/img_9_131.png", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 573}, {"image_id": "img_9_132", "page_number": 10, "bbox": [299.616455078125, 173.1528778076172, 378.818603515625, 174.35292053222656], "image_path": "data/temp_downloads/420c43a510576ebeecd5aca722620763_images/img_9_132.png", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 1065}, {"image_id": "img_9_133", "page_number": 10, "bbox": [299.616455078125, 174.3529052734375, 378.818603515625, 175.55294799804688], "image_path": "data/temp_downloads/420c43a510576ebeecd5aca722620763_images/img_9_133.png", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 621}, {"image_id": "img_9_134", "page_number": 10, "bbox": [299.616455078125, 175.55294799804688, 378.818603515625, 176.75299072265625], "image_path": "data/temp_downloads/420c43a510576ebeecd5aca722620763_images/img_9_134.png", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 1014}, {"image_id": "img_9_135", "page_number": 10, "bbox": [299.616455078125, 76.76842498779297, 378.818603515625, 77.96846008300781], "image_path": "data/temp_downloads/420c43a510576ebeecd5aca722620763_images/img_9_135.png", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 850}, {"image_id": "img_9_136", "page_number": 10, "bbox": [299.616455078125, 176.75296020507812, 378.818603515625, 177.9530029296875], "image_path": "data/temp_downloads/420c43a510576ebeecd5aca722620763_images/img_9_136.png", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 427}, {"image_id": "img_9_137", "page_number": 10, "bbox": [299.616455078125, 177.9530029296875, 378.818603515625, 179.15304565429688], "image_path": "data/temp_downloads/420c43a510576ebeecd5aca722620763_images/img_9_137.png", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 995}, {"image_id": "img_9_138", "page_number": 10, "bbox": [299.616455078125, 179.15304565429688, 378.818603515625, 180.35308837890625], "image_path": "data/temp_downloads/420c43a510576ebeecd5aca722620763_images/img_9_138.png", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 653}, {"image_id": "img_9_139", "page_number": 10, "bbox": [299.616455078125, 180.3530731201172, 378.818603515625, 181.55311584472656], "image_path": "data/temp_downloads/420c43a510576ebeecd5aca722620763_images/img_9_139.png", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 1098}, {"image_id": "img_9_140", "page_number": 10, "bbox": [299.616455078125, 181.5531005859375, 378.818603515625, 182.75314331054688], "image_path": "data/temp_downloads/420c43a510576ebeecd5aca722620763_images/img_9_140.png", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 414}, {"image_id": "img_9_141", "page_number": 10, "bbox": [299.616455078125, 182.75314331054688, 378.818603515625, 183.95318603515625], "image_path": "data/temp_downloads/420c43a510576ebeecd5aca722620763_images/img_9_141.png", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 1157}, {"image_id": "img_9_142", "page_number": 10, "bbox": [299.616455078125, 183.95318603515625, 378.818603515625, 185.15322875976562], "image_path": "data/temp_downloads/420c43a510576ebeecd5aca722620763_images/img_9_142.png", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 413}, {"image_id": "img_9_143", "page_number": 10, "bbox": [299.616455078125, 185.1531982421875, 378.818603515625, 186.35324096679688], "image_path": "data/temp_downloads/420c43a510576ebeecd5aca722620763_images/img_9_143.png", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 956}, {"image_id": "img_9_144", "page_number": 10, "bbox": [299.616455078125, 186.35324096679688, 378.818603515625, 187.55328369140625], "image_path": "data/temp_downloads/420c43a510576ebeecd5aca722620763_images/img_9_144.png", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 431}, {"image_id": "img_9_145", "page_number": 10, "bbox": [299.616455078125, 187.55328369140625, 378.818603515625, 188.75332641601562], "image_path": "data/temp_downloads/420c43a510576ebeecd5aca722620763_images/img_9_145.png", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 1174}, {"image_id": "img_9_146", "page_number": 10, "bbox": [299.616455078125, 77.96846771240234, 378.818603515625, 79.16850280761719], "image_path": "data/temp_downloads/420c43a510576ebeecd5aca722620763_images/img_9_146.png", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 826}, {"image_id": "img_9_147", "page_number": 10, "bbox": [299.616455078125, 188.75331115722656, 378.818603515625, 189.95335388183594], "image_path": "data/temp_downloads/420c43a510576ebeecd5aca722620763_images/img_9_147.png", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 429}, {"image_id": "img_9_148", "page_number": 10, "bbox": [299.616455078125, 189.95333862304688, 378.818603515625, 191.15338134765625], "image_path": "data/temp_downloads/420c43a510576ebeecd5aca722620763_images/img_9_148.png", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 1189}, {"image_id": "img_9_149", "page_number": 10, "bbox": [299.616455078125, 191.1533660888672, 378.818603515625, 192.35340881347656], "image_path": "data/temp_downloads/420c43a510576ebeecd5aca722620763_images/img_9_149.png", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 425}, {"image_id": "img_9_150", "page_number": 10, "bbox": [299.616455078125, 192.3533935546875, 378.818603515625, 193.55343627929688], "image_path": "data/temp_downloads/420c43a510576ebeecd5aca722620763_images/img_9_150.png", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 1162}, {"image_id": "img_9_151", "page_number": 10, "bbox": [299.616455078125, 193.55343627929688, 378.818603515625, 194.75347900390625], "image_path": "data/temp_downloads/420c43a510576ebeecd5aca722620763_images/img_9_151.png", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 405}, {"image_id": "img_9_152", "page_number": 10, "bbox": [299.616455078125, 194.75344848632812, 378.818603515625, 195.9534912109375], "image_path": "data/temp_downloads/420c43a510576ebeecd5aca722620763_images/img_9_152.png", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 1137}, {"image_id": "img_9_153", "page_number": 10, "bbox": [299.616455078125, 195.95350646972656, 378.818603515625, 197.15354919433594], "image_path": "data/temp_downloads/420c43a510576ebeecd5aca722620763_images/img_9_153.png", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 397}, {"image_id": "img_9_154", "page_number": 10, "bbox": [299.616455078125, 197.15353393554688, 378.818603515625, 198.35357666015625], "image_path": "data/temp_downloads/420c43a510576ebeecd5aca722620763_images/img_9_154.png", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 1289}, {"image_id": "img_9_155", "page_number": 10, "bbox": [299.616455078125, 198.35357666015625, 378.818603515625, 199.55361938476562], "image_path": "data/temp_downloads/420c43a510576ebeecd5aca722620763_images/img_9_155.png", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 402}, {"image_id": "img_9_156", "page_number": 10, "bbox": [299.616455078125, 199.5535888671875, 378.818603515625, 200.75363159179688], "image_path": "data/temp_downloads/420c43a510576ebeecd5aca722620763_images/img_9_156.png", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 1287}, {"image_id": "img_12_0", "page_number": 13, "bbox": [49.406314849853516, 222.96978759765625, 105.10527038574219, 278.6687316894531], "image_path": "data/temp_downloads/420c43a510576ebeecd5aca722620763_images/img_12_0.png", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 271, "height": 271, "format": "PNG", "file_size": 1914}, {"image_id": "img_12_1", "page_number": 13, "bbox": [114.79092407226562, 222.96978759765625, 170.4898681640625, 278.6687316894531], "image_path": "data/temp_downloads/420c43a510576ebeecd5aca722620763_images/img_12_1.png", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 271, "height": 271, "format": "PNG", "file_size": 1702}, {"image_id": "img_12_2", "page_number": 13, "bbox": [180.175537109375, 222.96978759765625, 235.87448120117188, 278.6687316894531], "image_path": "data/temp_downloads/420c43a510576ebeecd5aca722620763_images/img_12_2.png", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 271, "height": 271, "format": "PNG", "file_size": 1821}, {"image_id": "img_12_3", "page_number": 13, "bbox": [245.5601348876953, 222.96978759765625, 301.25909423828125, 278.6687316894531], "image_path": "data/temp_downloads/420c43a510576ebeecd5aca722620763_images/img_12_3.png", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 271, "height": 271, "format": "PNG", "file_size": 1796}, {"image_id": "img_12_4", "page_number": 13, "bbox": [310.9447937011719, 222.96978759765625, 366.64373779296875, 278.6687316894531], "image_path": "data/temp_downloads/420c43a510576ebeecd5aca722620763_images/img_12_4.png", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 271, "height": 271, "format": "PNG", "file_size": 1489}, {"image_id": "img_12_5", "page_number": 13, "bbox": [376.3293762207031, 222.96978759765625, 432.0283203125, 278.6687316894531], "image_path": "data/temp_downloads/420c43a510576ebeecd5aca722620763_images/img_12_5.png", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 271, "height": 271, "format": "PNG", "file_size": 1603}, {"image_id": "img_12_6", "page_number": 13, "bbox": [441.7139892578125, 222.96978759765625, 497.4129333496094, 278.6687316894531], "image_path": "data/temp_downloads/420c43a510576ebeecd5aca722620763_images/img_12_6.png", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 271, "height": 271, "format": "PNG", "file_size": 1558}, {"image_id": "img_12_7", "page_number": 13, "bbox": [507.0986328125, 222.96978759765625, 562.797607421875, 278.6687316894531], "image_path": "data/temp_downloads/420c43a510576ebeecd5aca722620763_images/img_12_7.png", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 271, "height": 271, "format": "PNG", "file_size": 1671}, {"image_id": "img_12_8", "page_number": 13, "bbox": [49.01103210449219, 301.5858459472656, 99.47600555419922, 328.4783630371094], "image_path": "data/temp_downloads/420c43a510576ebeecd5aca722620763_images/img_12_8.png", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 304, "height": 162, "format": "PNG", "file_size": 1361}, {"image_id": "img_12_9", "page_number": 13, "bbox": [537.1239013671875, 292.93707275390625, 563.0203857421875, 337.09393310546875], "image_path": "data/temp_downloads/420c43a510576ebeecd5aca722620763_images/img_12_9.png", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 156, "height": 266, "format": "PNG", "file_size": 837}, {"image_id": "img_12_10", "page_number": 13, "bbox": [101.883056640625, 301.5858459472656, 152.3480224609375, 328.4783630371094], "image_path": "data/temp_downloads/420c43a510576ebeecd5aca722620763_images/img_12_10.png", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 304, "height": 162, "format": "PNG", "file_size": 1266}, {"image_id": "img_12_11", "page_number": 13, "bbox": [167.01974487304688, 292.93707275390625, 192.91624450683594, 337.09393310546875], "image_path": "data/temp_downloads/420c43a510576ebeecd5aca722620763_images/img_12_11.png", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 156, "height": 266, "format": "PNG", "file_size": 865}, {"image_id": "img_12_12", "page_number": 13, "bbox": [219.8917694091797, 292.93707275390625, 245.78826904296875, 337.09393310546875], "image_path": "data/temp_downloads/420c43a510576ebeecd5aca722620763_images/img_12_12.png", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 156, "height": 266, "format": "PNG", "file_size": 776}, {"image_id": "img_12_13", "page_number": 13, "bbox": [272.7637939453125, 292.93707275390625, 298.6602783203125, 337.09393310546875], "image_path": "data/temp_downloads/420c43a510576ebeecd5aca722620763_images/img_12_13.png", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 156, "height": 266, "format": "PNG", "file_size": 757}, {"image_id": "img_12_14", "page_number": 13, "bbox": [325.63580322265625, 292.93707275390625, 351.53228759765625, 337.09393310546875], "image_path": "data/temp_downloads/420c43a510576ebeecd5aca722620763_images/img_12_14.png", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 156, "height": 266, "format": "PNG", "file_size": 852}, {"image_id": "img_12_15", "page_number": 13, "bbox": [378.5078430175781, 292.93707275390625, 404.40435791015625, 337.09393310546875], "image_path": "data/temp_downloads/420c43a510576ebeecd5aca722620763_images/img_12_15.png", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 156, "height": 266, "format": "PNG", "file_size": 840}, {"image_id": "img_12_16", "page_number": 13, "bbox": [431.3798522949219, 292.93707275390625, 457.2763671875, 337.09393310546875], "image_path": "data/temp_downloads/420c43a510576ebeecd5aca722620763_images/img_12_16.png", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 156, "height": 266, "format": "PNG", "file_size": 839}, {"image_id": "img_12_17", "page_number": 13, "bbox": [431.3798522949219, 292.93707275390625, 457.2763671875, 337.09393310546875], "image_path": "data/temp_downloads/420c43a510576ebeecd5aca722620763_images/img_12_17.png", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 156, "height": 266, "format": "PNG", "file_size": 839}, {"image_id": "img_13_0", "page_number": 14, "bbox": [375.6044616699219, 147.11178588867188, 530.95166015625, 244.20376586914062], "image_path": "data/temp_downloads/420c43a510576ebeecd5aca722620763_images/img_13_0.png", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 1536, "height": 960, "format": "PNG", "file_size": 126102}, {"image_id": "img_13_1", "page_number": 14, "bbox": [375.6044616699219, 250.28823852539062, 530.95166015625, 347.3802185058594], "image_path": "data/temp_downloads/420c43a510576ebeecd5aca722620763_images/img_13_1.png", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 1536, "height": 960, "format": "PNG", "file_size": 56716}, {"image_id": "img_13_2", "page_number": 14, "bbox": [218.12127685546875, 146.72341918945312, 373.46844482421875, 243.81539916992188], "image_path": "data/temp_downloads/420c43a510576ebeecd5aca722620763_images/img_13_2.png", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 1536, "height": 960, "format": "PNG", "file_size": 111242}, {"image_id": "img_13_3", "page_number": 14, "bbox": [218.250732421875, 250.2235107421875, 373.597900390625, 347.31549072265625], "image_path": "data/temp_downloads/420c43a510576ebeecd5aca722620763_images/img_13_3.png", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 1536, "height": 960, "format": "PNG", "file_size": 59816}, {"image_id": "img_13_4", "page_number": 14, "bbox": [98.95703125, 138.5029754638672, 212.14901733398438, 219.87451171875], "image_path": "data/temp_downloads/420c43a510576ebeecd5aca722620763_images/img_13_4.png", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 3323, "height": 2391, "format": "PNG", "file_size": 238050}, {"image_id": "img_13_5", "page_number": 14, "bbox": [113.0677261352539, 230.02838134765625, 189.82008361816406, 328.41961669921875], "image_path": "data/temp_downloads/420c43a510576ebeecd5aca722620763_images/img_13_5.png", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 1540, "height": 1974, "format": "PNG", "file_size": 121657}, {"image_id": "img_13_6", "page_number": 14, "bbox": [189.77040100097656, 243.8154296875, 207.1175079345703, 326.73199462890625], "image_path": "data/temp_downloads/420c43a510576ebeecd5aca722620763_images/img_13_6.png", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 1222, "height": 5550, "format": "PNG", "file_size": 115307}], "tables": [{"table_id": "table_9_0", "page_number": 10, "bbox": [203.88656616210938, 67.27725219726562, 282.54327392578125, 137.75189208984375], "caption": null, "table_number": null, "data": [["", "", "", "", "8", "", "7", "", ""], ["3", "", "", "", "4", "", "", "", ""], ["", "3", "8", "4", "", "", "", "2", ""], ["", "", "6", "", "", "3", "", "", "8"], ["9", "", "", "", "", "", "", "", "6"], ["", "", "", "5", "", "", "", "", ""], ["", "", "", "", "", "2", "", "", "1"], ["", "2", "5", "", "3", "", "", "8", ""]], "headers": ["", "8", "4", "", "", "5", "", "6", ""], "markdown": "|  | 8 | 4 |  |  | 5 |  | 6 |  |\n| --- | --- | --- | --- | --- | --- | --- | --- | --- |\n|  |  |  |  | 8 |  | 7 |  |  |\n| 3 |  |  |  | 4 |  |  |  |  |\n|  | 3 | 8 | 4 |  |  |  | 2 |  |\n|  |  | 6 |  |  | 3 |  |  | 8 |\n| 9 |  |  |  |  |  |  |  | 6 |\n|  |  |  | 5 |  |  |  |  |  |\n|  |  |  |  |  | 2 |  |  | 1 |\n|  | 2 | 5 |  | 3 |  |  | 8 |  |"}, {"table_id": "table_9_1", "page_number": 10, "bbox": [212.5867919921875, 160.7707061767578, 273.9248046875, 214.9903564453125], "caption": null, "table_number": null, "data": [["5", "9", "6", "4", "7", "8", "1"], ["3", "8", "4", "9", "6", "1", "2"], ["1", "6", "2", "7", "3", "5", "9"], ["7", "2", "8", "5", "1", "4", "3"], ["9", "3", "5", "1", "8", "2", "7"], ["4", "7", "9", "6", "2", "3", "5"]], "headers": ["6", "1", "3", "8", "9", "7", "4"], "markdown": "| 6 | 1 | 3 | 8 | 9 | 7 | 4 |\n| --- | --- | --- | --- | --- | --- | --- |\n| 5 | 9 | 6 | 4 | 7 | 8 | 1 |\n| 3 | 8 | 4 | 9 | 6 | 1 | 2 |\n| 1 | 6 | 2 | 7 | 3 | 5 | 9 |\n| 7 | 2 | 8 | 5 | 1 | 4 | 3 |\n| 9 | 3 | 5 | 1 | 8 | 2 | 7 |\n| 4 | 7 | 9 | 6 | 2 | 3 | 5 |"}], "full_text": "# Hierarchical Reasoning Model\n\n Guan Wang\\({}^{1,\\dagger}\\), Jin Li\\({}^{1}\\), Yuhao Sun\\({}^{1}\\), Xing Chen\\({}^{1}\\), Changling Liu\\({}^{1}\\),\n\nYue Wu\\({}^{1}\\), Meng Lu\\({}^{1,\\dagger}\\), Sen Song\\({}^{2,\\dagger}\\), Yasin Abbasi Yadkori\\({}^{1,\\dagger}\\)\n\n\\({}^{1}\\)Sapient Intelligence, Singapore\n\n###### Abstract\n\nReasoning, the process of devising and executing complex goal-oriented action sequences, remains a critical challenge in AI. Current large language models (LLMs) primarily employ Chain-of-Thought (CoT) techniques, which suffer from brittle task decomposition, extensive data requirements, and high latency. Inspired by the hierarchical and multi-timescale processing in the human brain, we propose the Hierarchical Reasoning Model (HRM), a novel recurrent architecture that attains significant computational depth while maintaining both training stability and efficiency. HRM executes sequential reasoning tasks in a single forward pass without explicit supervision of the intermediate process, through two interdependent recurrent modules: a high-level module responsible for slow, abstract planning, and a low-level module handling rapid, detailed computations. With only 27 million parameters, HRM achieves exceptional performance on complex reasoning tasks using only 1000 training samples. The model operates without pre-training or CoT data, yet achieves nearly perfect performance on challenging tasks including complex Sudoku puzzles and optimal path finding in large mazes. Furthermore, HRM outperforms much larger models with significantly longer context windows on the Abstraction and Reasoning Corpus (ARC), a key benchmark for measuring artificial general intelligence capabilities. These results underscore HRM's potential as a transformative advancement toward universal computation and general-purpose reasoning systems.\n\n\n\nFigure 1: **Left:** HRM is inspired by hierarchical processing and temporal separation in the brain. It has two recurrent networks operating at different timescales to collaboratively solve tasks. **Right:** With only about 1000 training examples, the HRM (\\(\\sim\\)27M parameters) surpasses state-of-the-art CoT models on inductive benchmarks (ARC-AGI) and challenging symbolic tree-search puzzles (_Sudoku-Extreme_, _Maze-Hard_) where CoT models failed completely. The HRM was randomly initialized, and it solved the tasks directly from inputs without chain of thoughts.\n\nIntroduction\n\nDeep learning, as its name suggests, emerged from the idea of stacking more layers to achieve increased representation power and improved performance [1, 2]. However, despite the remarkable success of large language models, their core architecture is paradoxically shallow [3]. This imposes a fundamental constraint on their most sought-after capability: reasoning. The fixed depth of standard Transformers places them in computational complexity classes such as \\(AC^{0}\\) or \\(TC^{0}\\)[4], preventing them from solving problems that require polynomial time [5, 6]. LLMs are not Turing-complete and thus they cannot, at least in a purely end-to-end manner, execute complex algorithmic reasoning that is necessary for deliberate planning or symbolic manipulation tasks [7, 8]. For example, our results on the Sudoku task show that increasing Transformer model depth _can_ improve performance,1 but performance remains far from optimal even with very deep models (see Figure 2), which supports the conjectured limitations of the LLM scaling paradigm [9].\n\nFootnote 1: Simply increasing the model width does not improve performance here.\n\nThe LLMs literature has relied largely on Chain-of-Thought (CoT) prompting for reasoning [10]. CoT externalizes reasoning into token-level language by breaking down complex tasks into simpler intermediate steps, sequentially generating text using a shallow model [11]. However, CoT for reasoning is a crutch, not a satisfactory solution. It relies on brittle, human-defined decompositions where a single misstep or a misorder of the steps can derail the reasoning process entirely [12, 13]. This dependency on explicit linguistic steps tethers reasoning to patterns at the token level. As a result, CoT reasoning often requires significant amount of training data and generates a large number of tokens for complex reasoning tasks, resulting in slow response times. A more efficient approach is needed to minimize these data requirements [14].\n\nTowards this goal, we explore \"latent reasoning\", where the model conducts computations within its internal hidden state space [15, 16]. This aligns with the understanding that language is a tool for human communication, not the substrate of thought itself [17]; the brain sustains lengthy, coherent chains of reasoning with remarkable efficiency in a latent space, without constant translation back to language. However, the power of latent reasoning is still fundamentally constrained by a model's _effective computational depth_. Naively stacking layers is notoriously difficult due to vanishing gradients, which plague training stability and effectiveness [1, 18]. Recurrent architectures, a natural alternative for sequential tasks, often suffer from early convergence, rendering subsequent computational steps inert, and rely on the biologically implausible, computationally expensive and memory intensive Backpropagation Through Time (BPTT) for training [19].\n\nThe human brain provides a compelling blueprint for achieving the effective computational depth that contemporary artificial models lack. It organizes computation hierarchically across cortical regions operating at different timescales, enabling deep, multi-stage reasoning [20, 21, 22]. Recurrent feedback loops iteratively refine internal representations, allowing slow, higher-level areas to guide, and fast, lower-level circuits to execute--subordinate processing while preserving global coherence [23, 24, 25]. Notably, the brain achieves such depth without incurring the prohibitive credit-assignment costs that typically hamper recurrent networks from backpropagation through time [19, 26].\n\nInspired by this hierarchical and multi-timescale biological architecture, we propose the Hierarchical Reasoning Model (HRM). HRM is designed to significantly increase the effective computational depth. It features two coupled recurrent modules: a high-level (H) module for abstract, deliberate reasoning, and a low-level (L) module for fast, detailed computations. This structure\n\navoids the rapid convergence of standard recurrent models through a process we term \"hierarchical convergence.\" The slow-updating H-module advances only after the fast-updating L-module has completed multiple computational steps and reached a local equilibrium, at which point the L-module is reset to begin a new computational phase.\n\nFurthermore, we propose a one-step gradient approximation for training HRM, which offers improved efficiency and eliminates the requirement for BPTT. This design maintains a constant memory footprint (\\(O(1)\\) compared to BPTT's \\(O(T)\\) for \\(T\\) timesteps) throughout the backpropagation process, making it scalable and more biologically plausible.\n\nLeveraging its enhanced effective depth, HRM excels at tasks that demand extensive search and backtracking. **Using only 1,000 input-output examples, without pre-training or CoT supervision,** HRM learns to solve problems that are intractable for even the most advanced LLMs. For example, it achieves near-perfect accuracy in complex Sudoku puzzles (_Sudoku-Extreme Full_) and optimal pathfinding in 30x30 mazes, where state-of-the-art CoT methods completely fail (0% accuracy). In the Abstraction and Reasoning Corpus (ARC) AGI Challenge [27, 28, 29] - a benchmark of inductive reasoning - HRM, trained from scratch with only the official dataset (~1000 examples), with only 27M parameters and a 30x30 grid context (900 tokens), achieves a performance of **40.3%**, which substantially surpasses leading CoT-based models like o3-mini-high (34.5%) and Claude 3.7 8K context (21.2%), despite their considerably larger parameter sizes and context lengths, as shown in Figure 1. This represents a promising direction toward the development of next-generation AI reasoning systems with universal computational capabilities.\n\n## 2 Hierarchical Reasoning Model\n\nWe present the HRM, inspired by three fundamental principles of neural computation observed in the brain:\n\n* [leftmargin=*]\n* **Hierarchical processing:** The brain processes information across a hierarchy of cortical areas. Higher-level areas integrate information over longer timescales and form abstract representations, while lower-level areas handle more immediate, detailed sensory and motor processing [20, 22, 21].\n\nFigure 2: **The necessity of depth for complex reasoning. Left:** On _Sudoku-Extreme Full_, which require extensive tree-search and backtracking, increasing a Transformer’s width yields no performance gain, while increasing depth is critical. **Right:** Standard architectures saturates, failing to benefit from increased depth. HRM overcomes this fundamental limitation, effectively using its computational depth to achieve near-perfect accuracy.\n\n* **Temporal Separation:** These hierarchical levels in the brain operate at distinct intrinsic timescales, reflected in neural rhythms (e.g., slow theta waves, 4-8 Hz and fast gamma waves, 30-100 Hz) [30, 31]. This separation allows for stable, high-level guidance of rapid, low-level computations [32, 33].\n* **Recurrent Connectivity:** The brain features extensive recurrent connections. These feedback loops enable iterative refinement, yielding more accurate and context-sensitive representations at the cost of additional processing time. Additionally, the brain largely avoids the problematic deep credit assignment problem associated with BPTT19. Footnote 19: While inspired by temporal separation in the brain, our model’s “high-level” and “low-level” modules are conceptual abstractions and do not map directly to specific neural oscillation frequencies.\n\nThe HRM model consists of four learnable components: an input network \\(f_{I}(\\cdot;\\theta_{I})\\), a low-level recurrent module \\(f_{L}(\\cdot;\\theta_{L})\\), a high-level recurrent module \\(f_{H}(\\cdot;\\theta_{H})\\), and an output network \\(f_{O}(\\cdot;\\theta_{O})\\). The model's dynamics unfold over \\(N\\) high-level cycles of \\(T\\) low-level timesteps each2. We index the total timesteps of one forward pass by \\(i=1,\\ldots,N\\times T\\). The modules \\(f_{L}\\) and \\(f_{H}\\) each keep a hidden state--\\(z_{L}^{i}\\) for \\(f_{L}\\) and \\(z_{H}^{i}\\) for \\(f_{H}\\)--which are initialized with the vectors \\(z_{L}^{0}\\) and \\(z_{H}^{0}\\), respectively.\n\nFootnote 2: While inspired by temporal separation in the brain, our model’s “high-level” and “low-level” modules are conceptual abstractions and do not map directly to specific neural oscillation frequencies.\n\nThe HRM maps an input vector \\(x\\) to an output prediction vector \\(\\hat{y}\\) as follows. First, the input \\(x\\) is projected into a working representation \\(\\tilde{x}\\) by the input network:\n\n\\[\\tilde{x}=f_{I}(x;\\theta_{I})\\ .\\]\n\nAt each timestep \\(i\\), the L-module updates its state conditioned on its own previous state, the H-module's current state (which remains fixed throughout the cycle), and the input representation. The H-module only updates once per cycle (i.e., every \\(T\\) timesteps) using the L-module's final state at the end of that cycle:\n\n\\[z_{L}^{i} =f_{L}\\left(z_{L}^{i-1},z_{H}^{i-1},\\tilde{x};\\theta_{L}\\right)\\ ,\\] \\[z_{H}^{i} =\\begin{cases}f_{H}\\left(z_{H}^{i-1},z_{L}^{i-1};\\theta_{H}\\right) &\\text{if }i\\equiv 0\\,\\left(\\mathrm{mod}\\ T\\right),\\\\ z_{H}^{i-1}&\\text{otherwise}\\ \\ .\\end{cases}\\]\n\nFinally, after \\(N\\) full cycles, a prediction \\(\\hat{y}\\) is extracted from the hidden state of the H-module:\n\n\\[\\hat{y}=f_{O}(z_{H}^{NT};\\theta_{O})\\ .\\]\n\nThis entire \\(NT\\)-timestep process represents a single forward pass of the HRM. A halting mechanism (detailed later in this section) determines whether the model should terminate, in which case \\(\\hat{y}\\) will be used as the final prediction, or continue with an additional forward pass.\n\n**Hierarchical convergence** Although convergence is crucial for recurrent networks, standard RNNs are fundamentally limited by their tendency to converge too early. As the hidden state settles toward a fixed point, update magnitudes shrink, effectively stalling subsequent computation and capping the network's effective depth. To preserve computational power, we actually want convergence to proceed very slowly-but engineering that gradual approach is difficult, since pushing convergence too far edges the system toward instability.\n\nHRM is explicitly designed to counteract this premature convergence through a process we term _hierarchical convergence_. During each cycle, the L-module (an RNN) exhibits stable convergence to a _local equilibrium_. This equilibrium, however, depends on the high-level state \\(z_{H}\\) supplied during that cycle. After completing the \\(T\\) steps, the H-module incorporates the sub-computation's outcome (the final state \\(z_{L}\\)) and performs its own update. This \\(z_{H}\\) update establishes a fresh context for the L-module, essentially \"restarting\" its computational path and initiating a new convergence phase toward a different local equilibrium.\n\nThis process allows the HRM to perform a sequence of distinct, stable, nested computations, where the H-module directs the overall problem-solving strategy and the L-module executes the intensive search or refinement required for each step. Although a standard RNN may approach convergence within \\(T\\) iterations, the hierarchical convergence benefits from an enhanced effective depth of \\(NT\\) steps. As empirically shown in Figure 3, this mechanism allows HRM both to maintain high computational activity (forward residual) over many steps (in contrast to a standard RNN, whose activity rapidly decays) and to enjoy stable convergence. This translates into better performance at any computation depth, as illustrated in Figure 2.\n\n\n\n**Approximate gradient** Recurrent models typically use BPTT to compute gradients. However, BPTT requires storing the hidden states from the forward pass and then combining them with gradients during the backward pass, which demands \\(O(T)\\) memory for T timesteps. This heavy memory burden forces smaller batch sizes and leads to poor GPU utilization, especially for large-scale networks. Additionally, because retaining the full history trace through time is biologically implausible, it is unlikely that the brain implements BPTT [19].\n\nFortunately, if a recurrent neural network converges to a fixed point, we can avoid unrolling its state sequence by applying backpropagation in a single step at that equilibrium point. Moreover, such a mechanism could plausibly be implemented in the brain using only local learning rules [34, 35]. Based\n\nFigure 3: Comparison of forward residuals and PCA trajectories. HRM shows hierarchical convergence: the H-module steadily converges, while the L-module repeatedly converges within cycles before being reset by H, resulting in residual spikes. The recurrent neural network exhibits rapid convergence with residuals quickly approaching zero. In contrast, the deep neural network experiences vanishing gradients, with significant residuals primarily in the initial (input) and final layers.\n\non this finding, we propose a one-step approximation of the HRM gradient-using the gradient of the last state of each module and treating other states as constant. The gradient path is, therefore,\n\n\\[\\text{Output head}\\rightarrow\\text{final state of the H-module}\\rightarrow\\text{final state of the L-module}\\rightarrow\\text{input embedding}\\]\n\nThe above method needs \\(O(1)\\) memory, does not require unrolling through time, and can be easily implemented with an autograd framework such as PyTorch, as shown in Figure 4. Given that each module only needs to back-propagate errors through its most recent local synaptic activity, this approach aligns well with the perspective that cortical credit assignment relies on short-range, temporally local mechanisms rather than on a global replay of activity patterns.\n\n\n\nThe one-step gradient approximation is theoretically grounded in the mathematics of Deep Equilibrium Models (DEQ) [36] which employs the Implicit Function Theorem (IFT) to bypass BPTT, as detailed next. Consider an idealized HRM behavior where, during high-level cycle \\(k\\), the L-module repeatedly updates until its state \\(z_{L}\\) converges to a local fixed point \\(z_{H}^{\\star}\\). This fixed point, given the current high-level state \\(z_{H}^{k-1}\\), can be expressed as\n\n\\[z_{L}^{\\star}=f_{L}(z_{L}^{\\star},z_{H}^{k-1},\\tilde{x};\\theta_{L})\\ .\\]\n\nThe H-module then performs a single update using this converged L-state:\n\n\\[z_{H}^{k}=f_{H}(z_{H}^{k-1},z_{L}^{\\star};\\theta_{H})\\ .\\]\n\nWith a proper mapping \\(\\mathcal{F}\\), the updates to the high-level state can be written in a more compact form as \\(z_{H}^{k}=\\mathcal{F}(z_{H}^{k-1};\\tilde{x},\\theta)\\), where \\(\\theta=(\\theta_{I},\\theta_{L})\\), and the fixed-point can be written as \\(z_{H}^{\\star}=\\mathcal{F}(z_{H}^{\\star};\\tilde{x},\\theta)\\). Let \\(J_{\\mathcal{F}}=\\frac{\\partial\\mathcal{F}}{\\partial z_{H}}\\) be the Jacobian of \\(\\mathcal{F}\\), and assume that the matrix \\(I-J_{\\mathcal{F}}\\) is invertible at \\(z_{H}^{\\star}\\) and that the mapping \\(\\mathcal{F}\\) is continuously differentiable. The Implicit Function Theorem then allows us to calculate the exact gradient of fixed point \\(z_{H}^{\\star}\\) with respect to the parameters \\(\\theta\\) without explicit back-propagation:\n\n\\[\\frac{\\partial z_{H}^{\\star}}{\\partial\\theta}=\\left(I-J_{\\mathcal{F}}\\big{|}_ {z_{H}^{\\star}}\\right)^{-1}\\frac{\\partial\\mathcal{F}}{\\partial\\theta}\\bigg{|} _{z_{H}^{\\star}}\\ .\\] (1)\n\nCalculating the above gradient requires evaluating and inverting matrix \\((I-J_{\\mathcal{F}})\\) that can be computationally expensive. Given the Neumann series expansion,\n\n\\[(I-J_{\\mathcal{F}})^{-1}=I+J_{\\mathcal{F}}+J_{\\mathcal{F}}^{2}+J_{\\mathcal{F} }^{3}+\\ldots\\ ,\\]\n\nthe so-called _1-step gradient_[37] approximates the series by considering only its first term, i.e. \\((I-J_{\\mathcal{F}})^{-1}\\approx I\\), and leads to the following approximation of Equation (1):\n\n\\[\\frac{\\partial z_{H}^{\\star}}{\\partial\\theta_{H}}\\approx\\frac{ \\partial f_{H}}{\\partial\\theta_{H}},\\quad\\frac{\\partial z_{H}^{\\star}}{ \\partial\\theta_{L}}\\approx\\frac{\\partial f_{H}}{\\partial z_{L}^{\\star}}\\cdot \\frac{\\partial z_{L}^{\\star}}{\\partial\\theta_{L}},\\quad\\frac{\\partial z_{H}^ {\\star}}{\\partial\\theta_{I}}\\approx\\frac{\\partial f_{H}}{\\partial z_{L}^{ \\star}}\\cdot\\frac{\\partial z_{L}^{\\star}}{\\partial\\theta_{I}}\\ .\\] (2)\n\nFigure 4: **Top:** Diagram of HRM with approximate gradient. **Bottom:** Pseudocode of HRM with deep supervision training in PyTorch.\n\nThe gradients of the low-level fixed point, \\(\\frac{\\partial z_{L}^{*}}{\\partial\\theta_{L}}\\) and \\(\\frac{\\partial z_{L}^{*}}{\\partial\\theta_{I}}\\), can also be approximated using another application of the 1-step gradient:\n\n\\[\\frac{\\partial z_{L}^{*}}{\\partial\\theta_{L}}\\approx\\frac{\\partial f_{L}}{ \\partial\\theta_{L}},\\quad\\frac{\\partial z_{L}^{*}}{\\partial\\theta_{I}}\\approx \\frac{\\partial f_{L}}{\\partial\\theta_{I}}\\ .\\] (3)\n\nBy substituting Equation (3) back into Equation (2), we arrive at the final simplified gradients.\n\nBefore defining our loss function, we must first introduce two key elements of our proposed method: _deep supervision_ and _adaptive computational time_.\n\n**Deep supervision** Inspired by the principle that periodic neural oscillations regulate when learning occurs in the brain [38], we incorporate a deep supervision mechanism into HRM, as detailed next.\n\nGiven a data sample \\((x,y)\\), we run multiple forward passes of the HRM model, each of which we refer to as a _segment_. Let \\(M\\) denote the total number of segments executed before termination. For each segment \\(m\\in\\{1,\\dots,M\\}\\), let \\(z^{m}=(z_{H}^{mNT},z_{L}^{mNT})\\) represent the hidden state at the conclusion of segment \\(m\\), encompassing both high-level and low-level state components.\n\nAt each segment \\(m\\), we apply a deep supervision step as follows:\n\n1. Given the state \\(z^{m-1}\\) from the previous segment, compute the next state \\(z^{m}\\) and its associated output \\(\\hat{y}^{m}\\) through a forward pass in the HRM model: \\[(z^{m},\\hat{y}^{m})\\leftarrow\\text{HRM}(z^{m-1},x;\\theta)\\]\n2. Compute the loss for the current segment: \\[L^{m}\\leftarrow\\text{Loss}(\\hat{y}^{m},y)\\]\n3. Update parameters: \\[\\theta\\leftarrow\\text{OptimizerStep}(\\theta,\\nabla_{\\theta}L^{m})\\]\n\nThe crucial aspect of this procedure is that the hidden state \\(z^{m}\\) is \"detached\" from the computation graph before being used as the input state for the next segment. Consequently, gradients from segment \\(m+1\\) do not propagate back through segment \\(m\\), effectively creating a 1-step approximation of the gradient of the recursive deep supervision process [39, 40]. This approach provides more frequent feedback to the H-module and serves as a regularization mechanism, demonstrating superior empirical performance and enhanced stability in deep equilibrium models when compared to more complex, Jacobian-based regularization techniques [39, 41]. Figure 4 shows pseudocode of deep supervision training.\n\n\n\n**Adaptive computational time (ACT)** The brain dynamically alternates between automatic thinking (\"System 1\") and deliberate reasoning (\"System 2\") [42]. Neuroscientific evidence shows that these cognitive modes share overlapping neural circuits, particularly within regions such as the prefrontal cortex and the default mode network [43, 44]. This indicates that the brain dynamically modulates the \"runtime\" of these circuits according to task complexity and potential rewards [45, 46].\n\nInspired by the above mechanism, we incorporate an adaptive halting strategy into HRM that enables \"thinking, fast and slow\". This integration leverages deep supervision and uses the Q-learning\n\nalgorithm [47] to adaptively determine the number of segments. A Q-head uses the final state of the H-module to predict the Q-values \\(\\hat{Q}^{m}=(\\hat{Q}^{m}_{\\text{halt}},\\hat{Q}^{m}_{\\text{continue}})\\) of the \"halt\" and \"continue\" actions:\n\n\\[\\hat{Q}^{m}=\\sigma(\\theta_{Q}^{\\top}z_{H}^{mNT})\\,,\\]\n\nwhere \\(\\sigma\\) denotes the sigmoid function applied element-wise. The halt or continue action is chosen using a randomized strategy as detailed next. Let \\(M_{\\max}\\) denote the maximum number of segments (a fixed hyperparameter) and \\(M_{\\min}\\) denote the minimum number of segments (a random variable). The value of \\(M_{\\min}\\) is determined stochastically: with probability \\(\\varepsilon\\), it is sampled uniformly from the set \\(\\{2,\\cdots,M_{\\max}\\}\\) (to encourage longer thinking), and with probability \\(1-\\varepsilon\\), it is set to 1. The halt action is selected under two conditions: when the segment count surpasses the maximum threshold \\(M_{\\max}\\), or when the estimated halt value \\(\\hat{Q}_{\\text{halt}}\\) exceeds the estimated continue value \\(\\hat{Q}_{\\text{continue}}\\) and the segment count has reached at least the minimum threshold \\(M_{\\min}\\).\n\nThe Q-head is updated through a Q-learning algorithm, which is defined on the following episodic Markov Decision Process (MDP). The state of the MDP at segment \\(m\\) is \\(z^{m}\\), and the action space is \\(\\{\\text{halt},\\text{continue}\\}\\). Choosing the action \"halt\" terminates the episode and returns a binary reward indicating prediction correctness, i.e., \\(\\mathbf{1}\\{\\hat{y}^{m}=y\\}\\). Choosing \"continue\" yields a reward of 0 and the state transitions to \\(z^{m+1}\\). Thus, the Q-learning targets for the two actions \\(\\hat{G}^{m}=(\\hat{G}^{m}_{\\text{halt}},\\hat{G}^{m}_{\\text{continue}})\\) are given by\n\n\\[\\hat{G}^{m}_{\\text{halt}} =\\mathbf{1}\\{\\hat{y}^{m}=y\\}\\,,\\] \\[\\hat{G}^{m}_{\\text{continue}} =\\begin{cases}\\hat{Q}^{m+1}_{\\text{halt}},&\\text{if }m\\geq N_{ \\max}\\,,\\\\ \\max(\\hat{Q}^{m+1}_{\\text{halt}},\\hat{Q}^{m+1}_{\\text{continue}})\\,,&\\text{ otherwise }.\\end{cases}\\]\n\nWe can now define the loss function of our learning procedure. The overall loss for each supervision segment combines both the Q-head loss and the sequence-to-sequence loss:\n\n\\[L^{m}_{\\text{ACT}}=\\textsc{Loss}(\\hat{y}^{m},y)+\\textsc{BinaryCrossEntropy}( \\hat{Q}^{m},\\hat{G}^{m})\\ .\\]\n\nMinimizing the above loss enables both accurate predictions and nearly optimal stopping decisions.\n\nSelecting the \"halt\" action ends the supervision loop. In practice, sequences are processed in batches, which can be easily handled by substituting any halted sample in the batch with a fresh sample from the dataloader.\n\nFigure 5 presents a performance comparison between two HRM variants: one incorporating ACT and another employing a fixed computational step count equivalent to ACT's \\(M_{\\max}\\) parameter. It shows that ACT effectively adapts its computational resources based on task complexity, achieving significant computational savings with minimal impact on performance.\n\n**Inference-time scaling** An effective neural model should exploit additional computational resources during inference to enhance performance. As illustrated in Figure 5-(c), HRM seamlessly achieves inference-time scaling by simply increasing the computational limit parameter, \\(M_{\\max}\\) without requiring further training or architectural modifications.\n\nAdditional compute is especially effective for tasks that demand deeper reasoning. On Sudoku--a problem that often requires long-term planning--HRM exhibits strong inference-time scaling. On the other hand, we find that extra computational resources yield minimal gains in ARC-AGI challenge, as solutions generally require only a few transformations.\n\n#### 3.2.2 Stability of Q-learning in ACT\n\nThe deep Q-learning that underpins our ACT mechanism is known to be prone to instability, often requiring stabilization techniques such as replay buffers and target networks [48], which are absent in our design. Our approach, however, achieves stability through the intrinsic properties of our model and training procedure. Recent theoretical work by Gallici et al. [49] shows that Q-learning can achieve convergence if network parameters are bounded, weight decay is incorporated during training, and post-normalization layers are implemented. Our model satisfies these conditions through its Post-Norm architecture that employs RMSNorm (a layer normalization variant) and the AdamW optimizer. AdamW has been shown to solve an \\(L_{\\infty}\\)-constrained optimization problem, ensuring that model parameters remain bounded by \\(1/\\lambda\\)[50].\n\n#### 3.2.3 Architectural details\n\nWe employ a sequence-to-sequence architecture for HRM. Both input and output are represented as token sequences: \\(x=(x_{1},\\ldots,x_{l})\\) and \\(y=(y_{1},\\ldots,y_{l^{\\prime}})\\) respectively. The model includes an embedding layer \\(f_{I}\\) that converts discrete tokens into vector representations, and an output head \\(f_{O}(z;\\theta_{O})=\\text{softmax}(\\theta_{O}z)\\) that transforms hidden states into token probability distributions \\(\\hat{y}\\). For small-sample experiments, we replace softmax with stablemax [51] to improve generalization performance. The sequence-to-sequence loss is averaged over all tokens, \\(\\textsc{Loss}(\\hat{y},y)=\\frac{1}{l^{\\prime}}\\sum_{i=1}^{l^{\\prime}}\\log p(y_{ i})\\), where \\(p(y_{i})\\) is the probability that distribution \\(\\hat{y}_{i}\\) assigns to token \\(y_{i}\\). The initial hidden states \\(z^{0}\\) are initialized by sampling from a truncated normal distribution with standard deviation of 1, truncation of 2, and kept fixed throughout training.\n\nBoth the low-level and high-level recurrent modules \\(f_{L}\\) and \\(f_{H}\\) are implemented using encoder-only Transformer [52] blocks with identical architectures and dimensions. These modules take multiple inputs, and we use straightforward element-wise addition to combine them, though more sophisticated merging techniques such as gating mechanisms could potentially improve performance and is left for future work. For all Transformer blocks in this work--including those in the baseline models--we incorporate the enhancements found in modern LLMs (based on Llama [53] architectures). These improvements include Rotary Positional Encoding [54], Gated Linear Units [55], RMSNorm [56], and the removal of bias terms from linear layers.\n\nFurthermore, both HRM and recurrent Transformer models implement a Post-Norm architecture\n\nFigure 5: **Effectiveness of Adaptive Computation Time (ACT)** on the _Sudoku-Extreme-Full_. **(a)** Mean compute steps used by models with ACT versus models with a fixed number of compute steps (\\(M\\)). ACT maintains a low and stable number of average compute steps even as the maximum limit (\\(M_{\\max}\\)) increases. **(b)** Accuracy comparison. The ACT model achieves performance comparable to the fixed-compute model while utilizing substantially fewer computational steps on average. **(c)** Inference-time scalability. Models trained with a specific \\(M_{\\max}\\) can generalize to higher computational limits during inference, leading to improved accuracy. For example, a model trained with \\(M_{\\max}=8\\) continues to see accuracy gains when run with \\(M_{\\max}=16\\) during inference.\n\nwith weights initialized via truncated LeCun Normal initialization [57, 58, 59], while the scale and bias parameters are excluded from RMSNorm. All parameters are optimized using the Adam-atan2 optimizer [60], a scale-invariant variant of Adam [61], combined with a constant learning rate that includes linear warm-up.\n\n## 3 Results\n\nThis section begins by describing the ARC-AGI, Sudoku, and Maze benchmarks, followed by an overview of the baseline models and their results. Figure 6-(a,b,c) presents a visual representation of the three benchmark tasks, which are selected to evaluate various reasoning abilities in AI models.\n\n### Benchmarks\n\n#### 3.1.1 ARC-AGI Challenge\n\nThe ARC-AGI benchmark evaluates general fluid intelligence through IQ-test-like puzzles that require inductive reasoning [27]. The initial version, ARC-AGI-1, presents challenges as input-output grid pairs that force AI systems to extract and generalize abstract rules from just a few examples. Each task provides a few input-output example pairs (usually 2-3) and a test input. An AI model has two attempts to produce the correct output grid. Although some believe that mastering ARC-AGI would signal true artificial general intelligence, its primary purpose is to expose the current roadblocks in AGI progress. In fact, both conventional deep learning methods and CoT techniques have faced significant challenges with ARC-AGI-1, primarily because it requires the ability to generalize to entirely new tasks [28].\n\nAddressing the limitations identified in ARC-AGI-1, ARC-AGI-2 significantly expands the benchmark by providing a more comprehensive and carefully refined collection of tasks. These new tasks emphasize deeper compositional reasoning, multi-step logic, contextual rule application, and symbolic abstraction. Human calibration studies show these tasks are challenging but doable for people, while being much harder for current AI systems, offering a clearer measure of general reasoning abilities [29].\n\nFigure 6: **Left: Visualization of benchmark tasks. Right: Difficulty of _Sudoku-Extreme_ examples.**\n\nSudoku-ExtremeSudoku is a 9\\(\\times\\)9 logic puzzle, requiring each row, column, and 3\\(\\times\\)3 block to contain the digits 1-9 exactly once. A prediction is considered correct if it exactly matches the puzzle's unique solution. Sudoku's complex logical structure makes it a popular benchmark for evaluating logical reasoning in machine learning [62, 63, 64].\n\nThe most frequently used Sudoku dataset in research, namely the Kaggle dataset [65], can be fully solved using elementary single-digit techniques [66]. The minimal 17-clue puzzles [62], another widely-used collection, might seem more challenging due to its small number of clues. However, this perception is misleading--since 17 represents the minimum number of clues required to guarantee a unique Sudoku solution, these hints need to be highly orthogonal to each other. This orthogonal arrangement leads to many direct, easily-resolved solution paths [67].\n\nWe introduce _Sudoku-Extreme_, a more challenging dataset that is compiled from the aforementioned easy datasets as well as puzzles recognized by the Sudoku community as exceptionally difficult for human players:\n\n* Easy puzzles compiled from Kaggle, 17-clue, plus unbiased samples from the Sudoku puzzle distribution [67]: totaling \\(1\\,149\\,158\\) puzzles.\n* Challenging puzzles compiled from Magictour 1465, Forum-Hard and Forum-Extreme subsets: totaling \\(3\\,104\\,157\\) puzzles.\n\nThe compiled data then undergo a strict 90/10 train-test split, ensuring that the test set puzzles cannot be derived through equivalent transformations of any training samples. _Sudoku-Extreme_ is a down-sampled subset of this data containing 1000 training examples. We use _Sudoku-Extreme_ in our main experiments (Figure 1), which focuses on small-sample learning scenarios. To guarantee convergence and control overfitting effects in our analysis experiments (Figures 2, 3 and 5), we use the complete training data, _Sudoku-Extreme-Full_, containing \\(3\\,831\\,994\\) examples.\n\nWe measure puzzle difficulty by counting the number of search backtracks (\"guesses\") required by a smart Sudoku solver program _tdoku_, which uses propositional logic to reduce the number of guesses [67]. Our _Sudoku-Extreme_ dataset exhibits a mean difficulty of \\(22\\) backtracks per puzzle, significantly higher than existing datasets, including recent handmade puzzles Sudoku-Bench [68] which average just \\(0.45\\) backtracks per puzzle. These subset complexity levels are shown in Figure 6-(d).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFor ARC-AGI challenge, we start with all input-output example pairs in the training and the evaluation sets. The dataset is augmented by applying translations, rotations, flips, and color permutations to the puzzles. Each task example is prepended with a learnable special token that represents the puzzle it belongs to. At test time, we proceed as follows for each test input in the evaluation set: (1) Generate and solve 1000 augmented variants and, for each, apply the inverse-augmentation transform to obtain a prediction. (2) Choose the two most popular predictions as the final outputs.3 All results are reported on the evaluation set.\n\nFootnote 3: The ARC-AGI allows two attempts for each test input.\n\nWe augment Sudoku puzzles by applying band and digit permutations, while data augmentation is disabled for Maze tasks. Both tasks undergo only a single inference pass.\n\nFor ARC-AGI, the scores of the CoT models are taken from the official leaderboard [29], while for Sudoku and Maze, the scores are obtained by evaluating through the corresponding API.\n\nIn Figure 1, the baselines are grouped based on whether they are pre-trained and use CoT, or neither. The \"Direct pred\" baseline means using \"direct prediction without CoT and pre-training\", which retains the exact training setup of HRM but swaps in a Transformer architecture. Interestingly, on ARC-AGI-1, \"Direct pred\" matches the performance of Liao and Gu [73], who built a carefully designed, domain-specific equivariant network for learning the ARC-AGI task from scratch, without pre-training. By substituting the Transformer architecture with HRM's hierarchical framework and implementing ACT, we achieve more than a twofold performance improvement.\n\nOn the _Sudoku-Extreme_ and _Maze-Hard_ benchmarks, the performance gap between HRM and the baseline methods is significant, as the baselines almost never manage to solve the tasks. These benchmarks that demand lengthy reasoning traces are particularly difficult for CoT-based methods. With only 1000 training examples, the \"Direct pred\" baseline--which employs an 8-layer Transformer identical in size to HRM--fails entirely on these challenging reasoning problems. When trained on the larger _Sudoku-Extreme-Full_ dataset, however, \"Direct pred\" can solve some easy Sudoku puzzles and reaches \\(16.9\\%\\) accuracy (see Figure 2). Lehnert et al. [71] showed that a large vanilla Transformer model with 175M parameters, trained on 1 million examples across multiple trials, achieved only marginal success on 30x30 Maze tasks, with accuracy below \\(20\\%\\) using the \\(pass@64\\) evaluation metric.\n\n### Visualization of intermediate timesteps\n\nAlthough HRM demonstrates strong performance on complex reasoning tasks, it raises an intriguing question: what underlying reasoning algorithms does the HRM neural network actually implement? Addressing this question is important for enhancing model interpretability and developing a deeper understanding of the HRM solution space.\n\nWhile a definitive answer lies beyond our current scope, we begin our investigation by analyzing state trajectories and their corresponding solution evolution. More specifically, at each timestep \\(i\\) and given the low-level and high-level state pair (\\(z^{i}_{L}\\) and \\(z^{i}_{H}\\)) we perform a preliminary forward pass through the H-module to obtain \\(\\bar{z}^{i}=f_{H}(z^{i}_{H},z^{i}_{L};\\theta_{H})\\) and its corresponding decoded prediction \\(\\bar{y}^{i}=f_{O}(\\bar{z}^{i};\\theta_{O})\\). The prediction \\(\\bar{y}^{i}\\) is then visualized in Figure 7.\n\nIn the Maze task, HRM appears to initially explore several potential paths simultaneously, subsequently eliminating blocked or inefficient routes, then constructing a preliminary solution outline followed by multiple refinement iterations. In Sudoku, the strategy resembles a depth-first search\n\napproach, where the model appears to explore potential solutions and backtracks when it hits dead ends. HRM uses a different approach for ARC tasks, making incremental adjustments to the board and iteratively improving it until reaching a solution. Unlike Sudoku, which involves frequent backtracking, the ARC solution path follows a more consistent progression similar to hill-climbing optimization. Importantly, the model shows that it can adapt to different reasoning approaches, likely choosing an effective strategy for each particular task. Further research is needed to gain more comprehensive insights into these solution strategies.\n\n## 4 Brain Correspondence\n\nA key principle from systems neuroscience is that a brain region's functional repertoire--its ability to handle diverse and complex tasks--is closely linked to the dimensionality of its neural representations [75, 76]. Higher-order cortical areas, responsible for complex reasoning and decision-making, must handle a wide variety of tasks, demanding more flexible and context-dependent processing [77]. In dynamical systems, this flexibility is often realized through higher-dimensional state-space trajectories, which allow for a richer repertoire of potential computations [78]. This principle gives rise to an observable _dimensionality hierarchy_, where a region's position in the processing hierarchy correlates with its _effective dimensionality_. To quantify this phenomenon, we can examine the\n\nFigure 7: **Visualization of intermediate predictions by HRM on benchmark tasks.****Top:**_MazeHard_—blue cells indicate the predicted path. **Middle:**_Sudoku-Extreme_—bold cells represent initial givens; red highlights cells violating Sudoku constraints; grey shading indicates changes from the previous timestep. **Bottom:** ARC-AGI-2 Task—left: provided example input-output pair; right: intermediate steps solving the test input.\n\nFigure 8: **Hierarchical Dimensionality Organization in the HRM and Mouse Cortex.****(a,b)** are adapted from Posani et al. 74. (a) Anatomical illustration of mouse cortical areas, color-coded by functional modules. (b) Correlation between Participation Ratio (PR), a measure of effective neural dimensionality, and hierarchical position across different mouse cortical areas. Higher positions in the hierarchy (e.g., MOs, ACAd) exhibit significantly higher PR values compared to lower sensory areas (e.g., SSp-n), with a Spearman correlation coefficient of \\(\\rho\\) = 0.79 (P = 0.0003). (c,d) **Trained HRM.** (c) PR scaling of the trained HRM with task diversity. The dimensionality of the high-level module (\\(z_{H}\\)) scales with the number of unique tasks (trajectories) included in the analysis, indicating an adaptive expansion of its representational capacity. In contrast, the low-level module’s (\\(z_{L}\\)) dimensionality remains stable. (d) PR values for the low-level (\\(z_{L}\\), PR = 30.22) and high-level (\\(z_{H}\\), PR = 89.95) modules of the _trained_ HRM, computed from neural activity during 100 unique Sudoku-solving trajectories. A clear dimensionality hierarchy is observed, with the high-level module operating in a substantially higher-dimensional space. (e,f) **Analysis of Untrained Network.** To verify that the dimensionality hierarchy is an emergent property of training, the same analyses were performed on an _untrained_ HRM with random weights. (e) In contrast to the trained model’s scaling in (c), the dimensionality of both modules in the untrained model remains low and stable, failing to scale with the number of tasks. (f) Similarly, contrasting with the clear separation in (d), the PR values for the untrained model’s modules (\\(z_{L}\\), PR = 42.09; \\(z_{H}\\), PR = 40.75) are low and nearly identical, showing no evidence of hierarchical separation. This confirms that the observed hierarchical organization of dimensionality is a learned property that emerges through training, not an artifact of the model’s architecture.\n\nParticipation Ratio (PR), which serves as a standard measure of the effective dimensionality of a high-dimensional representation [79]. The PR is calculated using the formula\n\n\\[\\text{PR}=\\frac{(\\sum_{i}\\lambda_{i})^{2}}{\\sum_{i}\\lambda_{i}^{2}}\\,,\\]\n\nwhere \\(\\{\\lambda_{i}\\}\\) are the eigenvalues of the covariance matrix of neural trajectories. Intuitively, a higher PR value signifies that variance is distributed more evenly across many dimensions, corresponding to a higher-dimensional representation. Conversely, a lower PR value indicates that variance is concentrated in only a few principal components, reflecting a more compact, lower-dimensional structure.\n\nThe dimensionality hierarchy can be observed, for example, in the mouse cortex, where the PR of population activity increases monotonically from low-level sensory areas to high-level associative areas, supporting this link between dimensionality and functional complexity [74] (Figure 8 (a,b)).\n\nWe evaluated whether HRM reproduces this neuroscientific principle by calculating the PR for both recurrent modules after training on the _Sudoku-Extreme Full_ dataset. The PR computation used the covariance matrix derived from neural states gathered across multiple Sudoku-solving trajectories. The results show a striking parallel to the biological findings. The low-level module's state (\\(z_{L}\\)) occupies a relatively small subspace with a participation ratio of 30.22, whereas the high-level module's state (\\(z_{H}\\)) operates in a substantially larger subspace with a participation ratio of 89.95, as shown in Figure 8(c). Furthermore, Figure 8(d) shows that increasing the number of unique tasks (trajectories) from 10 to 100 causes \\(z_{H}\\) dimensionality to scale up accordingly, while \\(z_{L}\\) dimensionality remains stable. These results suggest an _emergent_ separation of representational capacity between the modules that parallels their functional roles.\n\nTo confirm that this hierarchical organization is an emergent property of training, and not an artifact of the network's architecture, we performed a control analysis using an identical but untrained network with random weights.\n\nWe initialized an identical HRM architecture with random weights and, without any training, measured the PR of its modules as the network processed the same task-specific inputs given to the trained model.\n\nThe results, shown in Figure 8(e,f), reveal a stark contrast: the high-level and low-level modules of the untrained network exhibit no hierarchical separation, with their PR values remaining low and nearly indistinguishable from each other. This control analysis validates that the dimensionality hierarchy is an _emergent property_ that arises as the model learns to perform complex reasoning.\n\nThe high-to-low PR ratio in HRM (\\(z_{H}/z_{L}\\approx 2.98\\)) closely matches that measured in the mouse cortex (\\(\\approx 2.25\\)). In contrast, conventional deep networks often exhibit _neural collapse_, where last-layer features converge to a low-dimensional subspace [80, 81, 82]. HRM therefore departs from the collapse pattern and instead fosters a high-dimensional representation in its higher module. This is significant because such representations are considered crucial for cognitive flexibility and are a hallmark of higher-order brain regions like the prefrontal cortex (PFC), which is central to complex reasoning.\n\nThis structural parallel suggests the model has discovered a fundamental organizational principle. By learning to partition its representations into a high-capacity, high-dimensional subspace (\\(z_{H}\\)) and a more specialized, low-dimensional one (\\(z_{L}\\)), HRM autonomously discovers an organizational\n\nprinciple that is thought to be fundamental for achieving robust and flexible reasoning in biological systems. This provides a potential mechanistic explanation for the model's success on complex, long-horizon tasks that are intractable for models lacking such a differentiated internal structure. We emphasize, however, that this evidence is correlational. While a causal link could be tested via intervention (e.g., by constraining the H-module's dimensionality), such methods are difficult to interpret in deep learning due to potential confounding effects on the training process itself. Thus, the causal necessity of this emergent hierarchy remains an important question for future investigation.\n\n## 5 Related Work\n\n#### 5.0.1 Reasoning and algorithm learning\n\nGiven the central role of reasoning problems and their close relation to algorithms, researchers have long explored neural architectures that enable algorithm learning from training instances. This line of work includes Neural Turing Machines (NTM)[83], the Differentiable Neural Computer (DNC)[84], and Neural GPUs[85]-all of which construct iterative neural architectures that mimic computational hardware for algorithm execution, and are trained to learn algorithms from data. Another notable work in this area is Recurrent Relational Networks (RRN)[62], which executes algorithms on graph representations through graph neural networks.\n\nRecent studies have integrated algorithm learning approaches with Transformer-based architectures. Universal Transformers extend the standard Transformer model by introducing a recurrent loop over the layers and implementing an adaptive halting mechanism. Geiping et al.[86] demonstrate that looped Transformers can generalize to a larger number of recurrent steps during inference than what they were trained on. Shen et al.[16] propose adding continuous recurrent reasoning tokens to the Transformer. Finally, TransNAR[8] combine recurrent graph neural networks with language models.\n\nBuilding on the success of CoT-based reasoning, a line of work have introduced fine-tuning methods that use reasoning paths from search algorithms (like A*) as SFT targets[87, 71, 70].\n\nWe also mention adaptive halting mechanisms designed to allocate additional computational resources to more challenging problems. This includes the Adaptive Computation Time (ACT) for RNNs[88] and follow-up research like PonderNet[89], which aims to improve the stability of this allocation process.\n\nHRM further pushes the boundary of algorithm learning through a brain-inspired computational architecture that achieves exceptional data efficiency and model expressiveness, successfully discovering complex and diverse algorithms from just 1000 training examples.\n\n#### 5.0.2 Brain-inspired reasoning architectures\n\nDeveloping a model with the reasoning power of the brain has long been a goal in brain-inspired computing. Spaun[90] is one notable example, which uses spiking neural networks to create distinct modules corresponding to brain regions like the visual cortex and prefrontal cortex. This design enables an architecture to perform a range of cognitive tasks, from memory recall to simple reasoning puzzles. However, its reasoning relies on hand-designed algorithms, which may limit its ability to learn new tasks. Another significant model is the Tolman-Eichenbaum Machine (TEM)[91], which is inspired by the hippocampal-entorhinal system's role in spatial and relational memory tasks. TEM proposes that medial entorhinal cells create a basis for structural knowledge, while hippocampal cells link this basis to sensory information. This allows TEM to generalize and explains the emergence of various cell types like grid, border, and\n\nplace cells. Another approach involves neural sampling models [92], which view the neural signaling process as inference over a distribution, functioning similarly to a Boltzmann machine. These models often require hand-made rules to be set up for solving a specific reasoning task. In essence, while prior models are restricted to simple reasoning problems, HRM is designed to solve complex tasks that are hard for even advanced LLMs, without pre-training or task-specific manual design.\n\n**Hierarchical memory**: The hierarchical multi-timescale structure also plays an important role in how the brain processes memory. Models such as Hierarchical Sequential Models [93] and Clockwork RNN [94] use multiple recurrent modules that operate at varying time scales to more effectively capture long-range dependencies within sequences, thereby mitigating the forgetting issue in RNNs.\n\nSimilar mechanisms have also been adopted in linear attention methods for memorizing long contexts (see the Discussions section). Since HRM focuses on reasoning, full attention is applied for simplicity. Incorporating hierarchical memory into HRM could be a promising future direction.\n\n## 6 Discussions\n\n**Turing-completeness of HRM**: Like earlier neural reasoning algorithms including the Universal Transformer [95], HRM is computationally universal when given sufficient memory and time constraints. In other words, it falls into the category of models that can simulate any Turing machine, overcoming the computational limitations of standard Transformers discussed previously in the introduction. Given that earlier neural algorithm reasoners were trained as recurrent neural networks, they suffer from premature convergence and memory intensive BPTT. Therefore, in practice, their effective computational depth remains limited, though still deeper than that of a standard Transformer. By resolving these two challenges and being equipped with adaptive computation, HRM could be trained on long reasoning processes, solve complex puzzles requiring intensive depth-first search and backtracking, and move closer to practical Turing-completeness.\n\n**Reinforcement learning with chain-of-thought**: Beyond fine-tuning using human-annotated CoT, reinforcement learning (RL) represents another widely adopted training methodology. However, recent evidence suggests that RL primarily unlocks existing CoT-like capabilities rather than discovering fundamentally new reasoning mechanisms [96, 97, 98, 99]. Additionally, CoT-training with RL is known for its instability and data inefficiency, often requiring extensive exploration and careful reward design. In contrast, HRM takes feedback from dense gradient-based supervision rather than relying on a sparse reward signal. Moreover, HRM operates naturally in a continuous space, which is biologically plausible and avoids allocating same computational resources to each token, even though tokens vary in their reasoning and planning complexity [16].\n**Linear attention**: Recurrence has been explored not only for its capability in universal computation, but also as a means to replace the attention mechanism in Transformers, which suffers from quadratic time and memory complexity [100]. Recurrent alternatives offer a more efficient design by processing input tokens sequentially and predicting the next token at each time step, similar to early RNN-based language models.\n\nSome linear-attention variants, such as Log-linear Attention [101], share an RNN-like state-update that can be interpreted as propagating multi-timescale summary statistics, thereby retaining long-range context without the quadratic memory growth of standard self-attention. However, substituting the attention mechanism alone does not change the fact that Transformers are still fixed-depth, and require CoT as a compensatory mechanism. Notably, linear attention can operate with a reduced\n\nkey-value cache over extended contexts, making them more suitable for deployment on resource-constrained edge devices.\n\n## 7 Conclusion\n\nThis work introduces the Hierarchical Reasoning Model, a brain-inspired architecture that leverages hierarchical structure and multi-timescale processing to achieve substantial computational depth without sacrificing training stability or efficiency. With only 27M parameters and training on just 1000 examples, HRM effectively solves challenging reasoning problems such as ARC, Sudoku, and complex maze navigation-tasks that typically pose significant difficulties for contemporary LLM and chain-of-thought models.\n\nAlthough the brain relies heavily on hierarchical structures to enable most cognitive processes, these concepts have largely remained confined to academic literature rather than being translated into practical applications. The prevailing AI approach continues to favor non-hierarchical models. Our results challenge this established paradigm and suggest that the Hierarchical Reasoning Model represents a viable alternative to the currently dominant chain-of-thought reasoning methods, advancing toward a foundational framework capable of Turing-complete universal computation.\n\n**Acknowledgements** We thank Mingli Yuan, Ahmed Murtadha Hasan Mahyoub and Hengshuai Yao for their insightful discussions and valuable feedback throughout the course of this work.\n\n## References\n\n* [1] Ian Goodfellow, Yoshua Bengio, and Aaron Courville. _Deep Learning_. MIT Press, 2016. http://www.deeplearningbook.org.\n* [2] Kaiming He, X. Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. _2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 770-778, 2015.\n* [3] Lena Strobl. Average-hard attention transformers are constant-depth uniform threshold circuits, 2023.\n* [4] Tom Bylander. Complexity results for planning. In _Proceedings of the 12th International Joint Conference on Artificial Intelligence - Volume 1_, IJCAI'91, page 274-279, San Francisco, CA, USA, 1991. Morgan Kaufmann Publishers Inc. ISBN 1558601600.\n* [5] William Merrill and Ashish Sabharwal. A logic for expressing log-precision transformers. In _Neural Information Processing Systems_, 2023.\n* [6] David Chiang. Transformers in DLOGTIME-uniform TC\\({}^{0}\\). _Transactions on Machine Learning Research_, 2025.\n* [7] Lucas Lehnert, Sainbayar Sukhbaatar, DiJia Su, Qinqing Zheng, Paul McVay, Michael Rabbat, and Yuandong Tian. Beyond a*: Better planning with transformers via search dynamics bootstrapping. In _First Conference on Language Modeling_, 2024.\n* [8] Wilfried Bounsi, Borja Ibarz, Andrew Dudzik, Jessica B. Hamrick, Larisa Markeeva, Alex Vitvitskyi, Razvan Pascanu, and Petar Velivckovic'c. Transformers meet neural algorithmic reasoners. _ArXiv_, abs/2406.09308, 2024.\n* [9] William Merrill and Ashish Sabharwal. The parallelism tradeoff: Limitations of log-precision transformers. _Transactions of the Association for Computational Linguistics_, 11:531-545, 2023. doi: 10.1162/tacl_a_00562.\n* [10] Jason Wei, Yi Tay, et al. Chain-of-thought prompting elicits reasoning in large language models, 2022. arXiv preprint arXiv:2201.11903.\n* [11] William Merrill and Ashish Sabharwal. The expressive power of transformers with chain of thought. In _ICLR_, 2024.\n* [12] Xinyun Chen, Ryan A. Chi, Xuezhi Wang, and Denny Zhou. Premise order matters in reasoning with large language models. _ArXiv_, abs/2402.08939, 2024.\n* [13] Rongwu Xu, Zehan Qi, and Wei Xu. Preemptive answer \"attacks\" on chain-of-thought reasoning. In _Annual Meeting of the Association for Computational Linguistics_, 2024.\n* [14] Pablo Villalobos, Anson Ho, Jaime Sevilla, Tamay Besiroglu, Lennart Heim, and Marius Hobbhahn. Will we run out of data? limits of llm scaling based on human-generated data. _arXiv preprint arXiv:2211.04325_, 2022.\n* [15] Xinghao Chen, Anhao Zhao, Heming Xia, Xuan Lu, Hanlin Wang, Yanjun Chen, Wei Zhang, Jian Wang, Wenjie Li, and Xiaoyu Shen. Reasoning beyond language: A comprehensive survey on latent chain-of-thought reasoning, 2025.\n* [16] Xuan Shen, Yizhou Wang, Xiangxi Shi, Yanzhi Wang, Pu Zhao, and Jiuxiang Gu. Training large language models to reason in a continuous latent space. _arXiv preprint arXiv:2412.07423_, 2024.\n\n* Fedorenko et al. [2024] Evelina Fedorenko, Steven T Piantadosi, and Edward AF Gibson. Language is primarily a tool for communication rather than thought. _Nature_, 630(8017):575-586, 2024.\n* Wang et al. [2024] Hongyu Wang, Shuming Ma, Li Dong, Shaohan Huang, Dongdong Zhang, and Furu Wei. Deepnet: Scaling transformers to 1,000 layers. _IEEE Transactions on Pattern Analysis and Machine Intelligence_, 2024.\n* Lillicrap and Santoro [2019] Timothy P Lillicrap and Adam Santoro. Backpropagation through time and the brain. _Current Opinion in Neurobiology_, 55:82-89, 2019. ISSN 0959-4388. doi: https://doi.org/10.1016/j.conb.2019.01.011.\n* Murray et al. [2014] John D Murray, Alberto Bernacchia, David J Freedman, Ranulfo Romo, Jonathan D Wallis, Xinying Cai, Camillo Padoa-Schioppa, Tatiana Pasternak, Hyojung Seo, Daeyeol Lee, et al. A hierarchy of intrinsic timescales across primate cortex. _Nature neuroscience_, 17(12):1661-1663, 2014.\n* Zeraati et al. [2023] Roxana Zeraati, Yan-Liang Shi, Nicholas A Steinmetz, Marc A Gieselmann, Alexander Thiele, Tirin Moore, Anna Levina, and Tatiana A Engel. Intrinsic timescales in the visual cortex change with selective attention and reflect spatial connectivity. _Nature communications_, 14(1):1858, 2023.\n* Huntenburg et al. [2018] Julia M Huntenburg, Pierre-Louis Bazin, and Daniel S Margulies. Large-scale gradients in human cortical organization. _Trends in cognitive sciences_, 22(1):21-31, 2018.\n* Lamme and Roelfsema [2000] Victor AF Lamme and Pieter R Roelfsema. The distinct modes of vision offered by feedforward and recurrent processing. _Trends in neurosciences_, 23(11):571-579, 2000.\n* Bastos et al. [2012] Andre M Bastos, W Martin Usrey, Rick A Adams, George R Mangun, Pascal Fries, and Karl J Friston. Canonical microcircuits for predictive coding. _Neuron_, 76(4):695-711, 2012.\n* Kaleb et al. [2024] Klara Kaleb, Barbara Feulner, Juan Gallego, and Claudia Clopath. Feedback control guides credit assignment in recurrent neural networks. _Advances in Neural Information Processing Systems_, 37:5122-5144, 2024.\n* Lillicrap et al. [2020] Timothy P Lillicrap, Adam Santoro, Luke Marris, Colin J Akerman, and Geoffrey Hinton. Backpropagation and the brain. _Nature Reviews Neuroscience_, 21(6):335-346, 2020.\n* Chollet [2019] Francois Chollet. On the measure of intelligence (abstraction and reasoning corpus), 2019. arXiv preprint arXiv:1911.01547.\n* Chollet et al. [2024] Francois Chollet, Mike Knoop, Gregory Kamradt, and Bryan Landers. Arc prize 2024: Technical report. _ArXiv_, abs/2412.04604, 2024.\n* Chollet et al. [2025] Francois Chollet, Mike Knoop, Gregory Kamradt, Bryan Landers, and Henry Pinkard. Arcagi-2: A new challenge for frontier ai reasoning systems. _arXiv preprint arXiv:2505.11831_, 2025.\n* Buzsaki [2000] Gyorgy Buzsaki. Gamma, alpha, delta, and theta oscillations govern cognitive processes. _International Journal of Psychophysiology_, 39:241-248, 2000.\n* Buzsaki [2006] Gyorgy Buzsaki. _Rhythms of the Brain_. Oxford university press, 2006.\n* Pahor and Jausovec [2014] Anja Pahor and Norbert Jausovec. Theta-gamma cross-frequency coupling relates to the level of human intelligence. _Intelligence_, 46:283-290, 2014.\n* Tort et al. [2009] Adriano BL Tort, Robert W Komorowski, Joseph R Manns, Nancy J Kopell, and Howard Eichenbaum. Theta-gamma coupling increases during the learning of item-context associations. _Proceedings of the National Academy of Sciences_, 106(49):20942-20947, 2009.\n\n* [34] Benjamin Scellier and Yoshua Bengio. Equilibrium propagation: Bridging the gap between energy-based models and backpropagation. _Frontiers in Computational Neuroscience_, 11, 2016.\n* [35] Guillaume Bellec, Franz Scherr, Anand Subramoney, Elias Hajek, Darjan Salaj, Robert Legenstein, and Wolfgang Maass. A solution to the learning dilemma for recurrent networks of spiking neurons. _Nature Communications_, 11, 07 2020. doi: 10.1038/s41467-020-17236-y.\n* [36] Shaojie Bai, J Zico Kolter, and Vladlen Koltun. Deep equilibrium models. In _Advances in Neural Information Processing Systems_, pages 690-701, 2019.\n* [37] Zhengyang Geng, Xinyu Zhang, Shaojie Bai, Yisen Wang, and Zhouchen Lin. On training implicit models. _ArXiv_, abs/2111.05177, 2021.\n* [38] Katarina Begus and Elizabeth Bonawitz. The rhythm of learning: Theta oscillations as an index of active learning in infancy. _Developmental Cognitive Neuroscience_, 45:100810, 2020. ISSN 1878-9293. doi: https://doi.org/10.1016/j.dcn.2020.100810.\n* [39] Shaojie Bai, Zhengyang Geng, Yash Savani, and J. Zico Kolter. Deep Equilibrium Optical Flow Estimation . In _2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 610-620, 2022.\n* [40] Zaccharie Ramzi, Florian Mannel, Shaojie Bai, Jean-Luc Starck, Philippe Ciuciu, and Thomas Moreau. Shine: Sharing the inverse estimate from the forward pass for bi-level optimization and implicit models. _ArXiv_, abs/2106.00553, 2021.\n* [41] Shaojie Bai, Vladlen Koltun, and J. Zico Kolter. Stabilizing equilibrium models by jacobian regularization. In _International Conference on Machine Learning_, 2021.\n* [42] Daniel Kahneman and P Egan. Thinking, fast and slow (farrar, straus and giroux, new york), 2011.\n* [43] Matthew D Lieberman. Social cognitive neuroscience: a review of core processes. _Annu. Rev. Psychol._, 58(1):259-289, 2007.\n* [44] Randy L Buckner, Jessica R Andrews-Hanna, and Daniel L Schacter. The brain's default network: anatomy, function, and relevance to disease. _Annals of the new York Academy of Sciences_, 1124(1):1-38, 2008.\n* [45] Marcus E Raichle. The brain's default mode network. _Annual review of neuroscience_, 38(1):433-447, 2015.\n* [46] Andrew Westbrook and Todd S Braver. Cognitive effort: A neuroeconomic approach. _Cognitive, Affective, & Behavioral Neuroscience_, 15:395-415, 2015.\n* [47] Richard S. Sutton and Andrew G. Barto. _Reinforcement Learning: An Introduction_. MIT Press, Cambridge, MA, 2018.\n* [48] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and Martin A. Riedmiller. Playing atari with deep reinforcement learning. _ArXiv_, abs/1312.5602, 2013.\n* [49] Matteo Gallici, Mattie Fellows, Benjamin Ellis, Bartomeu Pou, Ivan Masmitja, Jakob Nicolaus Foerster, and Mario Martin. Simplifying deep temporal difference learning, 2025.\n\n* [50] Shuo Xie and Zhiyuan Li. Implicit bias of adamw: L inf norm constrained optimization. _ArXiv_, abs/2404.04454, 2024.\n* [51] Lucas Prieto, Melih Barsbey, Pedro A. M. Mediano, and Tolga Birdal. Grokking at the edge of numerical stability. In _The Thirteenth International Conference on Learning Representations_, 2025.\n* [52] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In _Advances in neural information processing systems_, pages 5998-6008, 2017.\n* [53] Meta AI. Llama 3: State-of-the-art open weight language models. Technical report, Meta, 2024. URL https://ai.meta.com/llama/.\n* [54] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. _Neurocomputing_, 568:127063, 2024.\n* [55] Noam M. Shazeer. Glu variants improve transformer. _ArXiv_, abs/2002.05202, 2020.\n* [56] Biao Zhang and Rico Sennrich. Root mean square layer normalization. _ArXiv_, abs/1910.07467, 2019.\n* [57] Gunter Klambauer, Thomas Unterthiner, Andreas Mayr, and Sepp Hochreiter. Self-normalizing neural networks. In _Neural Information Processing Systems_, 2017.\n* [58] JAX Developers. _jax.nn.initializers.lecun_normal_. Google Research, 2025. URL https://docs.jax.dev/en/latest/_autosummary/jax.nn.initializers.lecun_normal.html. Accessed June 22, 2025.\n* [59] Yann LeCun, Leon Bottou, Genevieve B Orr, and Klaus-Robert Muller. Efficient backprop. In _Neural networks: Tricks of the trade_, pages 9-50. Springer, 2002.\n* [60] Katie E Everett, Lechao Xiao, Mitchell Wortsman, Alexander A Alemi, Roman Novak, Peter J Liu, Izzeddin Gur, Jascha Sohl-Dickstein, Leslie Pack Kaelbling, Jaehoon Lee, and Jeffrey Pennington. Scaling exponents across parameterizations and optimizers. In _Forty-first International Conference on Machine Learning_, 2024.\n* [61] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization, 2017.\n* [62] Rasmus Berg Palm, Ulrich Paquet, and Ole Winther. Recurrent relational networks. In _Neural Information Processing Systems_, 2017.\n* [63] Jieyi Long. Large language model guided tree-of-thought. _ArXiv_, abs/2305.08291, 2023.\n* [64] Yilun Du, Jiayuan Mao, and Josh Tenenbaum. Learning iterative reasoning through energy diffusion. _ArXiv_, abs/2406.11179, 2024.\n* [65] Kyubyong Park. Can convolutional neural networks crack sudoku puzzles? https://github.com/Kyubyong/sudoku, 2018.\n* [66] Single-digit techniques. https://hodoku.sourceforge.net/en/tech_singles.php. Accessed: 2025-06-16.\n* [67] Tom Dillion. Tdoku: A fast sudoku solver and generator. https://t-dillon.github.io/tdoku/, 2025.\n* [68] Jeffrey Seely, Yuki Imajuku, Tianyu Zhao, Edoardo Cetin, and Llion Jones. Sudoku-bench: Evaluating creative reasoning with sudoku variants. _arXiv preprint arXiv:2505.16135_, 2025.\n* [69] Luke Darlow, Ciaran Regan, Sebastian Risi, Jeffrey Seely, and Llion Jones. Continuous thought machines. _arXiv preprint arXiv:2505.05522_, 2025.\n\n* Su et al. [2025] DiJia Su, Sainbayar Sukhbaatar, Michael Rabbat, Yuandong Tian, and Qinqing Zheng. Dualformer: Controllable fast and slow thinking by learning with randomized reasoning traces, 2025.\n* Lehnert et al. [2024] Lucas Lehnert, Sainbayar Sukhbaatar, DiJia Su, Qinqing Zheng, Paul McVay, Michael Rabbat, and Yuandong Tian. Beyond a*: Better planning with transformers via search dynamics bootstrapping. In _First Conference on Language Modeling_, 2024.\n* Kapadia et al. [2013] Mubbasir Kapadia, Francisco Garcia, Cory D. Boatright, and Norman I. Badler. Dynamic search on the gpu. In _2013 IEEE/RSJ International Conference on Intelligent Robots and Systems_, pages 3332-3337, 2013. doi: 10.1109/IROS.2013.6696830.\n* Liao and Gu [2025] Isaac Liao and Albert Gu. Arc-agi without pretraining, 2025. URL https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/arc_agi_without_pretraining.html.\n* Posani et al. [2025] Lorenzo Posani, Shuqi Wang, Samuel P Muscinelli, Liam Paninski, and Stefano Fusi. Rarely categorical, always high-dimensional: how the neural code changes along the cortical hierarchy. _bioRxiv_, pages 2024-11, 2025.\n* Rigotti et al. [2013] Mattia Rigotti, Omri Barak, Melissa R. Warden, Xiao-Jing Wang, Nathaniel D. Daw, Earl K. Miller, and Stefano Fusi. The importance of mixed selectivity in complex cognitive tasks. _Nature_, 497:585-590, 2013. doi: 10.1038/nature12160.\n* Mante et al. [2013] Valerio Mante, David Sussillo, Krishna V. Shenoy, and William T. Newsome. Context-dependent computation by recurrent dynamics in prefrontal cortex. _Nature_, 503(7474):78-84, 2013. doi: 10.1038/nature12742.\n* Miller and Cohen [2001] Earl K. Miller and Jonathan D. Cohen. An integrative theory of prefrontal cortex function. _Annual Review of Neuroscience_, 24(1):167-202, 2001. doi: 10.1146/annurev.neuro.24.1.167.\n* Maass [2002] Wolfgang Maass. Real-time computing without stable states: a new framework for neural computation based on perturbations. _Neural Computation_, 14(11):2531-2560, 2002. doi: 10.1162/089976602760407955.\n* Altan et al. [2021] Ege Altan, Sara A. Solla, Lee E. Miller, and Eric J. Perreault. Estimating the dimensionality of the manifold underlying multi-electrode neural recordings. _PLoS Computational Biology_, 17(11):e1008591, 2021. doi: 10.1371/journal.pcbi.1008591.\n* Papyan et al. [2020] Vardan Papyan, X. Y. Han, and David L. Donoho. Prevalence of neural collapse during the terminal phase of deep learning training. _Proceedings of the National Academy of Sciences_, 117(40):24652-24663, 2020. doi: 10.1073/pnas.2015509117.\n* Fang et al. [2021] Cong Fang, Hangfeng He, Qi Long, and Weijie J. Su. Exploring deep neural networks via layer-peeled model: Minority collapse in imbalanced training. _Proceedings of the National Academy of Sciences_, 118(43):e2103091118, 2021. doi: 10.1073/pnas.2103091118.\n* Zhu et al. [2021] Zhihui Zhu, Tianyu Ding, Jinxin Zhou, Xiao Li, Chong You, Jeremias Sulam, and Qing Qu. A geometric analysis of neural collapse with unconstrained features. In _Advances in Neural Information Processing Systems_, volume 34 of _NeurIPS_, pages 29820-29834, 2021.\n* Graves et al. [2014] Alex Graves, Greg Wayne, and Ivo Danihelka. Neural turing machines, 2014.\n* Graves et al. [2016] Alex Graves, Greg Wayne, Malcolm Reynolds, Tim Harley, Ivo Danihelka, Agnieszka Grabska-Barwinska, Sergio Gomez Colmenarejo, Edward Grefenstette, Tiago Ramalho, John Agapiou, et al. Hybrid computing using a neural network with dynamic external memory. _Nature_, 538(7626):471-476, 2016.\n\n* [85] Lukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In _ICLR_, 2016.\n* [86] Jonas Geiping, Sean McLeish, Neel Jain, John Kirchenbauer, Siddharth Singh, Brian R. Bartoldson, Bhavya Kailkhura, Abhinav Bhatele, and Tom Goldstein. Scaling up test-time compute with latent reasoning: A recurrent depth approach, 2025.\n* [87] Tiedong Liu and Kian Hsiang Low. Goat: Fine-tuned llama outperforms gpt-4 on arithmetic tasks. _ArXiv_, abs/2305.14201, 2023.\n* [88] Alex Graves. Adaptive computation time for recurrent neural networks. _ArXiv_, abs/1603.08983, 2016.\n* [89] Andrea Banino, Jan Balaguer, and Charles Blundell. Pondernet: Learning to ponder. _ArXiv_, abs/2107.05407, 2021.\n* [90] Chris Eliasmith, Terrence C Stewart, Xuan Choo, Trevor Bekolay, Travis DeWolf, Yichuan Tang, and Daniel Rasmussen. A large-scale model of the functioning brain. _science_, 338 (6111):1202-1205, 2012.\n* [91] James CR Whittington, Timothy H Muller, Shirley Mark, Guifen Chen, Caswell Barry, Neil Burgess, and Timothy EJ Behrens. The tolman-eichenbaum machine: unifying space and relational memory through generalization in the hippocampal formation. _Cell_, 183(5):1249-1263, 2020.\n* [92] Lars Buesing, Johannes Bill, Bernhard Nessler, and Wolfgang Maass. Neural dynamics as sampling: a model for stochastic computation in recurrent networks of spiking neurons. _PLoS computational biology_, 7(11):e1002211, 2011.\n* [93] Salah Hihi and Yoshua Bengio. Hierarchical recurrent neural networks for long-term dependencies. In D. Touretzky, M.C. Mozer, and M. Hasselmo, editors, _Advances in Neural Information Processing Systems_, volume 8. MIT Press, 1995.\n* [94] Jan Koutnik, Klaus Greff, Faustino J. Gomez, and Jurgen Schmidhuber. A clockwork rnn. In _International Conference on Machine Learning_, 2014.\n* [95] Mostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob Uszkoreit, and Lukasz Kaiser. Universal transformers, 2018. arXiv preprint arXiv:1807.03819.\n* [96] Yiping Wang, Qing Yang, Zhiyuan Zeng, Liliang Ren, Lucas Liu, Baolin Peng, Hao Cheng, Xuehai He, Kuan Wang, Jianfeng Gao, Weizhu Chen, Shuohang Wang, Simon Shaolei Du, and Yelong Shen. Reinforcement learning for reasoning in large language models with one training example, 2025. URL https://arxiv.org/abs/2504.20571.\n* [97] Niklas Muennighoff. s1: Simple test-time scaling. _arXiv preprint arXiv:2502.23456_, 2025.\n* [98] Liang Wen, Yunke Cai, Fenrui Xiao, Xin He, Qi An, Zhenyu Duan, Yimin Du, Junchen Liu, Lifu Tang, Xiaowei Lv, Haosheng Zou, Yongchao Deng, Shousheng Jia, and Xiangzheng Zhang. Light-r1: Curriculum sft, dpo and rl for long cot from scratch and beyond, 2025.\n* [99] Xuefeng Li, Haoyang Zou, and Pengfei Liu. Limr: Less is more for rl scaling, 2025.\n* [100] Tri Dao and Albert Gu. Transformers are ssms: Generalized models and efficient algorithms through structured state space duality. _ArXiv_, abs/2405.21060, 2024.\n* [101] Han Guo, Songlin Yang, Tarushii Goel, Eric P Xing, Tri Dao, and Yoon Kim. Log-linear attention. _arXiv preprint arXiv:2506.04761_, 2025.", "abstract": "Reasoning, the process of devising and executing complex goal-oriented action sequences, remains a critical challenge in AI. Current large language models (LLMs) primarily employ Chain-of-Thought (CoT) techniques, which suffer from brittle task decomposition, extensive data requirements, and high latency. Inspired by the hierarchical and multi-timescale processing in the human brain, we propose the Hierarchical Reasoning Model (HRM), a novel recurrent architecture that attains significant computational depth while maintaining both training stability and efficiency. HRM executes sequential reasoning tasks in a single forward pass without explicit supervision of the intermediate process, through two interdependent recurrent modules: a high-level module responsible for slow, abstract planning, and a low-level module handling rapid, detailed computations. With only 27 million parameters, HRM achieves exceptional performance on complex reasoning tasks using only 1000 training samples. The m", "references": [], "parse_results": [{"parser_name": "Nougat", "success": true, "quality": "excellent", "content": "# Hierarchical Reasoning Model\n\n Guan Wang\\({}^{1,\\dagger}\\), Jin Li\\({}^{1}\\), Yuhao Sun\\({}^{1}\\), Xing Chen\\({}^{1}\\), Changling Liu\\({}^{1}\\),\n\nYue Wu\\({}^{1}\\), Meng Lu\\({}^{1,\\dagger}\\), Sen Song\\({}^{2,\\dagger}\\), Yasin Abbasi Yadkori\\({}^{1,\\dagger}\\)\n\n\\({}^{1}\\)Sapient Intelligence, Singapore\n\n###### Abstract\n\nReasoning, the process of devising and executing complex goal-oriented action sequences, remains a critical challenge in AI. Current large language models (LLMs) primarily employ Chain-of-Thought (CoT) techniques, which suffer from brittle task decomposition, extensive data requirements, and high latency. Inspired by the hierarchical and multi-timescale processing in the human brain, we propose the Hierarchical Reasoning Model (HRM), a novel recurrent architecture that attains significant computational depth while maintaining both training stability and efficiency. HRM executes sequential reasoning tasks in a single forward pass without explicit supervision of the intermediate process, through two interdependent recurrent modules: a high-level module responsible for slow, abstract planning, and a low-level module handling rapid, detailed computations. With only 27 million parameters, HRM achieves exceptional performance on complex reasoning tasks using only 1000 training samples. The model operates without pre-training or CoT data, yet achieves nearly perfect performance on challenging tasks including complex Sudoku puzzles and optimal path finding in large mazes. Furthermore, HRM outperforms much larger models with significantly longer context windows on the Abstraction and Reasoning Corpus (ARC), a key benchmark for measuring artificial general intelligence capabilities. These results underscore HRM's potential as a transformative advancement toward universal computation and general-purpose reasoning systems.\n\n\n\nFigure 1: **Left:** HRM is inspired by hierarchical processing and temporal separation in the brain. It has two recurrent networks operating at different timescales to collaboratively solve tasks. **Right:** With only about 1000 training examples, the HRM (\\(\\sim\\)27M parameters) surpasses state-of-the-art CoT models on inductive benchmarks (ARC-AGI) and challenging symbolic tree-search puzzles (_Sudoku-Extreme_, _Maze-Hard_) where CoT models failed completely. The HRM was randomly initialized, and it solved the tasks directly from inputs without chain of thoughts.\n\nIntroduction\n\nDeep learning, as its name suggests, emerged from the idea of stacking more layers to achieve increased representation power and improved performance [1, 2]. However, despite the remarkable success of large language models, their core architecture is paradoxically shallow [3]. This imposes a fundamental constraint on their most sought-after capability: reasoning. The fixed depth of standard Transformers places them in computational complexity classes such as \\(AC^{0}\\) or \\(TC^{0}\\)[4], preventing them from solving problems that require polynomial time [5, 6]. LLMs are not Turing-complete and thus they cannot, at least in a purely end-to-end manner, execute complex algorithmic reasoning that is necessary for deliberate planning or symbolic manipulation tasks [7, 8]. For example, our results on the Sudoku task show that increasing Transformer model depth _can_ improve performance,1 but performance remains far from optimal even with very deep models (see Figure 2), which supports the conjectured limitations of the LLM scaling paradigm [9].\n\nFootnote 1: Simply increasing the model width does not improve performance here.\n\nThe LLMs literature has relied largely on Chain-of-Thought (CoT) prompting for reasoning [10]. CoT externalizes reasoning into token-level language by breaking down complex tasks into simpler intermediate steps, sequentially generating text using a shallow model [11]. However, CoT for reasoning is a crutch, not a satisfactory solution. It relies on brittle, human-defined decompositions where a single misstep or a misorder of the steps can derail the reasoning process entirely [12, 13]. This dependency on explicit linguistic steps tethers reasoning to patterns at the token level. As a result, CoT reasoning often requires significant amount of training data and generates a large number of tokens for complex reasoning tasks, resulting in slow response times. A more efficient approach is needed to minimize these data requirements [14].\n\nTowards this goal, we explore \"latent reasoning\", where the model conducts computations within its internal hidden state space [15, 16]. This aligns with the understanding that language is a tool for human communication, not the substrate of thought itself [17]; the brain sustains lengthy, coherent chains of reasoning with remarkable efficiency in a latent space, without constant translation back to language. However, the power of latent reasoning is still fundamentally constrained by a model's _effective computational depth_. Naively stacking layers is notoriously difficult due to vanishing gradients, which plague training stability and effectiveness [1, 18]. Recurrent architectures, a natural alternative for sequential tasks, often suffer from early convergence, rendering subsequent computational steps inert, and rely on the biologically implausible, computationally expensive and memory intensive Backpropagation Through Time (BPTT) for training [19].\n\nThe human brain provides a compelling blueprint for achieving the effective computational depth that contemporary artificial models lack. It organizes computation hierarchically across cortical regions operating at different timescales, enabling deep, multi-stage reasoning [20, 21, 22]. Recurrent feedback loops iteratively refine internal representations, allowing slow, higher-level areas to guide, and fast, lower-level circuits to execute--subordinate processing while preserving global coherence [23, 24, 25]. Notably, the brain achieves such depth without incurring the prohibitive credit-assignment costs that typically hamper recurrent networks from backpropagation through time [19, 26].\n\nInspired by this hierarchical and multi-timescale biological architecture, we propose the Hierarchical Reasoning Model (HRM). HRM is designed to significantly increase the effective computational depth. It features two coupled recurrent modules: a high-level (H) module for abstract, deliberate reasoning, and a low-level (L) module for fast, detailed computations. This structure\n\navoids the rapid convergence of standard recurrent models through a process we term \"hierarchical convergence.\" The slow-updating H-module advances only after the fast-updating L-module has completed multiple computational steps and reached a local equilibrium, at which point the L-module is reset to begin a new computational phase.\n\nFurthermore, we propose a one-step gradient approximation for training HRM, which offers improved efficiency and eliminates the requirement for BPTT. This design maintains a constant memory footprint (\\(O(1)\\) compared to BPTT's \\(O(T)\\) for \\(T\\) timesteps) throughout the backpropagation process, making it scalable and more biologically plausible.\n\nLeveraging its enhanced effective depth, HRM excels at tasks that demand extensive search and backtracking. **Using only 1,000 input-output examples, without pre-training or CoT supervision,** HRM learns to solve problems that are intractable for even the most advanced LLMs. For example, it achieves near-perfect accuracy in complex Sudoku puzzles (_Sudoku-Extreme Full_) and optimal pathfinding in 30x30 mazes, where state-of-the-art CoT methods completely fail (0% accuracy). In the Abstraction and Reasoning Corpus (ARC) AGI Challenge [27, 28, 29] - a benchmark of inductive reasoning - HRM, trained from scratch with only the official dataset (~1000 examples), with only 27M parameters and a 30x30 grid context (900 tokens), achieves a performance of **40.3%**, which substantially surpasses leading CoT-based models like o3-mini-high (34.5%) and Claude 3.7 8K context (21.2%), despite their considerably larger parameter sizes and context lengths, as shown in Figure 1. This represents a promising direction toward the development of next-generation AI reasoning systems with universal computational capabilities.\n\n## 2 Hierarchical Reasoning Model\n\nWe present the HRM, inspired by three fundamental principles of neural computation observed in the brain:\n\n* [leftmargin=*]\n* **Hierarchical processing:** The brain processes information across a hierarchy of cortical areas. Higher-level areas integrate information over longer timescales and form abstract representations, while lower-level areas handle more immediate, detailed sensory and motor processing [20, 22, 21].\n\nFigure 2: **The necessity of depth for complex reasoning. Left:** On _Sudoku-Extreme Full_, which require extensive tree-search and backtracking, increasing a Transformer’s width yields no performance gain, while increasing depth is critical. **Right:** Standard architectures saturates, failing to benefit from increased depth. HRM overcomes this fundamental limitation, effectively using its computational depth to achieve near-perfect accuracy.\n\n* **Temporal Separation:** These hierarchical levels in the brain operate at distinct intrinsic timescales, reflected in neural rhythms (e.g., slow theta waves, 4-8 Hz and fast gamma waves, 30-100 Hz) [30, 31]. This separation allows for stable, high-level guidance of rapid, low-level computations [32, 33].\n* **Recurrent Connectivity:** The brain features extensive recurrent connections. These feedback loops enable iterative refinement, yielding more accurate and context-sensitive representations at the cost of additional processing time. Additionally, the brain largely avoids the problematic deep credit assignment problem associated with BPTT19. Footnote 19: While inspired by temporal separation in the brain, our model’s “high-level” and “low-level” modules are conceptual abstractions and do not map directly to specific neural oscillation frequencies.\n\nThe HRM model consists of four learnable components: an input network \\(f_{I}(\\cdot;\\theta_{I})\\), a low-level recurrent module \\(f_{L}(\\cdot;\\theta_{L})\\), a high-level recurrent module \\(f_{H}(\\cdot;\\theta_{H})\\), and an output network \\(f_{O}(\\cdot;\\theta_{O})\\). The model's dynamics unfold over \\(N\\) high-level cycles of \\(T\\) low-level timesteps each2. We index the total timesteps of one forward pass by \\(i=1,\\ldots,N\\times T\\). The modules \\(f_{L}\\) and \\(f_{H}\\) each keep a hidden state--\\(z_{L}^{i}\\) for \\(f_{L}\\) and \\(z_{H}^{i}\\) for \\(f_{H}\\)--which are initialized with the vectors \\(z_{L}^{0}\\) and \\(z_{H}^{0}\\), respectively.\n\nFootnote 2: While inspired by temporal separation in the brain, our model’s “high-level” and “low-level” modules are conceptual abstractions and do not map directly to specific neural oscillation frequencies.\n\nThe HRM maps an input vector \\(x\\) to an output prediction vector \\(\\hat{y}\\) as follows. First, the input \\(x\\) is projected into a working representation \\(\\tilde{x}\\) by the input network:\n\n\\[\\tilde{x}=f_{I}(x;\\theta_{I})\\ .\\]\n\nAt each timestep \\(i\\), the L-module updates its state conditioned on its own previous state, the H-module's current state (which remains fixed throughout the cycle), and the input representation. The H-module only updates once per cycle (i.e., every \\(T\\) timesteps) using the L-module's final state at the end of that cycle:\n\n\\[z_{L}^{i} =f_{L}\\left(z_{L}^{i-1},z_{H}^{i-1},\\tilde{x};\\theta_{L}\\right)\\ ,\\] \\[z_{H}^{i} =\\begin{cases}f_{H}\\left(z_{H}^{i-1},z_{L}^{i-1};\\theta_{H}\\right) &\\text{if }i\\equiv 0\\,\\left(\\mathrm{mod}\\ T\\right),\\\\ z_{H}^{i-1}&\\text{otherwise}\\ \\ .\\end{cases}\\]\n\nFinally, after \\(N\\) full cycles, a prediction \\(\\hat{y}\\) is extracted from the hidden state of the H-module:\n\n\\[\\hat{y}=f_{O}(z_{H}^{NT};\\theta_{O})\\ .\\]\n\nThis entire \\(NT\\)-timestep process represents a single forward pass of the HRM. A halting mechanism (detailed later in this section) determines whether the model should terminate, in which case \\(\\hat{y}\\) will be used as the final prediction, or continue with an additional forward pass.\n\n**Hierarchical convergence** Although convergence is crucial for recurrent networks, standard RNNs are fundamentally limited by their tendency to converge too early. As the hidden state settles toward a fixed point, update magnitudes shrink, effectively stalling subsequent computation and capping the network's effective depth. To preserve computational power, we actually want convergence to proceed very slowly-but engineering that gradual approach is difficult, since pushing convergence too far edges the system toward instability.\n\nHRM is explicitly designed to counteract this premature convergence through a process we term _hierarchical convergence_. During each cycle, the L-module (an RNN) exhibits stable convergence to a _local equilibrium_. This equilibrium, however, depends on the high-level state \\(z_{H}\\) supplied during that cycle. After completing the \\(T\\) steps, the H-module incorporates the sub-computation's outcome (the final state \\(z_{L}\\)) and performs its own update. This \\(z_{H}\\) update establishes a fresh context for the L-module, essentially \"restarting\" its computational path and initiating a new convergence phase toward a different local equilibrium.\n\nThis process allows the HRM to perform a sequence of distinct, stable, nested computations, where the H-module directs the overall problem-solving strategy and the L-module executes the intensive search or refinement required for each step. Although a standard RNN may approach convergence within \\(T\\) iterations, the hierarchical convergence benefits from an enhanced effective depth of \\(NT\\) steps. As empirically shown in Figure 3, this mechanism allows HRM both to maintain high computational activity (forward residual) over many steps (in contrast to a standard RNN, whose activity rapidly decays) and to enjoy stable convergence. This translates into better performance at any computation depth, as illustrated in Figure 2.\n\n\n\n**Approximate gradient** Recurrent models typically use BPTT to compute gradients. However, BPTT requires storing the hidden states from the forward pass and then combining them with gradients during the backward pass, which demands \\(O(T)\\) memory for T timesteps. This heavy memory burden forces smaller batch sizes and leads to poor GPU utilization, especially for large-scale networks. Additionally, because retaining the full history trace through time is biologically implausible, it is unlikely that the brain implements BPTT [19].\n\nFortunately, if a recurrent neural network converges to a fixed point, we can avoid unrolling its state sequence by applying backpropagation in a single step at that equilibrium point. Moreover, such a mechanism could plausibly be implemented in the brain using only local learning rules [34, 35]. Based\n\nFigure 3: Comparison of forward residuals and PCA trajectories. HRM shows hierarchical convergence: the H-module steadily converges, while the L-module repeatedly converges within cycles before being reset by H, resulting in residual spikes. The recurrent neural network exhibits rapid convergence with residuals quickly approaching zero. In contrast, the deep neural network experiences vanishing gradients, with significant residuals primarily in the initial (input) and final layers.\n\non this finding, we propose a one-step approximation of the HRM gradient-using the gradient of the last state of each module and treating other states as constant. The gradient path is, therefore,\n\n\\[\\text{Output head}\\rightarrow\\text{final state of the H-module}\\rightarrow\\text{final state of the L-module}\\rightarrow\\text{input embedding}\\]\n\nThe above method needs \\(O(1)\\) memory, does not require unrolling through time, and can be easily implemented with an autograd framework such as PyTorch, as shown in Figure 4. Given that each module only needs to back-propagate errors through its most recent local synaptic activity, this approach aligns well with the perspective that cortical credit assignment relies on short-range, temporally local mechanisms rather than on a global replay of activity patterns.\n\n\n\nThe one-step gradient approximation is theoretically grounded in the mathematics of Deep Equilibrium Models (DEQ) [36] which employs the Implicit Function Theorem (IFT) to bypass BPTT, as detailed next. Consider an idealized HRM behavior where, during high-level cycle \\(k\\), the L-module repeatedly updates until its state \\(z_{L}\\) converges to a local fixed point \\(z_{H}^{\\star}\\). This fixed point, given the current high-level state \\(z_{H}^{k-1}\\), can be expressed as\n\n\\[z_{L}^{\\star}=f_{L}(z_{L}^{\\star},z_{H}^{k-1},\\tilde{x};\\theta_{L})\\ .\\]\n\nThe H-module then performs a single update using this converged L-state:\n\n\\[z_{H}^{k}=f_{H}(z_{H}^{k-1},z_{L}^{\\star};\\theta_{H})\\ .\\]\n\nWith a proper mapping \\(\\mathcal{F}\\), the updates to the high-level state can be written in a more compact form as \\(z_{H}^{k}=\\mathcal{F}(z_{H}^{k-1};\\tilde{x},\\theta)\\), where \\(\\theta=(\\theta_{I},\\theta_{L})\\), and the fixed-point can be written as \\(z_{H}^{\\star}=\\mathcal{F}(z_{H}^{\\star};\\tilde{x},\\theta)\\). Let \\(J_{\\mathcal{F}}=\\frac{\\partial\\mathcal{F}}{\\partial z_{H}}\\) be the Jacobian of \\(\\mathcal{F}\\), and assume that the matrix \\(I-J_{\\mathcal{F}}\\) is invertible at \\(z_{H}^{\\star}\\) and that the mapping \\(\\mathcal{F}\\) is continuously differentiable. The Implicit Function Theorem then allows us to calculate the exact gradient of fixed point \\(z_{H}^{\\star}\\) with respect to the parameters \\(\\theta\\) without explicit back-propagation:\n\n\\[\\frac{\\partial z_{H}^{\\star}}{\\partial\\theta}=\\left(I-J_{\\mathcal{F}}\\big{|}_ {z_{H}^{\\star}}\\right)^{-1}\\frac{\\partial\\mathcal{F}}{\\partial\\theta}\\bigg{|} _{z_{H}^{\\star}}\\ .\\] (1)\n\nCalculating the above gradient requires evaluating and inverting matrix \\((I-J_{\\mathcal{F}})\\) that can be computationally expensive. Given the Neumann series expansion,\n\n\\[(I-J_{\\mathcal{F}})^{-1}=I+J_{\\mathcal{F}}+J_{\\mathcal{F}}^{2}+J_{\\mathcal{F} }^{3}+\\ldots\\ ,\\]\n\nthe so-called _1-step gradient_[37] approximates the series by considering only its first term, i.e. \\((I-J_{\\mathcal{F}})^{-1}\\approx I\\), and leads to the following approximation of Equation (1):\n\n\\[\\frac{\\partial z_{H}^{\\star}}{\\partial\\theta_{H}}\\approx\\frac{ \\partial f_{H}}{\\partial\\theta_{H}},\\quad\\frac{\\partial z_{H}^{\\star}}{ \\partial\\theta_{L}}\\approx\\frac{\\partial f_{H}}{\\partial z_{L}^{\\star}}\\cdot \\frac{\\partial z_{L}^{\\star}}{\\partial\\theta_{L}},\\quad\\frac{\\partial z_{H}^ {\\star}}{\\partial\\theta_{I}}\\approx\\frac{\\partial f_{H}}{\\partial z_{L}^{ \\star}}\\cdot\\frac{\\partial z_{L}^{\\star}}{\\partial\\theta_{I}}\\ .\\] (2)\n\nFigure 4: **Top:** Diagram of HRM with approximate gradient. **Bottom:** Pseudocode of HRM with deep supervision training in PyTorch.\n\nThe gradients of the low-level fixed point, \\(\\frac{\\partial z_{L}^{*}}{\\partial\\theta_{L}}\\) and \\(\\frac{\\partial z_{L}^{*}}{\\partial\\theta_{I}}\\), can also be approximated using another application of the 1-step gradient:\n\n\\[\\frac{\\partial z_{L}^{*}}{\\partial\\theta_{L}}\\approx\\frac{\\partial f_{L}}{ \\partial\\theta_{L}},\\quad\\frac{\\partial z_{L}^{*}}{\\partial\\theta_{I}}\\approx \\frac{\\partial f_{L}}{\\partial\\theta_{I}}\\ .\\] (3)\n\nBy substituting Equation (3) back into Equation (2), we arrive at the final simplified gradients.\n\nBefore defining our loss function, we must first introduce two key elements of our proposed method: _deep supervision_ and _adaptive computational time_.\n\n**Deep supervision** Inspired by the principle that periodic neural oscillations regulate when learning occurs in the brain [38], we incorporate a deep supervision mechanism into HRM, as detailed next.\n\nGiven a data sample \\((x,y)\\), we run multiple forward passes of the HRM model, each of which we refer to as a _segment_. Let \\(M\\) denote the total number of segments executed before termination. For each segment \\(m\\in\\{1,\\dots,M\\}\\), let \\(z^{m}=(z_{H}^{mNT},z_{L}^{mNT})\\) represent the hidden state at the conclusion of segment \\(m\\), encompassing both high-level and low-level state components.\n\nAt each segment \\(m\\), we apply a deep supervision step as follows:\n\n1. Given the state \\(z^{m-1}\\) from the previous segment, compute the next state \\(z^{m}\\) and its associated output \\(\\hat{y}^{m}\\) through a forward pass in the HRM model: \\[(z^{m},\\hat{y}^{m})\\leftarrow\\text{HRM}(z^{m-1},x;\\theta)\\]\n2. Compute the loss for the current segment: \\[L^{m}\\leftarrow\\text{Loss}(\\hat{y}^{m},y)\\]\n3. Update parameters: \\[\\theta\\leftarrow\\text{OptimizerStep}(\\theta,\\nabla_{\\theta}L^{m})\\]\n\nThe crucial aspect of this procedure is that the hidden state \\(z^{m}\\) is \"detached\" from the computation graph before being used as the input state for the next segment. Consequently, gradients from segment \\(m+1\\) do not propagate back through segment \\(m\\), effectively creating a 1-step approximation of the gradient of the recursive deep supervision process [39, 40]. This approach provides more frequent feedback to the H-module and serves as a regularization mechanism, demonstrating superior empirical performance and enhanced stability in deep equilibrium models when compared to more complex, Jacobian-based regularization techniques [39, 41]. Figure 4 shows pseudocode of deep supervision training.\n\n\n\n**Adaptive computational time (ACT)** The brain dynamically alternates between automatic thinking (\"System 1\") and deliberate reasoning (\"System 2\") [42]. Neuroscientific evidence shows that these cognitive modes share overlapping neural circuits, particularly within regions such as the prefrontal cortex and the default mode network [43, 44]. This indicates that the brain dynamically modulates the \"runtime\" of these circuits according to task complexity and potential rewards [45, 46].\n\nInspired by the above mechanism, we incorporate an adaptive halting strategy into HRM that enables \"thinking, fast and slow\". This integration leverages deep supervision and uses the Q-learning\n\nalgorithm [47] to adaptively determine the number of segments. A Q-head uses the final state of the H-module to predict the Q-values \\(\\hat{Q}^{m}=(\\hat{Q}^{m}_{\\text{halt}},\\hat{Q}^{m}_{\\text{continue}})\\) of the \"halt\" and \"continue\" actions:\n\n\\[\\hat{Q}^{m}=\\sigma(\\theta_{Q}^{\\top}z_{H}^{mNT})\\,,\\]\n\nwhere \\(\\sigma\\) denotes the sigmoid function applied element-wise. The halt or continue action is chosen using a randomized strategy as detailed next. Let \\(M_{\\max}\\) denote the maximum number of segments (a fixed hyperparameter) and \\(M_{\\min}\\) denote the minimum number of segments (a random variable). The value of \\(M_{\\min}\\) is determined stochastically: with probability \\(\\varepsilon\\), it is sampled uniformly from the set \\(\\{2,\\cdots,M_{\\max}\\}\\) (to encourage longer thinking), and with probability \\(1-\\varepsilon\\), it is set to 1. The halt action is selected under two conditions: when the segment count surpasses the maximum threshold \\(M_{\\max}\\), or when the estimated halt value \\(\\hat{Q}_{\\text{halt}}\\) exceeds the estimated continue value \\(\\hat{Q}_{\\text{continue}}\\) and the segment count has reached at least the minimum threshold \\(M_{\\min}\\).\n\nThe Q-head is updated through a Q-learning algorithm, which is defined on the following episodic Markov Decision Process (MDP). The state of the MDP at segment \\(m\\) is \\(z^{m}\\), and the action space is \\(\\{\\text{halt},\\text{continue}\\}\\). Choosing the action \"halt\" terminates the episode and returns a binary reward indicating prediction correctness, i.e., \\(\\mathbf{1}\\{\\hat{y}^{m}=y\\}\\). Choosing \"continue\" yields a reward of 0 and the state transitions to \\(z^{m+1}\\). Thus, the Q-learning targets for the two actions \\(\\hat{G}^{m}=(\\hat{G}^{m}_{\\text{halt}},\\hat{G}^{m}_{\\text{continue}})\\) are given by\n\n\\[\\hat{G}^{m}_{\\text{halt}} =\\mathbf{1}\\{\\hat{y}^{m}=y\\}\\,,\\] \\[\\hat{G}^{m}_{\\text{continue}} =\\begin{cases}\\hat{Q}^{m+1}_{\\text{halt}},&\\text{if }m\\geq N_{ \\max}\\,,\\\\ \\max(\\hat{Q}^{m+1}_{\\text{halt}},\\hat{Q}^{m+1}_{\\text{continue}})\\,,&\\text{ otherwise }.\\end{cases}\\]\n\nWe can now define the loss function of our learning procedure. The overall loss for each supervision segment combines both the Q-head loss and the sequence-to-sequence loss:\n\n\\[L^{m}_{\\text{ACT}}=\\textsc{Loss}(\\hat{y}^{m},y)+\\textsc{BinaryCrossEntropy}( \\hat{Q}^{m},\\hat{G}^{m})\\ .\\]\n\nMinimizing the above loss enables both accurate predictions and nearly optimal stopping decisions.\n\nSelecting the \"halt\" action ends the supervision loop. In practice, sequences are processed in batches, which can be easily handled by substituting any halted sample in the batch with a fresh sample from the dataloader.\n\nFigure 5 presents a performance comparison between two HRM variants: one incorporating ACT and another employing a fixed computational step count equivalent to ACT's \\(M_{\\max}\\) parameter. It shows that ACT effectively adapts its computational resources based on task complexity, achieving significant computational savings with minimal impact on performance.\n\n**Inference-time scaling** An effective neural model should exploit additional computational resources during inference to enhance performance. As illustrated in Figure 5-(c), HRM seamlessly achieves inference-time scaling by simply increasing the computational limit parameter, \\(M_{\\max}\\) without requiring further training or architectural modifications.\n\nAdditional compute is especially effective for tasks that demand deeper reasoning. On Sudoku--a problem that often requires long-term planning--HRM exhibits strong inference-time scaling. On the other hand, we find that extra computational resources yield minimal gains in ARC-AGI challenge, as solutions generally require only a few transformations.\n\n#### 3.2.2 Stability of Q-learning in ACT\n\nThe deep Q-learning that underpins our ACT mechanism is known to be prone to instability, often requiring stabilization techniques such as replay buffers and target networks [48], which are absent in our design. Our approach, however, achieves stability through the intrinsic properties of our model and training procedure. Recent theoretical work by Gallici et al. [49] shows that Q-learning can achieve convergence if network parameters are bounded, weight decay is incorporated during training, and post-normalization layers are implemented. Our model satisfies these conditions through its Post-Norm architecture that employs RMSNorm (a layer normalization variant) and the AdamW optimizer. AdamW has been shown to solve an \\(L_{\\infty}\\)-constrained optimization problem, ensuring that model parameters remain bounded by \\(1/\\lambda\\)[50].\n\n#### 3.2.3 Architectural details\n\nWe employ a sequence-to-sequence architecture for HRM. Both input and output are represented as token sequences: \\(x=(x_{1},\\ldots,x_{l})\\) and \\(y=(y_{1},\\ldots,y_{l^{\\prime}})\\) respectively. The model includes an embedding layer \\(f_{I}\\) that converts discrete tokens into vector representations, and an output head \\(f_{O}(z;\\theta_{O})=\\text{softmax}(\\theta_{O}z)\\) that transforms hidden states into token probability distributions \\(\\hat{y}\\). For small-sample experiments, we replace softmax with stablemax [51] to improve generalization performance. The sequence-to-sequence loss is averaged over all tokens, \\(\\textsc{Loss}(\\hat{y},y)=\\frac{1}{l^{\\prime}}\\sum_{i=1}^{l^{\\prime}}\\log p(y_{ i})\\), where \\(p(y_{i})\\) is the probability that distribution \\(\\hat{y}_{i}\\) assigns to token \\(y_{i}\\). The initial hidden states \\(z^{0}\\) are initialized by sampling from a truncated normal distribution with standard deviation of 1, truncation of 2, and kept fixed throughout training.\n\nBoth the low-level and high-level recurrent modules \\(f_{L}\\) and \\(f_{H}\\) are implemented using encoder-only Transformer [52] blocks with identical architectures and dimensions. These modules take multiple inputs, and we use straightforward element-wise addition to combine them, though more sophisticated merging techniques such as gating mechanisms could potentially improve performance and is left for future work. For all Transformer blocks in this work--including those in the baseline models--we incorporate the enhancements found in modern LLMs (based on Llama [53] architectures). These improvements include Rotary Positional Encoding [54], Gated Linear Units [55], RMSNorm [56], and the removal of bias terms from linear layers.\n\nFurthermore, both HRM and recurrent Transformer models implement a Post-Norm architecture\n\nFigure 5: **Effectiveness of Adaptive Computation Time (ACT)** on the _Sudoku-Extreme-Full_. **(a)** Mean compute steps used by models with ACT versus models with a fixed number of compute steps (\\(M\\)). ACT maintains a low and stable number of average compute steps even as the maximum limit (\\(M_{\\max}\\)) increases. **(b)** Accuracy comparison. The ACT model achieves performance comparable to the fixed-compute model while utilizing substantially fewer computational steps on average. **(c)** Inference-time scalability. Models trained with a specific \\(M_{\\max}\\) can generalize to higher computational limits during inference, leading to improved accuracy. For example, a model trained with \\(M_{\\max}=8\\) continues to see accuracy gains when run with \\(M_{\\max}=16\\) during inference.\n\nwith weights initialized via truncated LeCun Normal initialization [57, 58, 59], while the scale and bias parameters are excluded from RMSNorm. All parameters are optimized using the Adam-atan2 optimizer [60], a scale-invariant variant of Adam [61], combined with a constant learning rate that includes linear warm-up.\n\n## 3 Results\n\nThis section begins by describing the ARC-AGI, Sudoku, and Maze benchmarks, followed by an overview of the baseline models and their results. Figure 6-(a,b,c) presents a visual representation of the three benchmark tasks, which are selected to evaluate various reasoning abilities in AI models.\n\n### Benchmarks\n\n#### 3.1.1 ARC-AGI Challenge\n\nThe ARC-AGI benchmark evaluates general fluid intelligence through IQ-test-like puzzles that require inductive reasoning [27]. The initial version, ARC-AGI-1, presents challenges as input-output grid pairs that force AI systems to extract and generalize abstract rules from just a few examples. Each task provides a few input-output example pairs (usually 2-3) and a test input. An AI model has two attempts to produce the correct output grid. Although some believe that mastering ARC-AGI would signal true artificial general intelligence, its primary purpose is to expose the current roadblocks in AGI progress. In fact, both conventional deep learning methods and CoT techniques have faced significant challenges with ARC-AGI-1, primarily because it requires the ability to generalize to entirely new tasks [28].\n\nAddressing the limitations identified in ARC-AGI-1, ARC-AGI-2 significantly expands the benchmark by providing a more comprehensive and carefully refined collection of tasks. These new tasks emphasize deeper compositional reasoning, multi-step logic, contextual rule application, and symbolic abstraction. Human calibration studies show these tasks are challenging but doable for people, while being much harder for current AI systems, offering a clearer measure of general reasoning abilities [29].\n\nFigure 6: **Left: Visualization of benchmark tasks. Right: Difficulty of _Sudoku-Extreme_ examples.**\n\nSudoku-ExtremeSudoku is a 9\\(\\times\\)9 logic puzzle, requiring each row, column, and 3\\(\\times\\)3 block to contain the digits 1-9 exactly once. A prediction is considered correct if it exactly matches the puzzle's unique solution. Sudoku's complex logical structure makes it a popular benchmark for evaluating logical reasoning in machine learning [62, 63, 64].\n\nThe most frequently used Sudoku dataset in research, namely the Kaggle dataset [65], can be fully solved using elementary single-digit techniques [66]. The minimal 17-clue puzzles [62], another widely-used collection, might seem more challenging due to its small number of clues. However, this perception is misleading--since 17 represents the minimum number of clues required to guarantee a unique Sudoku solution, these hints need to be highly orthogonal to each other. This orthogonal arrangement leads to many direct, easily-resolved solution paths [67].\n\nWe introduce _Sudoku-Extreme_, a more challenging dataset that is compiled from the aforementioned easy datasets as well as puzzles recognized by the Sudoku community as exceptionally difficult for human players:\n\n* Easy puzzles compiled from Kaggle, 17-clue, plus unbiased samples from the Sudoku puzzle distribution [67]: totaling \\(1\\,149\\,158\\) puzzles.\n* Challenging puzzles compiled from Magictour 1465, Forum-Hard and Forum-Extreme subsets: totaling \\(3\\,104\\,157\\) puzzles.\n\nThe compiled data then undergo a strict 90/10 train-test split, ensuring that the test set puzzles cannot be derived through equivalent transformations of any training samples. _Sudoku-Extreme_ is a down-sampled subset of this data containing 1000 training examples. We use _Sudoku-Extreme_ in our main experiments (Figure 1), which focuses on small-sample learning scenarios. To guarantee convergence and control overfitting effects in our analysis experiments (Figures 2, 3 and 5), we use the complete training data, _Sudoku-Extreme-Full_, containing \\(3\\,831\\,994\\) examples.\n\nWe measure puzzle difficulty by counting the number of search backtracks (\"guesses\") required by a smart Sudoku solver program _tdoku_, which uses propositional logic to reduce the number of guesses [67]. Our _Sudoku-Extreme_ dataset exhibits a mean difficulty of \\(22\\) backtracks per puzzle, significantly higher than existing datasets, including recent handmade puzzles Sudoku-Bench [68] which average just \\(0.45\\) backtracks per puzzle. These subset complexity levels are shown in Figure 6-(d).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFor ARC-AGI challenge, we start with all input-output example pairs in the training and the evaluation sets. The dataset is augmented by applying translations, rotations, flips, and color permutations to the puzzles. Each task example is prepended with a learnable special token that represents the puzzle it belongs to. At test time, we proceed as follows for each test input in the evaluation set: (1) Generate and solve 1000 augmented variants and, for each, apply the inverse-augmentation transform to obtain a prediction. (2) Choose the two most popular predictions as the final outputs.3 All results are reported on the evaluation set.\n\nFootnote 3: The ARC-AGI allows two attempts for each test input.\n\nWe augment Sudoku puzzles by applying band and digit permutations, while data augmentation is disabled for Maze tasks. Both tasks undergo only a single inference pass.\n\nFor ARC-AGI, the scores of the CoT models are taken from the official leaderboard [29], while for Sudoku and Maze, the scores are obtained by evaluating through the corresponding API.\n\nIn Figure 1, the baselines are grouped based on whether they are pre-trained and use CoT, or neither. The \"Direct pred\" baseline means using \"direct prediction without CoT and pre-training\", which retains the exact training setup of HRM but swaps in a Transformer architecture. Interestingly, on ARC-AGI-1, \"Direct pred\" matches the performance of Liao and Gu [73], who built a carefully designed, domain-specific equivariant network for learning the ARC-AGI task from scratch, without pre-training. By substituting the Transformer architecture with HRM's hierarchical framework and implementing ACT, we achieve more than a twofold performance improvement.\n\nOn the _Sudoku-Extreme_ and _Maze-Hard_ benchmarks, the performance gap between HRM and the baseline methods is significant, as the baselines almost never manage to solve the tasks. These benchmarks that demand lengthy reasoning traces are particularly difficult for CoT-based methods. With only 1000 training examples, the \"Direct pred\" baseline--which employs an 8-layer Transformer identical in size to HRM--fails entirely on these challenging reasoning problems. When trained on the larger _Sudoku-Extreme-Full_ dataset, however, \"Direct pred\" can solve some easy Sudoku puzzles and reaches \\(16.9\\%\\) accuracy (see Figure 2). Lehnert et al. [71] showed that a large vanilla Transformer model with 175M parameters, trained on 1 million examples across multiple trials, achieved only marginal success on 30x30 Maze tasks, with accuracy below \\(20\\%\\) using the \\(pass@64\\) evaluation metric.\n\n### Visualization of intermediate timesteps\n\nAlthough HRM demonstrates strong performance on complex reasoning tasks, it raises an intriguing question: what underlying reasoning algorithms does the HRM neural network actually implement? Addressing this question is important for enhancing model interpretability and developing a deeper understanding of the HRM solution space.\n\nWhile a definitive answer lies beyond our current scope, we begin our investigation by analyzing state trajectories and their corresponding solution evolution. More specifically, at each timestep \\(i\\) and given the low-level and high-level state pair (\\(z^{i}_{L}\\) and \\(z^{i}_{H}\\)) we perform a preliminary forward pass through the H-module to obtain \\(\\bar{z}^{i}=f_{H}(z^{i}_{H},z^{i}_{L};\\theta_{H})\\) and its corresponding decoded prediction \\(\\bar{y}^{i}=f_{O}(\\bar{z}^{i};\\theta_{O})\\). The prediction \\(\\bar{y}^{i}\\) is then visualized in Figure 7.\n\nIn the Maze task, HRM appears to initially explore several potential paths simultaneously, subsequently eliminating blocked or inefficient routes, then constructing a preliminary solution outline followed by multiple refinement iterations. In Sudoku, the strategy resembles a depth-first search\n\napproach, where the model appears to explore potential solutions and backtracks when it hits dead ends. HRM uses a different approach for ARC tasks, making incremental adjustments to the board and iteratively improving it until reaching a solution. Unlike Sudoku, which involves frequent backtracking, the ARC solution path follows a more consistent progression similar to hill-climbing optimization. Importantly, the model shows that it can adapt to different reasoning approaches, likely choosing an effective strategy for each particular task. Further research is needed to gain more comprehensive insights into these solution strategies.\n\n## 4 Brain Correspondence\n\nA key principle from systems neuroscience is that a brain region's functional repertoire--its ability to handle diverse and complex tasks--is closely linked to the dimensionality of its neural representations [75, 76]. Higher-order cortical areas, responsible for complex reasoning and decision-making, must handle a wide variety of tasks, demanding more flexible and context-dependent processing [77]. In dynamical systems, this flexibility is often realized through higher-dimensional state-space trajectories, which allow for a richer repertoire of potential computations [78]. This principle gives rise to an observable _dimensionality hierarchy_, where a region's position in the processing hierarchy correlates with its _effective dimensionality_. To quantify this phenomenon, we can examine the\n\nFigure 7: **Visualization of intermediate predictions by HRM on benchmark tasks.****Top:**_MazeHard_—blue cells indicate the predicted path. **Middle:**_Sudoku-Extreme_—bold cells represent initial givens; red highlights cells violating Sudoku constraints; grey shading indicates changes from the previous timestep. **Bottom:** ARC-AGI-2 Task—left: provided example input-output pair; right: intermediate steps solving the test input.\n\nFigure 8: **Hierarchical Dimensionality Organization in the HRM and Mouse Cortex.****(a,b)** are adapted from Posani et al. 74. (a) Anatomical illustration of mouse cortical areas, color-coded by functional modules. (b) Correlation between Participation Ratio (PR), a measure of effective neural dimensionality, and hierarchical position across different mouse cortical areas. Higher positions in the hierarchy (e.g., MOs, ACAd) exhibit significantly higher PR values compared to lower sensory areas (e.g., SSp-n), with a Spearman correlation coefficient of \\(\\rho\\) = 0.79 (P = 0.0003). (c,d) **Trained HRM.** (c) PR scaling of the trained HRM with task diversity. The dimensionality of the high-level module (\\(z_{H}\\)) scales with the number of unique tasks (trajectories) included in the analysis, indicating an adaptive expansion of its representational capacity. In contrast, the low-level module’s (\\(z_{L}\\)) dimensionality remains stable. (d) PR values for the low-level (\\(z_{L}\\), PR = 30.22) and high-level (\\(z_{H}\\), PR = 89.95) modules of the _trained_ HRM, computed from neural activity during 100 unique Sudoku-solving trajectories. A clear dimensionality hierarchy is observed, with the high-level module operating in a substantially higher-dimensional space. (e,f) **Analysis of Untrained Network.** To verify that the dimensionality hierarchy is an emergent property of training, the same analyses were performed on an _untrained_ HRM with random weights. (e) In contrast to the trained model’s scaling in (c), the dimensionality of both modules in the untrained model remains low and stable, failing to scale with the number of tasks. (f) Similarly, contrasting with the clear separation in (d), the PR values for the untrained model’s modules (\\(z_{L}\\), PR = 42.09; \\(z_{H}\\), PR = 40.75) are low and nearly identical, showing no evidence of hierarchical separation. This confirms that the observed hierarchical organization of dimensionality is a learned property that emerges through training, not an artifact of the model’s architecture.\n\nParticipation Ratio (PR), which serves as a standard measure of the effective dimensionality of a high-dimensional representation [79]. The PR is calculated using the formula\n\n\\[\\text{PR}=\\frac{(\\sum_{i}\\lambda_{i})^{2}}{\\sum_{i}\\lambda_{i}^{2}}\\,,\\]\n\nwhere \\(\\{\\lambda_{i}\\}\\) are the eigenvalues of the covariance matrix of neural trajectories. Intuitively, a higher PR value signifies that variance is distributed more evenly across many dimensions, corresponding to a higher-dimensional representation. Conversely, a lower PR value indicates that variance is concentrated in only a few principal components, reflecting a more compact, lower-dimensional structure.\n\nThe dimensionality hierarchy can be observed, for example, in the mouse cortex, where the PR of population activity increases monotonically from low-level sensory areas to high-level associative areas, supporting this link between dimensionality and functional complexity [74] (Figure 8 (a,b)).\n\nWe evaluated whether HRM reproduces this neuroscientific principle by calculating the PR for both recurrent modules after training on the _Sudoku-Extreme Full_ dataset. The PR computation used the covariance matrix derived from neural states gathered across multiple Sudoku-solving trajectories. The results show a striking parallel to the biological findings. The low-level module's state (\\(z_{L}\\)) occupies a relatively small subspace with a participation ratio of 30.22, whereas the high-level module's state (\\(z_{H}\\)) operates in a substantially larger subspace with a participation ratio of 89.95, as shown in Figure 8(c). Furthermore, Figure 8(d) shows that increasing the number of unique tasks (trajectories) from 10 to 100 causes \\(z_{H}\\) dimensionality to scale up accordingly, while \\(z_{L}\\) dimensionality remains stable. These results suggest an _emergent_ separation of representational capacity between the modules that parallels their functional roles.\n\nTo confirm that this hierarchical organization is an emergent property of training, and not an artifact of the network's architecture, we performed a control analysis using an identical but untrained network with random weights.\n\nWe initialized an identical HRM architecture with random weights and, without any training, measured the PR of its modules as the network processed the same task-specific inputs given to the trained model.\n\nThe results, shown in Figure 8(e,f), reveal a stark contrast: the high-level and low-level modules of the untrained network exhibit no hierarchical separation, with their PR values remaining low and nearly indistinguishable from each other. This control analysis validates that the dimensionality hierarchy is an _emergent property_ that arises as the model learns to perform complex reasoning.\n\nThe high-to-low PR ratio in HRM (\\(z_{H}/z_{L}\\approx 2.98\\)) closely matches that measured in the mouse cortex (\\(\\approx 2.25\\)). In contrast, conventional deep networks often exhibit _neural collapse_, where last-layer features converge to a low-dimensional subspace [80, 81, 82]. HRM therefore departs from the collapse pattern and instead fosters a high-dimensional representation in its higher module. This is significant because such representations are considered crucial for cognitive flexibility and are a hallmark of higher-order brain regions like the prefrontal cortex (PFC), which is central to complex reasoning.\n\nThis structural parallel suggests the model has discovered a fundamental organizational principle. By learning to partition its representations into a high-capacity, high-dimensional subspace (\\(z_{H}\\)) and a more specialized, low-dimensional one (\\(z_{L}\\)), HRM autonomously discovers an organizational\n\nprinciple that is thought to be fundamental for achieving robust and flexible reasoning in biological systems. This provides a potential mechanistic explanation for the model's success on complex, long-horizon tasks that are intractable for models lacking such a differentiated internal structure. We emphasize, however, that this evidence is correlational. While a causal link could be tested via intervention (e.g., by constraining the H-module's dimensionality), such methods are difficult to interpret in deep learning due to potential confounding effects on the training process itself. Thus, the causal necessity of this emergent hierarchy remains an important question for future investigation.\n\n## 5 Related Work\n\n#### 5.0.1 Reasoning and algorithm learning\n\nGiven the central role of reasoning problems and their close relation to algorithms, researchers have long explored neural architectures that enable algorithm learning from training instances. This line of work includes Neural Turing Machines (NTM)[83], the Differentiable Neural Computer (DNC)[84], and Neural GPUs[85]-all of which construct iterative neural architectures that mimic computational hardware for algorithm execution, and are trained to learn algorithms from data. Another notable work in this area is Recurrent Relational Networks (RRN)[62], which executes algorithms on graph representations through graph neural networks.\n\nRecent studies have integrated algorithm learning approaches with Transformer-based architectures. Universal Transformers extend the standard Transformer model by introducing a recurrent loop over the layers and implementing an adaptive halting mechanism. Geiping et al.[86] demonstrate that looped Transformers can generalize to a larger number of recurrent steps during inference than what they were trained on. Shen et al.[16] propose adding continuous recurrent reasoning tokens to the Transformer. Finally, TransNAR[8] combine recurrent graph neural networks with language models.\n\nBuilding on the success of CoT-based reasoning, a line of work have introduced fine-tuning methods that use reasoning paths from search algorithms (like A*) as SFT targets[87, 71, 70].\n\nWe also mention adaptive halting mechanisms designed to allocate additional computational resources to more challenging problems. This includes the Adaptive Computation Time (ACT) for RNNs[88] and follow-up research like PonderNet[89], which aims to improve the stability of this allocation process.\n\nHRM further pushes the boundary of algorithm learning through a brain-inspired computational architecture that achieves exceptional data efficiency and model expressiveness, successfully discovering complex and diverse algorithms from just 1000 training examples.\n\n#### 5.0.2 Brain-inspired reasoning architectures\n\nDeveloping a model with the reasoning power of the brain has long been a goal in brain-inspired computing. Spaun[90] is one notable example, which uses spiking neural networks to create distinct modules corresponding to brain regions like the visual cortex and prefrontal cortex. This design enables an architecture to perform a range of cognitive tasks, from memory recall to simple reasoning puzzles. However, its reasoning relies on hand-designed algorithms, which may limit its ability to learn new tasks. Another significant model is the Tolman-Eichenbaum Machine (TEM)[91], which is inspired by the hippocampal-entorhinal system's role in spatial and relational memory tasks. TEM proposes that medial entorhinal cells create a basis for structural knowledge, while hippocampal cells link this basis to sensory information. This allows TEM to generalize and explains the emergence of various cell types like grid, border, and\n\nplace cells. Another approach involves neural sampling models [92], which view the neural signaling process as inference over a distribution, functioning similarly to a Boltzmann machine. These models often require hand-made rules to be set up for solving a specific reasoning task. In essence, while prior models are restricted to simple reasoning problems, HRM is designed to solve complex tasks that are hard for even advanced LLMs, without pre-training or task-specific manual design.\n\n**Hierarchical memory**: The hierarchical multi-timescale structure also plays an important role in how the brain processes memory. Models such as Hierarchical Sequential Models [93] and Clockwork RNN [94] use multiple recurrent modules that operate at varying time scales to more effectively capture long-range dependencies within sequences, thereby mitigating the forgetting issue in RNNs.\n\nSimilar mechanisms have also been adopted in linear attention methods for memorizing long contexts (see the Discussions section). Since HRM focuses on reasoning, full attention is applied for simplicity. Incorporating hierarchical memory into HRM could be a promising future direction.\n\n## 6 Discussions\n\n**Turing-completeness of HRM**: Like earlier neural reasoning algorithms including the Universal Transformer [95], HRM is computationally universal when given sufficient memory and time constraints. In other words, it falls into the category of models that can simulate any Turing machine, overcoming the computational limitations of standard Transformers discussed previously in the introduction. Given that earlier neural algorithm reasoners were trained as recurrent neural networks, they suffer from premature convergence and memory intensive BPTT. Therefore, in practice, their effective computational depth remains limited, though still deeper than that of a standard Transformer. By resolving these two challenges and being equipped with adaptive computation, HRM could be trained on long reasoning processes, solve complex puzzles requiring intensive depth-first search and backtracking, and move closer to practical Turing-completeness.\n\n**Reinforcement learning with chain-of-thought**: Beyond fine-tuning using human-annotated CoT, reinforcement learning (RL) represents another widely adopted training methodology. However, recent evidence suggests that RL primarily unlocks existing CoT-like capabilities rather than discovering fundamentally new reasoning mechanisms [96, 97, 98, 99]. Additionally, CoT-training with RL is known for its instability and data inefficiency, often requiring extensive exploration and careful reward design. In contrast, HRM takes feedback from dense gradient-based supervision rather than relying on a sparse reward signal. Moreover, HRM operates naturally in a continuous space, which is biologically plausible and avoids allocating same computational resources to each token, even though tokens vary in their reasoning and planning complexity [16].\n**Linear attention**: Recurrence has been explored not only for its capability in universal computation, but also as a means to replace the attention mechanism in Transformers, which suffers from quadratic time and memory complexity [100]. Recurrent alternatives offer a more efficient design by processing input tokens sequentially and predicting the next token at each time step, similar to early RNN-based language models.\n\nSome linear-attention variants, such as Log-linear Attention [101], share an RNN-like state-update that can be interpreted as propagating multi-timescale summary statistics, thereby retaining long-range context without the quadratic memory growth of standard self-attention. However, substituting the attention mechanism alone does not change the fact that Transformers are still fixed-depth, and require CoT as a compensatory mechanism. Notably, linear attention can operate with a reduced\n\nkey-value cache over extended contexts, making them more suitable for deployment on resource-constrained edge devices.\n\n## 7 Conclusion\n\nThis work introduces the Hierarchical Reasoning Model, a brain-inspired architecture that leverages hierarchical structure and multi-timescale processing to achieve substantial computational depth without sacrificing training stability or efficiency. With only 27M parameters and training on just 1000 examples, HRM effectively solves challenging reasoning problems such as ARC, Sudoku, and complex maze navigation-tasks that typically pose significant difficulties for contemporary LLM and chain-of-thought models.\n\nAlthough the brain relies heavily on hierarchical structures to enable most cognitive processes, these concepts have largely remained confined to academic literature rather than being translated into practical applications. The prevailing AI approach continues to favor non-hierarchical models. Our results challenge this established paradigm and suggest that the Hierarchical Reasoning Model represents a viable alternative to the currently dominant chain-of-thought reasoning methods, advancing toward a foundational framework capable of Turing-complete universal computation.\n\n**Acknowledgements** We thank Mingli Yuan, Ahmed Murtadha Hasan Mahyoub and Hengshuai Yao for their insightful discussions and valuable feedback throughout the course of this work.\n\n## References\n\n* [1] Ian Goodfellow, Yoshua Bengio, and Aaron Courville. _Deep Learning_. MIT Press, 2016. http://www.deeplearningbook.org.\n* [2] Kaiming He, X. Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. _2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 770-778, 2015.\n* [3] Lena Strobl. Average-hard attention transformers are constant-depth uniform threshold circuits, 2023.\n* [4] Tom Bylander. Complexity results for planning. In _Proceedings of the 12th International Joint Conference on Artificial Intelligence - Volume 1_, IJCAI'91, page 274-279, San Francisco, CA, USA, 1991. Morgan Kaufmann Publishers Inc. ISBN 1558601600.\n* [5] William Merrill and Ashish Sabharwal. A logic for expressing log-precision transformers. In _Neural Information Processing Systems_, 2023.\n* [6] David Chiang. Transformers in DLOGTIME-uniform TC\\({}^{0}\\). _Transactions on Machine Learning Research_, 2025.\n* [7] Lucas Lehnert, Sainbayar Sukhbaatar, DiJia Su, Qinqing Zheng, Paul McVay, Michael Rabbat, and Yuandong Tian. Beyond a*: Better planning with transformers via search dynamics bootstrapping. In _First Conference on Language Modeling_, 2024.\n* [8] Wilfried Bounsi, Borja Ibarz, Andrew Dudzik, Jessica B. Hamrick, Larisa Markeeva, Alex Vitvitskyi, Razvan Pascanu, and Petar Velivckovic'c. Transformers meet neural algorithmic reasoners. _ArXiv_, abs/2406.09308, 2024.\n* [9] William Merrill and Ashish Sabharwal. The parallelism tradeoff: Limitations of log-precision transformers. _Transactions of the Association for Computational Linguistics_, 11:531-545, 2023. doi: 10.1162/tacl_a_00562.\n* [10] Jason Wei, Yi Tay, et al. Chain-of-thought prompting elicits reasoning in large language models, 2022. arXiv preprint arXiv:2201.11903.\n* [11] William Merrill and Ashish Sabharwal. The expressive power of transformers with chain of thought. In _ICLR_, 2024.\n* [12] Xinyun Chen, Ryan A. Chi, Xuezhi Wang, and Denny Zhou. Premise order matters in reasoning with large language models. _ArXiv_, abs/2402.08939, 2024.\n* [13] Rongwu Xu, Zehan Qi, and Wei Xu. Preemptive answer \"attacks\" on chain-of-thought reasoning. In _Annual Meeting of the Association for Computational Linguistics_, 2024.\n* [14] Pablo Villalobos, Anson Ho, Jaime Sevilla, Tamay Besiroglu, Lennart Heim, and Marius Hobbhahn. Will we run out of data? limits of llm scaling based on human-generated data. _arXiv preprint arXiv:2211.04325_, 2022.\n* [15] Xinghao Chen, Anhao Zhao, Heming Xia, Xuan Lu, Hanlin Wang, Yanjun Chen, Wei Zhang, Jian Wang, Wenjie Li, and Xiaoyu Shen. Reasoning beyond language: A comprehensive survey on latent chain-of-thought reasoning, 2025.\n* [16] Xuan Shen, Yizhou Wang, Xiangxi Shi, Yanzhi Wang, Pu Zhao, and Jiuxiang Gu. Training large language models to reason in a continuous latent space. _arXiv preprint arXiv:2412.07423_, 2024.\n\n* Fedorenko et al. [2024] Evelina Fedorenko, Steven T Piantadosi, and Edward AF Gibson. Language is primarily a tool for communication rather than thought. _Nature_, 630(8017):575-586, 2024.\n* Wang et al. [2024] Hongyu Wang, Shuming Ma, Li Dong, Shaohan Huang, Dongdong Zhang, and Furu Wei. Deepnet: Scaling transformers to 1,000 layers. _IEEE Transactions on Pattern Analysis and Machine Intelligence_, 2024.\n* Lillicrap and Santoro [2019] Timothy P Lillicrap and Adam Santoro. Backpropagation through time and the brain. _Current Opinion in Neurobiology_, 55:82-89, 2019. ISSN 0959-4388. doi: https://doi.org/10.1016/j.conb.2019.01.011.\n* Murray et al. [2014] John D Murray, Alberto Bernacchia, David J Freedman, Ranulfo Romo, Jonathan D Wallis, Xinying Cai, Camillo Padoa-Schioppa, Tatiana Pasternak, Hyojung Seo, Daeyeol Lee, et al. A hierarchy of intrinsic timescales across primate cortex. _Nature neuroscience_, 17(12):1661-1663, 2014.\n* Zeraati et al. [2023] Roxana Zeraati, Yan-Liang Shi, Nicholas A Steinmetz, Marc A Gieselmann, Alexander Thiele, Tirin Moore, Anna Levina, and Tatiana A Engel. Intrinsic timescales in the visual cortex change with selective attention and reflect spatial connectivity. _Nature communications_, 14(1):1858, 2023.\n* Huntenburg et al. [2018] Julia M Huntenburg, Pierre-Louis Bazin, and Daniel S Margulies. Large-scale gradients in human cortical organization. _Trends in cognitive sciences_, 22(1):21-31, 2018.\n* Lamme and Roelfsema [2000] Victor AF Lamme and Pieter R Roelfsema. The distinct modes of vision offered by feedforward and recurrent processing. _Trends in neurosciences_, 23(11):571-579, 2000.\n* Bastos et al. [2012] Andre M Bastos, W Martin Usrey, Rick A Adams, George R Mangun, Pascal Fries, and Karl J Friston. Canonical microcircuits for predictive coding. _Neuron_, 76(4):695-711, 2012.\n* Kaleb et al. [2024] Klara Kaleb, Barbara Feulner, Juan Gallego, and Claudia Clopath. Feedback control guides credit assignment in recurrent neural networks. _Advances in Neural Information Processing Systems_, 37:5122-5144, 2024.\n* Lillicrap et al. [2020] Timothy P Lillicrap, Adam Santoro, Luke Marris, Colin J Akerman, and Geoffrey Hinton. Backpropagation and the brain. _Nature Reviews Neuroscience_, 21(6):335-346, 2020.\n* Chollet [2019] Francois Chollet. On the measure of intelligence (abstraction and reasoning corpus), 2019. arXiv preprint arXiv:1911.01547.\n* Chollet et al. [2024] Francois Chollet, Mike Knoop, Gregory Kamradt, and Bryan Landers. Arc prize 2024: Technical report. _ArXiv_, abs/2412.04604, 2024.\n* Chollet et al. [2025] Francois Chollet, Mike Knoop, Gregory Kamradt, Bryan Landers, and Henry Pinkard. Arcagi-2: A new challenge for frontier ai reasoning systems. _arXiv preprint arXiv:2505.11831_, 2025.\n* Buzsaki [2000] Gyorgy Buzsaki. Gamma, alpha, delta, and theta oscillations govern cognitive processes. _International Journal of Psychophysiology_, 39:241-248, 2000.\n* Buzsaki [2006] Gyorgy Buzsaki. _Rhythms of the Brain_. Oxford university press, 2006.\n* Pahor and Jausovec [2014] Anja Pahor and Norbert Jausovec. Theta-gamma cross-frequency coupling relates to the level of human intelligence. _Intelligence_, 46:283-290, 2014.\n* Tort et al. [2009] Adriano BL Tort, Robert W Komorowski, Joseph R Manns, Nancy J Kopell, and Howard Eichenbaum. Theta-gamma coupling increases during the learning of item-context associations. _Proceedings of the National Academy of Sciences_, 106(49):20942-20947, 2009.\n\n* [34] Benjamin Scellier and Yoshua Bengio. Equilibrium propagation: Bridging the gap between energy-based models and backpropagation. _Frontiers in Computational Neuroscience_, 11, 2016.\n* [35] Guillaume Bellec, Franz Scherr, Anand Subramoney, Elias Hajek, Darjan Salaj, Robert Legenstein, and Wolfgang Maass. A solution to the learning dilemma for recurrent networks of spiking neurons. _Nature Communications_, 11, 07 2020. doi: 10.1038/s41467-020-17236-y.\n* [36] Shaojie Bai, J Zico Kolter, and Vladlen Koltun. Deep equilibrium models. In _Advances in Neural Information Processing Systems_, pages 690-701, 2019.\n* [37] Zhengyang Geng, Xinyu Zhang, Shaojie Bai, Yisen Wang, and Zhouchen Lin. On training implicit models. _ArXiv_, abs/2111.05177, 2021.\n* [38] Katarina Begus and Elizabeth Bonawitz. The rhythm of learning: Theta oscillations as an index of active learning in infancy. _Developmental Cognitive Neuroscience_, 45:100810, 2020. ISSN 1878-9293. doi: https://doi.org/10.1016/j.dcn.2020.100810.\n* [39] Shaojie Bai, Zhengyang Geng, Yash Savani, and J. Zico Kolter. Deep Equilibrium Optical Flow Estimation . In _2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 610-620, 2022.\n* [40] Zaccharie Ramzi, Florian Mannel, Shaojie Bai, Jean-Luc Starck, Philippe Ciuciu, and Thomas Moreau. Shine: Sharing the inverse estimate from the forward pass for bi-level optimization and implicit models. _ArXiv_, abs/2106.00553, 2021.\n* [41] Shaojie Bai, Vladlen Koltun, and J. Zico Kolter. Stabilizing equilibrium models by jacobian regularization. In _International Conference on Machine Learning_, 2021.\n* [42] Daniel Kahneman and P Egan. Thinking, fast and slow (farrar, straus and giroux, new york), 2011.\n* [43] Matthew D Lieberman. Social cognitive neuroscience: a review of core processes. _Annu. Rev. Psychol._, 58(1):259-289, 2007.\n* [44] Randy L Buckner, Jessica R Andrews-Hanna, and Daniel L Schacter. The brain's default network: anatomy, function, and relevance to disease. _Annals of the new York Academy of Sciences_, 1124(1):1-38, 2008.\n* [45] Marcus E Raichle. The brain's default mode network. _Annual review of neuroscience_, 38(1):433-447, 2015.\n* [46] Andrew Westbrook and Todd S Braver. Cognitive effort: A neuroeconomic approach. _Cognitive, Affective, & Behavioral Neuroscience_, 15:395-415, 2015.\n* [47] Richard S. Sutton and Andrew G. Barto. _Reinforcement Learning: An Introduction_. MIT Press, Cambridge, MA, 2018.\n* [48] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and Martin A. Riedmiller. Playing atari with deep reinforcement learning. _ArXiv_, abs/1312.5602, 2013.\n* [49] Matteo Gallici, Mattie Fellows, Benjamin Ellis, Bartomeu Pou, Ivan Masmitja, Jakob Nicolaus Foerster, and Mario Martin. Simplifying deep temporal difference learning, 2025.\n\n* [50] Shuo Xie and Zhiyuan Li. Implicit bias of adamw: L inf norm constrained optimization. _ArXiv_, abs/2404.04454, 2024.\n* [51] Lucas Prieto, Melih Barsbey, Pedro A. M. Mediano, and Tolga Birdal. Grokking at the edge of numerical stability. In _The Thirteenth International Conference on Learning Representations_, 2025.\n* [52] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In _Advances in neural information processing systems_, pages 5998-6008, 2017.\n* [53] Meta AI. Llama 3: State-of-the-art open weight language models. Technical report, Meta, 2024. URL https://ai.meta.com/llama/.\n* [54] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. _Neurocomputing_, 568:127063, 2024.\n* [55] Noam M. Shazeer. Glu variants improve transformer. _ArXiv_, abs/2002.05202, 2020.\n* [56] Biao Zhang and Rico Sennrich. Root mean square layer normalization. _ArXiv_, abs/1910.07467, 2019.\n* [57] Gunter Klambauer, Thomas Unterthiner, Andreas Mayr, and Sepp Hochreiter. Self-normalizing neural networks. In _Neural Information Processing Systems_, 2017.\n* [58] JAX Developers. _jax.nn.initializers.lecun_normal_. Google Research, 2025. URL https://docs.jax.dev/en/latest/_autosummary/jax.nn.initializers.lecun_normal.html. Accessed June 22, 2025.\n* [59] Yann LeCun, Leon Bottou, Genevieve B Orr, and Klaus-Robert Muller. Efficient backprop. In _Neural networks: Tricks of the trade_, pages 9-50. Springer, 2002.\n* [60] Katie E Everett, Lechao Xiao, Mitchell Wortsman, Alexander A Alemi, Roman Novak, Peter J Liu, Izzeddin Gur, Jascha Sohl-Dickstein, Leslie Pack Kaelbling, Jaehoon Lee, and Jeffrey Pennington. Scaling exponents across parameterizations and optimizers. In _Forty-first International Conference on Machine Learning_, 2024.\n* [61] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization, 2017.\n* [62] Rasmus Berg Palm, Ulrich Paquet, and Ole Winther. Recurrent relational networks. In _Neural Information Processing Systems_, 2017.\n* [63] Jieyi Long. Large language model guided tree-of-thought. _ArXiv_, abs/2305.08291, 2023.\n* [64] Yilun Du, Jiayuan Mao, and Josh Tenenbaum. Learning iterative reasoning through energy diffusion. _ArXiv_, abs/2406.11179, 2024.\n* [65] Kyubyong Park. Can convolutional neural networks crack sudoku puzzles? https://github.com/Kyubyong/sudoku, 2018.\n* [66] Single-digit techniques. https://hodoku.sourceforge.net/en/tech_singles.php. Accessed: 2025-06-16.\n* [67] Tom Dillion. Tdoku: A fast sudoku solver and generator. https://t-dillon.github.io/tdoku/, 2025.\n* [68] Jeffrey Seely, Yuki Imajuku, Tianyu Zhao, Edoardo Cetin, and Llion Jones. Sudoku-bench: Evaluating creative reasoning with sudoku variants. _arXiv preprint arXiv:2505.16135_, 2025.\n* [69] Luke Darlow, Ciaran Regan, Sebastian Risi, Jeffrey Seely, and Llion Jones. Continuous thought machines. _arXiv preprint arXiv:2505.05522_, 2025.\n\n* Su et al. [2025] DiJia Su, Sainbayar Sukhbaatar, Michael Rabbat, Yuandong Tian, and Qinqing Zheng. Dualformer: Controllable fast and slow thinking by learning with randomized reasoning traces, 2025.\n* Lehnert et al. [2024] Lucas Lehnert, Sainbayar Sukhbaatar, DiJia Su, Qinqing Zheng, Paul McVay, Michael Rabbat, and Yuandong Tian. Beyond a*: Better planning with transformers via search dynamics bootstrapping. In _First Conference on Language Modeling_, 2024.\n* Kapadia et al. [2013] Mubbasir Kapadia, Francisco Garcia, Cory D. Boatright, and Norman I. Badler. Dynamic search on the gpu. In _2013 IEEE/RSJ International Conference on Intelligent Robots and Systems_, pages 3332-3337, 2013. doi: 10.1109/IROS.2013.6696830.\n* Liao and Gu [2025] Isaac Liao and Albert Gu. Arc-agi without pretraining, 2025. URL https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/arc_agi_without_pretraining.html.\n* Posani et al. [2025] Lorenzo Posani, Shuqi Wang, Samuel P Muscinelli, Liam Paninski, and Stefano Fusi. Rarely categorical, always high-dimensional: how the neural code changes along the cortical hierarchy. _bioRxiv_, pages 2024-11, 2025.\n* Rigotti et al. [2013] Mattia Rigotti, Omri Barak, Melissa R. Warden, Xiao-Jing Wang, Nathaniel D. Daw, Earl K. Miller, and Stefano Fusi. The importance of mixed selectivity in complex cognitive tasks. _Nature_, 497:585-590, 2013. doi: 10.1038/nature12160.\n* Mante et al. [2013] Valerio Mante, David Sussillo, Krishna V. Shenoy, and William T. Newsome. Context-dependent computation by recurrent dynamics in prefrontal cortex. _Nature_, 503(7474):78-84, 2013. doi: 10.1038/nature12742.\n* Miller and Cohen [2001] Earl K. Miller and Jonathan D. Cohen. An integrative theory of prefrontal cortex function. _Annual Review of Neuroscience_, 24(1):167-202, 2001. doi: 10.1146/annurev.neuro.24.1.167.\n* Maass [2002] Wolfgang Maass. Real-time computing without stable states: a new framework for neural computation based on perturbations. _Neural Computation_, 14(11):2531-2560, 2002. doi: 10.1162/089976602760407955.\n* Altan et al. [2021] Ege Altan, Sara A. Solla, Lee E. Miller, and Eric J. Perreault. Estimating the dimensionality of the manifold underlying multi-electrode neural recordings. _PLoS Computational Biology_, 17(11):e1008591, 2021. doi: 10.1371/journal.pcbi.1008591.\n* Papyan et al. [2020] Vardan Papyan, X. Y. Han, and David L. Donoho. Prevalence of neural collapse during the terminal phase of deep learning training. _Proceedings of the National Academy of Sciences_, 117(40):24652-24663, 2020. doi: 10.1073/pnas.2015509117.\n* Fang et al. [2021] Cong Fang, Hangfeng He, Qi Long, and Weijie J. Su. Exploring deep neural networks via layer-peeled model: Minority collapse in imbalanced training. _Proceedings of the National Academy of Sciences_, 118(43):e2103091118, 2021. doi: 10.1073/pnas.2103091118.\n* Zhu et al. [2021] Zhihui Zhu, Tianyu Ding, Jinxin Zhou, Xiao Li, Chong You, Jeremias Sulam, and Qing Qu. A geometric analysis of neural collapse with unconstrained features. In _Advances in Neural Information Processing Systems_, volume 34 of _NeurIPS_, pages 29820-29834, 2021.\n* Graves et al. [2014] Alex Graves, Greg Wayne, and Ivo Danihelka. Neural turing machines, 2014.\n* Graves et al. [2016] Alex Graves, Greg Wayne, Malcolm Reynolds, Tim Harley, Ivo Danihelka, Agnieszka Grabska-Barwinska, Sergio Gomez Colmenarejo, Edward Grefenstette, Tiago Ramalho, John Agapiou, et al. Hybrid computing using a neural network with dynamic external memory. _Nature_, 538(7626):471-476, 2016.\n\n* [85] Lukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In _ICLR_, 2016.\n* [86] Jonas Geiping, Sean McLeish, Neel Jain, John Kirchenbauer, Siddharth Singh, Brian R. Bartoldson, Bhavya Kailkhura, Abhinav Bhatele, and Tom Goldstein. Scaling up test-time compute with latent reasoning: A recurrent depth approach, 2025.\n* [87] Tiedong Liu and Kian Hsiang Low. Goat: Fine-tuned llama outperforms gpt-4 on arithmetic tasks. _ArXiv_, abs/2305.14201, 2023.\n* [88] Alex Graves. Adaptive computation time for recurrent neural networks. _ArXiv_, abs/1603.08983, 2016.\n* [89] Andrea Banino, Jan Balaguer, and Charles Blundell. Pondernet: Learning to ponder. _ArXiv_, abs/2107.05407, 2021.\n* [90] Chris Eliasmith, Terrence C Stewart, Xuan Choo, Trevor Bekolay, Travis DeWolf, Yichuan Tang, and Daniel Rasmussen. A large-scale model of the functioning brain. _science_, 338 (6111):1202-1205, 2012.\n* [91] James CR Whittington, Timothy H Muller, Shirley Mark, Guifen Chen, Caswell Barry, Neil Burgess, and Timothy EJ Behrens. The tolman-eichenbaum machine: unifying space and relational memory through generalization in the hippocampal formation. _Cell_, 183(5):1249-1263, 2020.\n* [92] Lars Buesing, Johannes Bill, Bernhard Nessler, and Wolfgang Maass. Neural dynamics as sampling: a model for stochastic computation in recurrent networks of spiking neurons. _PLoS computational biology_, 7(11):e1002211, 2011.\n* [93] Salah Hihi and Yoshua Bengio. Hierarchical recurrent neural networks for long-term dependencies. In D. Touretzky, M.C. Mozer, and M. Hasselmo, editors, _Advances in Neural Information Processing Systems_, volume 8. MIT Press, 1995.\n* [94] Jan Koutnik, Klaus Greff, Faustino J. Gomez, and Jurgen Schmidhuber. A clockwork rnn. In _International Conference on Machine Learning_, 2014.\n* [95] Mostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob Uszkoreit, and Lukasz Kaiser. Universal transformers, 2018. arXiv preprint arXiv:1807.03819.\n* [96] Yiping Wang, Qing Yang, Zhiyuan Zeng, Liliang Ren, Lucas Liu, Baolin Peng, Hao Cheng, Xuehai He, Kuan Wang, Jianfeng Gao, Weizhu Chen, Shuohang Wang, Simon Shaolei Du, and Yelong Shen. Reinforcement learning for reasoning in large language models with one training example, 2025. URL https://arxiv.org/abs/2504.20571.\n* [97] Niklas Muennighoff. s1: Simple test-time scaling. _arXiv preprint arXiv:2502.23456_, 2025.\n* [98] Liang Wen, Yunke Cai, Fenrui Xiao, Xin He, Qi An, Zhenyu Duan, Yimin Du, Junchen Liu, Lifu Tang, Xiaowei Lv, Haosheng Zou, Yongchao Deng, Shousheng Jia, and Xiangzheng Zhang. Light-r1: Curriculum sft, dpo and rl for long cot from scratch and beyond, 2025.\n* [99] Xuefeng Li, Haoyang Zou, and Pengfei Liu. Limr: Less is more for rl scaling, 2025.\n* [100] Tri Dao and Albert Gu. Transformers are ssms: Generalized models and efficient algorithms through structured state space duality. _ArXiv_, abs/2405.21060, 2024.\n* [101] Han Guo, Songlin Yang, Tarushii Goel, Eric P Xing, Tri Dao, and Yoon Kim. Log-linear attention. _arXiv preprint arXiv:2506.04761_, 2025.", "sections": [{"section_id": "section_0", "title": "Hierarchical Reasoning Model", "content": "Guan Wang\\({}^{1,\\dagger}\\), Jin Li\\({}^{1}\\), Yuhao Sun\\({}^{1}\\), Xing Chen\\({}^{1}\\), Changling Liu\\({}^{1}\\),\nYue Wu\\({}^{1}\\), Meng Lu\\({}^{1,\\dagger}\\), Sen Song\\({}^{2,\\dagger}\\), Yasin Abbasi Yadkori\\({}^{1,\\dagger}\\)\n\\({}^{1}\\)Sapient Intelligence, Singapore", "level": 1, "page_numbers": [], "subsections": [], "references": [], "equations": [], "images": [], "tables": []}, {"section_id": "section_1", "title": "Abstract", "content": "Reasoning, the process of devising and executing complex goal-oriented action sequences, remains a critical challenge in AI. Current large language models (LLMs) primarily employ Chain-of-Thought (CoT) techniques, which suffer from brittle task decomposition, extensive data requirements, and high latency. Inspired by the hierarchical and multi-timescale processing in the human brain, we propose the Hierarchical Reasoning Model (HRM), a novel recurrent architecture that attains significant computational depth while maintaining both training stability and efficiency. HRM executes sequential reasoning tasks in a single forward pass without explicit supervision of the intermediate process, through two interdependent recurrent modules: a high-level module responsible for slow, abstract planning, and a low-level module handling rapid, detailed computations. With only 27 million parameters, HRM achieves exceptional performance on complex reasoning tasks using only 1000 training samples. The model operates without pre-training or CoT data, yet achieves nearly perfect performance on challenging tasks including complex Sudoku puzzles and optimal path finding in large mazes. Furthermore, HRM outperforms much larger models with significantly longer context windows on the Abstraction and Reasoning Corpus (ARC), a key benchmark for measuring artificial general intelligence capabilities. These results underscore HRM's potential as a transformative advancement toward universal computation and general-purpose reasoning systems.\nFigure 1: **Left:** HRM is inspired by hierarchical processing and temporal separation in the brain. It has two recurrent networks operating at different timescales to collaboratively solve tasks. **Right:** With only about 1000 training examples, the HRM (\\(\\sim\\)27M parameters) surpasses state-of-the-art CoT models on inductive benchmarks (ARC-AGI) and challenging symbolic tree-search puzzles (_Sudoku-Extreme_, _Maze-Hard_) where CoT models failed completely. The HRM was randomly initialized, and it solved the tasks directly from inputs without chain of thoughts.\nIntroduction\nDeep learning, as its name suggests, emerged from the idea of stacking more layers to achieve increased representation power and improved performance [1, 2]. However, despite the remarkable success of large language models, their core architecture is paradoxically shallow [3]. This imposes a fundamental constraint on their most sought-after capability: reasoning. The fixed depth of standard Transformers places them in computational complexity classes such as \\(AC^{0}\\) or \\(TC^{0}\\)[4], preventing them from solving problems that require polynomial time [5, 6]. LLMs are not Turing-complete and thus they cannot, at least in a purely end-to-end manner, execute complex algorithmic reasoning that is necessary for deliberate planning or symbolic manipulation tasks [7, 8]. For example, our results on the Sudoku task show that increasing Transformer model depth _can_ improve performance,1 but performance remains far from optimal even with very deep models (see Figure 2), which supports the conjectured limitations of the LLM scaling paradigm [9].\nFootnote 1: Simply increasing the model width does not improve performance here.\nThe LLMs literature has relied largely on Chain-of-Thought (CoT) prompting for reasoning [10]. CoT externalizes reasoning into token-level language by breaking down complex tasks into simpler intermediate steps, sequentially generating text using a shallow model [11]. However, CoT for reasoning is a crutch, not a satisfactory solution. It relies on brittle, human-defined decompositions where a single misstep or a misorder of the steps can derail the reasoning process entirely [12, 13]. This dependency on explicit linguistic steps tethers reasoning to patterns at the token level. As a result, CoT reasoning often requires significant amount of training data and generates a large number of tokens for complex reasoning tasks, resulting in slow response times. A more efficient approach is needed to minimize these data requirements [14].\nTowards this goal, we explore \"latent reasoning\", where the model conducts computations within its internal hidden state space [15, 16]. This aligns with the understanding that language is a tool for human communication, not the substrate of thought itself [17]; the brain sustains lengthy, coherent chains of reasoning with remarkable efficiency in a latent space, without constant translation back to language. However, the power of latent reasoning is still fundamentally constrained by a model's _effective computational depth_. Naively stacking layers is notoriously difficult due to vanishing gradients, which plague training stability and effectiveness [1, 18]. Recurrent architectures, a natural alternative for sequential tasks, often suffer from early convergence, rendering subsequent computational steps inert, and rely on the biologically implausible, computationally expensive and memory intensive Backpropagation Through Time (BPTT) for training [19].\nThe human brain provides a compelling blueprint for achieving the effective computational depth that contemporary artificial models lack. It organizes computation hierarchically across cortical regions operating at different timescales, enabling deep, multi-stage reasoning [20, 21, 22]. Recurrent feedback loops iteratively refine internal representations, allowing slow, higher-level areas to guide, and fast, lower-level circuits to execute--subordinate processing while preserving global coherence [23, 24, 25]. Notably, the brain achieves such depth without incurring the prohibitive credit-assignment costs that typically hamper recurrent networks from backpropagation through time [19, 26].\nInspired by this hierarchical and multi-timescale biological architecture, we propose the Hierarchical Reasoning Model (HRM). HRM is designed to significantly increase the effective computational depth. It features two coupled recurrent modules: a high-level (H) module for abstract, deliberate reasoning, and a low-level (L) module for fast, detailed computations. This structure\navoids the rapid convergence of standard recurrent models through a process we term \"hierarchical convergence.\" The slow-updating H-module advances only after the fast-updating L-module has completed multiple computational steps and reached a local equilibrium, at which point the L-module is reset to begin a new computational phase.\nFurthermore, we propose a one-step gradient approximation for training HRM, which offers improved efficiency and eliminates the requirement for BPTT. This design maintains a constant memory footprint (\\(O(1)\\) compared to BPTT's \\(O(T)\\) for \\(T\\) timesteps) throughout the backpropagation process, making it scalable and more biologically plausible.\nLeveraging its enhanced effective depth, HRM excels at tasks that demand extensive search and backtracking. **Using only 1,000 input-output examples, without pre-training or CoT supervision,** HRM learns to solve problems that are intractable for even the most advanced LLMs. For example, it achieves near-perfect accuracy in complex Sudoku puzzles (_Sudoku-Extreme Full_) and optimal pathfinding in 30x30 mazes, where state-of-the-art CoT methods completely fail (0% accuracy). In the Abstraction and Reasoning Corpus (ARC) AGI Challenge [27, 28, 29] - a benchmark of inductive reasoning - HRM, trained from scratch with only the official dataset (~1000 examples), with only 27M parameters and a 30x30 grid context (900 tokens), achieves a performance of **40.3%**, which substantially surpasses leading CoT-based models like o3-mini-high (34.5%) and Claude 3.7 8K context (21.2%), despite their considerably larger parameter sizes and context lengths, as shown in Figure 1. This represents a promising direction toward the development of next-generation AI reasoning systems with universal computational capabilities.", "level": 1, "page_numbers": [], "subsections": [], "references": [], "equations": [], "images": [], "tables": []}, {"section_id": "section_2", "title": "2 Hierarchical Reasoning Model", "content": "We present the HRM, inspired by three fundamental principles of neural computation observed in the brain:\n* [leftmargin=*]\n* **Hierarchical processing:** The brain processes information across a hierarchy of cortical areas. Higher-level areas integrate information over longer timescales and form abstract representations, while lower-level areas handle more immediate, detailed sensory and motor processing [20, 22, 21].\nFigure 2: **The necessity of depth for complex reasoning. Left:** On _Sudoku-Extreme Full_, which require extensive tree-search and backtracking, increasing a Transformer’s width yields no performance gain, while increasing depth is critical. **Right:** Standard architectures saturates, failing to benefit from increased depth. HRM overcomes this fundamental limitation, effectively using its computational depth to achieve near-perfect accuracy.\n* **Temporal Separation:** These hierarchical levels in the brain operate at distinct intrinsic timescales, reflected in neural rhythms (e.g., slow theta waves, 4-8 Hz and fast gamma waves, 30-100 Hz) [30, 31]. This separation allows for stable, high-level guidance of rapid, low-level computations [32, 33].\n* **Recurrent Connectivity:** The brain features extensive recurrent connections. These feedback loops enable iterative refinement, yielding more accurate and context-sensitive representations at the cost of additional processing time. Additionally, the brain largely avoids the problematic deep credit assignment problem associated with BPTT19. Footnote 19: While inspired by temporal separation in the brain, our model’s “high-level” and “low-level” modules are conceptual abstractions and do not map directly to specific neural oscillation frequencies.\nThe HRM model consists of four learnable components: an input network \\(f_{I}(\\cdot;\\theta_{I})\\), a low-level recurrent module \\(f_{L}(\\cdot;\\theta_{L})\\), a high-level recurrent module \\(f_{H}(\\cdot;\\theta_{H})\\), and an output network \\(f_{O}(\\cdot;\\theta_{O})\\). The model's dynamics unfold over \\(N\\) high-level cycles of \\(T\\) low-level timesteps each2. We index the total timesteps of one forward pass by \\(i=1,\\ldots,N\\times T\\). The modules \\(f_{L}\\) and \\(f_{H}\\) each keep a hidden state--\\(z_{L}^{i}\\) for \\(f_{L}\\) and \\(z_{H}^{i}\\) for \\(f_{H}\\)--which are initialized with the vectors \\(z_{L}^{0}\\) and \\(z_{H}^{0}\\), respectively.\nFootnote 2: While inspired by temporal separation in the brain, our model’s “high-level” and “low-level” modules are conceptual abstractions and do not map directly to specific neural oscillation frequencies.\nThe HRM maps an input vector \\(x\\) to an output prediction vector \\(\\hat{y}\\) as follows. First, the input \\(x\\) is projected into a working representation \\(\\tilde{x}\\) by the input network:\n\\[\\tilde{x}=f_{I}(x;\\theta_{I})\\ .\\]\nAt each timestep \\(i\\), the L-module updates its state conditioned on its own previous state, the H-module's current state (which remains fixed throughout the cycle), and the input representation. The H-module only updates once per cycle (i.e., every \\(T\\) timesteps) using the L-module's final state at the end of that cycle:\n\\[z_{L}^{i} =f_{L}\\left(z_{L}^{i-1},z_{H}^{i-1},\\tilde{x};\\theta_{L}\\right)\\ ,\\] \\[z_{H}^{i} =\\begin{cases}f_{H}\\left(z_{H}^{i-1},z_{L}^{i-1};\\theta_{H}\\right) &\\text{if }i\\equiv 0\\,\\left(\\mathrm{mod}\\ T\\right),\\\\ z_{H}^{i-1}&\\text{otherwise}\\ \\ .\\end{cases}\\]\nFinally, after \\(N\\) full cycles, a prediction \\(\\hat{y}\\) is extracted from the hidden state of the H-module:\n\\[\\hat{y}=f_{O}(z_{H}^{NT};\\theta_{O})\\ .\\]\nThis entire \\(NT\\)-timestep process represents a single forward pass of the HRM. A halting mechanism (detailed later in this section) determines whether the model should terminate, in which case \\(\\hat{y}\\) will be used as the final prediction, or continue with an additional forward pass.\n**Hierarchical convergence** Although convergence is crucial for recurrent networks, standard RNNs are fundamentally limited by their tendency to converge too early. As the hidden state settles toward a fixed point, update magnitudes shrink, effectively stalling subsequent computation and capping the network's effective depth. To preserve computational power, we actually want convergence to proceed very slowly-but engineering that gradual approach is difficult, since pushing convergence too far edges the system toward instability.\nHRM is explicitly designed to counteract this premature convergence through a process we term _hierarchical convergence_. During each cycle, the L-module (an RNN) exhibits stable convergence to a _local equilibrium_. This equilibrium, however, depends on the high-level state \\(z_{H}\\) supplied during that cycle. After completing the \\(T\\) steps, the H-module incorporates the sub-computation's outcome (the final state \\(z_{L}\\)) and performs its own update. This \\(z_{H}\\) update establishes a fresh context for the L-module, essentially \"restarting\" its computational path and initiating a new convergence phase toward a different local equilibrium.\nThis process allows the HRM to perform a sequence of distinct, stable, nested computations, where the H-module directs the overall problem-solving strategy and the L-module executes the intensive search or refinement required for each step. Although a standard RNN may approach convergence within \\(T\\) iterations, the hierarchical convergence benefits from an enhanced effective depth of \\(NT\\) steps. As empirically shown in Figure 3, this mechanism allows HRM both to maintain high computational activity (forward residual) over many steps (in contrast to a standard RNN, whose activity rapidly decays) and to enjoy stable convergence. This translates into better performance at any computation depth, as illustrated in Figure 2.\n**Approximate gradient** Recurrent models typically use BPTT to compute gradients. However, BPTT requires storing the hidden states from the forward pass and then combining them with gradients during the backward pass, which demands \\(O(T)\\) memory for T timesteps. This heavy memory burden forces smaller batch sizes and leads to poor GPU utilization, especially for large-scale networks. Additionally, because retaining the full history trace through time is biologically implausible, it is unlikely that the brain implements BPTT [19].\nFortunately, if a recurrent neural network converges to a fixed point, we can avoid unrolling its state sequence by applying backpropagation in a single step at that equilibrium point. Moreover, such a mechanism could plausibly be implemented in the brain using only local learning rules [34, 35]. Based\nFigure 3: Comparison of forward residuals and PCA trajectories. HRM shows hierarchical convergence: the H-module steadily converges, while the L-module repeatedly converges within cycles before being reset by H, resulting in residual spikes. The recurrent neural network exhibits rapid convergence with residuals quickly approaching zero. In contrast, the deep neural network experiences vanishing gradients, with significant residuals primarily in the initial (input) and final layers.\non this finding, we propose a one-step approximation of the HRM gradient-using the gradient of the last state of each module and treating other states as constant. The gradient path is, therefore,\n\\[\\text{Output head}\\rightarrow\\text{final state of the H-module}\\rightarrow\\text{final state of the L-module}\\rightarrow\\text{input embedding}\\]\nThe above method needs \\(O(1)\\) memory, does not require unrolling through time, and can be easily implemented with an autograd framework such as PyTorch, as shown in Figure 4. Given that each module only needs to back-propagate errors through its most recent local synaptic activity, this approach aligns well with the perspective that cortical credit assignment relies on short-range, temporally local mechanisms rather than on a global replay of activity patterns.\nThe one-step gradient approximation is theoretically grounded in the mathematics of Deep Equilibrium Models (DEQ) [36] which employs the Implicit Function Theorem (IFT) to bypass BPTT, as detailed next. Consider an idealized HRM behavior where, during high-level cycle \\(k\\), the L-module repeatedly updates until its state \\(z_{L}\\) converges to a local fixed point \\(z_{H}^{\\star}\\). This fixed point, given the current high-level state \\(z_{H}^{k-1}\\), can be expressed as\n\\[z_{L}^{\\star}=f_{L}(z_{L}^{\\star},z_{H}^{k-1},\\tilde{x};\\theta_{L})\\ .\\]\nThe H-module then performs a single update using this converged L-state:\n\\[z_{H}^{k}=f_{H}(z_{H}^{k-1},z_{L}^{\\star};\\theta_{H})\\ .\\]\nWith a proper mapping \\(\\mathcal{F}\\), the updates to the high-level state can be written in a more compact form as \\(z_{H}^{k}=\\mathcal{F}(z_{H}^{k-1};\\tilde{x},\\theta)\\), where \\(\\theta=(\\theta_{I},\\theta_{L})\\), and the fixed-point can be written as \\(z_{H}^{\\star}=\\mathcal{F}(z_{H}^{\\star};\\tilde{x},\\theta)\\). Let \\(J_{\\mathcal{F}}=\\frac{\\partial\\mathcal{F}}{\\partial z_{H}}\\) be the Jacobian of \\(\\mathcal{F}\\), and assume that the matrix \\(I-J_{\\mathcal{F}}\\) is invertible at \\(z_{H}^{\\star}\\) and that the mapping \\(\\mathcal{F}\\) is continuously differentiable. The Implicit Function Theorem then allows us to calculate the exact gradient of fixed point \\(z_{H}^{\\star}\\) with respect to the parameters \\(\\theta\\) without explicit back-propagation:\n\\[\\frac{\\partial z_{H}^{\\star}}{\\partial\\theta}=\\left(I-J_{\\mathcal{F}}\\big{|}_ {z_{H}^{\\star}}\\right)^{-1}\\frac{\\partial\\mathcal{F}}{\\partial\\theta}\\bigg{|} _{z_{H}^{\\star}}\\ .\\] (1)\nCalculating the above gradient requires evaluating and inverting matrix \\((I-J_{\\mathcal{F}})\\) that can be computationally expensive. Given the Neumann series expansion,\n\\[(I-J_{\\mathcal{F}})^{-1}=I+J_{\\mathcal{F}}+J_{\\mathcal{F}}^{2}+J_{\\mathcal{F} }^{3}+\\ldots\\ ,\\]\nthe so-called _1-step gradient_[37] approximates the series by considering only its first term, i.e. \\((I-J_{\\mathcal{F}})^{-1}\\approx I\\), and leads to the following approximation of Equation (1):\n\\[\\frac{\\partial z_{H}^{\\star}}{\\partial\\theta_{H}}\\approx\\frac{ \\partial f_{H}}{\\partial\\theta_{H}},\\quad\\frac{\\partial z_{H}^{\\star}}{ \\partial\\theta_{L}}\\approx\\frac{\\partial f_{H}}{\\partial z_{L}^{\\star}}\\cdot \\frac{\\partial z_{L}^{\\star}}{\\partial\\theta_{L}},\\quad\\frac{\\partial z_{H}^ {\\star}}{\\partial\\theta_{I}}\\approx\\frac{\\partial f_{H}}{\\partial z_{L}^{ \\star}}\\cdot\\frac{\\partial z_{L}^{\\star}}{\\partial\\theta_{I}}\\ .\\] (2)\nFigure 4: **Top:** Diagram of HRM with approximate gradient. **Bottom:** Pseudocode of HRM with deep supervision training in PyTorch.\nThe gradients of the low-level fixed point, \\(\\frac{\\partial z_{L}^{*}}{\\partial\\theta_{L}}\\) and \\(\\frac{\\partial z_{L}^{*}}{\\partial\\theta_{I}}\\), can also be approximated using another application of the 1-step gradient:\n\\[\\frac{\\partial z_{L}^{*}}{\\partial\\theta_{L}}\\approx\\frac{\\partial f_{L}}{ \\partial\\theta_{L}},\\quad\\frac{\\partial z_{L}^{*}}{\\partial\\theta_{I}}\\approx \\frac{\\partial f_{L}}{\\partial\\theta_{I}}\\ .\\] (3)\nBy substituting Equation (3) back into Equation (2), we arrive at the final simplified gradients.\nBefore defining our loss function, we must first introduce two key elements of our proposed method: _deep supervision_ and _adaptive computational time_.\n**Deep supervision** Inspired by the principle that periodic neural oscillations regulate when learning occurs in the brain [38], we incorporate a deep supervision mechanism into HRM, as detailed next.\nGiven a data sample \\((x,y)\\), we run multiple forward passes of the HRM model, each of which we refer to as a _segment_. Let \\(M\\) denote the total number of segments executed before termination. For each segment \\(m\\in\\{1,\\dots,M\\}\\), let \\(z^{m}=(z_{H}^{mNT},z_{L}^{mNT})\\) represent the hidden state at the conclusion of segment \\(m\\), encompassing both high-level and low-level state components.\nAt each segment \\(m\\), we apply a deep supervision step as follows:", "level": 1, "page_numbers": [], "subsections": [], "references": [], "equations": [], "images": [], "tables": []}, {"section_id": "section_3", "title": "3. Update parameters: \\[\\theta\\leftarrow\\text{OptimizerStep}(\\theta,\\nabla_{\\theta}L^{m})\\]", "content": "The crucial aspect of this procedure is that the hidden state \\(z^{m}\\) is \"detached\" from the computation graph before being used as the input state for the next segment. Consequently, gradients from segment \\(m+1\\) do not propagate back through segment \\(m\\), effectively creating a 1-step approximation of the gradient of the recursive deep supervision process [39, 40]. This approach provides more frequent feedback to the H-module and serves as a regularization mechanism, demonstrating superior empirical performance and enhanced stability in deep equilibrium models when compared to more complex, Jacobian-based regularization techniques [39, 41]. Figure 4 shows pseudocode of deep supervision training.\n**Adaptive computational time (ACT)** The brain dynamically alternates between automatic thinking (\"System 1\") and deliberate reasoning (\"System 2\") [42]. Neuroscientific evidence shows that these cognitive modes share overlapping neural circuits, particularly within regions such as the prefrontal cortex and the default mode network [43, 44]. This indicates that the brain dynamically modulates the \"runtime\" of these circuits according to task complexity and potential rewards [45, 46].\nInspired by the above mechanism, we incorporate an adaptive halting strategy into HRM that enables \"thinking, fast and slow\". This integration leverages deep supervision and uses the Q-learning\nalgorithm [47] to adaptively determine the number of segments. A Q-head uses the final state of the H-module to predict the Q-values \\(\\hat{Q}^{m}=(\\hat{Q}^{m}_{\\text{halt}},\\hat{Q}^{m}_{\\text{continue}})\\) of the \"halt\" and \"continue\" actions:\n\\[\\hat{Q}^{m}=\\sigma(\\theta_{Q}^{\\top}z_{H}^{mNT})\\,,\\]\nwhere \\(\\sigma\\) denotes the sigmoid function applied element-wise. The halt or continue action is chosen using a randomized strategy as detailed next. Let \\(M_{\\max}\\) denote the maximum number of segments (a fixed hyperparameter) and \\(M_{\\min}\\) denote the minimum number of segments (a random variable). The value of \\(M_{\\min}\\) is determined stochastically: with probability \\(\\varepsilon\\), it is sampled uniformly from the set \\(\\{2,\\cdots,M_{\\max}\\}\\) (to encourage longer thinking), and with probability \\(1-\\varepsilon\\), it is set to 1. The halt action is selected under two conditions: when the segment count surpasses the maximum threshold \\(M_{\\max}\\), or when the estimated halt value \\(\\hat{Q}_{\\text{halt}}\\) exceeds the estimated continue value \\(\\hat{Q}_{\\text{continue}}\\) and the segment count has reached at least the minimum threshold \\(M_{\\min}\\).\nThe Q-head is updated through a Q-learning algorithm, which is defined on the following episodic Markov Decision Process (MDP). The state of the MDP at segment \\(m\\) is \\(z^{m}\\), and the action space is \\(\\{\\text{halt},\\text{continue}\\}\\). Choosing the action \"halt\" terminates the episode and returns a binary reward indicating prediction correctness, i.e., \\(\\mathbf{1}\\{\\hat{y}^{m}=y\\}\\). Choosing \"continue\" yields a reward of 0 and the state transitions to \\(z^{m+1}\\). Thus, the Q-learning targets for the two actions \\(\\hat{G}^{m}=(\\hat{G}^{m}_{\\text{halt}},\\hat{G}^{m}_{\\text{continue}})\\) are given by\n\\[\\hat{G}^{m}_{\\text{halt}} =\\mathbf{1}\\{\\hat{y}^{m}=y\\}\\,,\\] \\[\\hat{G}^{m}_{\\text{continue}} =\\begin{cases}\\hat{Q}^{m+1}_{\\text{halt}},&\\text{if }m\\geq N_{ \\max}\\,,\\\\ \\max(\\hat{Q}^{m+1}_{\\text{halt}},\\hat{Q}^{m+1}_{\\text{continue}})\\,,&\\text{ otherwise }.\\end{cases}\\]\nWe can now define the loss function of our learning procedure. The overall loss for each supervision segment combines both the Q-head loss and the sequence-to-sequence loss:\n\\[L^{m}_{\\text{ACT}}=\\textsc{Loss}(\\hat{y}^{m},y)+\\textsc{BinaryCrossEntropy}( \\hat{Q}^{m},\\hat{G}^{m})\\ .\\]\nMinimizing the above loss enables both accurate predictions and nearly optimal stopping decisions.\nSelecting the \"halt\" action ends the supervision loop. In practice, sequences are processed in batches, which can be easily handled by substituting any halted sample in the batch with a fresh sample from the dataloader.\nFigure 5 presents a performance comparison between two HRM variants: one incorporating ACT and another employing a fixed computational step count equivalent to ACT's \\(M_{\\max}\\) parameter. It shows that ACT effectively adapts its computational resources based on task complexity, achieving significant computational savings with minimal impact on performance.\n**Inference-time scaling** An effective neural model should exploit additional computational resources during inference to enhance performance. As illustrated in Figure 5-(c), HRM seamlessly achieves inference-time scaling by simply increasing the computational limit parameter, \\(M_{\\max}\\) without requiring further training or architectural modifications.\nAdditional compute is especially effective for tasks that demand deeper reasoning. On Sudoku--a problem that often requires long-term planning--HRM exhibits strong inference-time scaling. On the other hand, we find that extra computational resources yield minimal gains in ARC-AGI challenge, as solutions generally require only a few transformations.", "level": 1, "page_numbers": [], "subsections": [], "references": [], "equations": [], "images": [], "tables": []}, {"section_id": "section_4", "title": "3.2.2 Stability of Q-learning in ACT", "content": "The deep Q-learning that underpins our ACT mechanism is known to be prone to instability, often requiring stabilization techniques such as replay buffers and target networks [48], which are absent in our design. Our approach, however, achieves stability through the intrinsic properties of our model and training procedure. Recent theoretical work by Gallici et al. [49] shows that Q-learning can achieve convergence if network parameters are bounded, weight decay is incorporated during training, and post-normalization layers are implemented. Our model satisfies these conditions through its Post-Norm architecture that employs RMSNorm (a layer normalization variant) and the AdamW optimizer. AdamW has been shown to solve an \\(L_{\\infty}\\)-constrained optimization problem, ensuring that model parameters remain bounded by \\(1/\\lambda\\)[50].", "level": 1, "page_numbers": [], "subsections": [], "references": [], "equations": [], "images": [], "tables": []}, {"section_id": "section_5", "title": "3.2.3 Architectural details", "content": "We employ a sequence-to-sequence architecture for HRM. Both input and output are represented as token sequences: \\(x=(x_{1},\\ldots,x_{l})\\) and \\(y=(y_{1},\\ldots,y_{l^{\\prime}})\\) respectively. The model includes an embedding layer \\(f_{I}\\) that converts discrete tokens into vector representations, and an output head \\(f_{O}(z;\\theta_{O})=\\text{softmax}(\\theta_{O}z)\\) that transforms hidden states into token probability distributions \\(\\hat{y}\\). For small-sample experiments, we replace softmax with stablemax [51] to improve generalization performance. The sequence-to-sequence loss is averaged over all tokens, \\(\\textsc{Loss}(\\hat{y},y)=\\frac{1}{l^{\\prime}}\\sum_{i=1}^{l^{\\prime}}\\log p(y_{ i})\\), where \\(p(y_{i})\\) is the probability that distribution \\(\\hat{y}_{i}\\) assigns to token \\(y_{i}\\). The initial hidden states \\(z^{0}\\) are initialized by sampling from a truncated normal distribution with standard deviation of 1, truncation of 2, and kept fixed throughout training.\nBoth the low-level and high-level recurrent modules \\(f_{L}\\) and \\(f_{H}\\) are implemented using encoder-only Transformer [52] blocks with identical architectures and dimensions. These modules take multiple inputs, and we use straightforward element-wise addition to combine them, though more sophisticated merging techniques such as gating mechanisms could potentially improve performance and is left for future work. For all Transformer blocks in this work--including those in the baseline models--we incorporate the enhancements found in modern LLMs (based on Llama [53] architectures). These improvements include Rotary Positional Encoding [54], Gated Linear Units [55], RMSNorm [56], and the removal of bias terms from linear layers.\nFurthermore, both HRM and recurrent Transformer models implement a Post-Norm architecture\nFigure 5: **Effectiveness of Adaptive Computation Time (ACT)** on the _Sudoku-Extreme-Full_. **(a)** Mean compute steps used by models with ACT versus models with a fixed number of compute steps (\\(M\\)). ACT maintains a low and stable number of average compute steps even as the maximum limit (\\(M_{\\max}\\)) increases. **(b)** Accuracy comparison. The ACT model achieves performance comparable to the fixed-compute model while utilizing substantially fewer computational steps on average. **(c)** Inference-time scalability. Models trained with a specific \\(M_{\\max}\\) can generalize to higher computational limits during inference, leading to improved accuracy. For example, a model trained with \\(M_{\\max}=8\\) continues to see accuracy gains when run with \\(M_{\\max}=16\\) during inference.\nwith weights initialized via truncated LeCun Normal initialization [57, 58, 59], while the scale and bias parameters are excluded from RMSNorm. All parameters are optimized using the Adam-atan2 optimizer [60], a scale-invariant variant of Adam [61], combined with a constant learning rate that includes linear warm-up.", "level": 1, "page_numbers": [], "subsections": [], "references": [], "equations": [], "images": [], "tables": []}, {"section_id": "section_6", "title": "3 Results", "content": "This section begins by describing the ARC-AGI, Sudoku, and Maze benchmarks, followed by an overview of the baseline models and their results. Figure 6-(a,b,c) presents a visual representation of the three benchmark tasks, which are selected to evaluate various reasoning abilities in AI models.", "level": 1, "page_numbers": [], "subsections": [], "references": [], "equations": [], "images": [], "tables": []}, {"section_id": "section_7", "title": "3.1.1 ARC-AGI Challenge", "content": "The ARC-AGI benchmark evaluates general fluid intelligence through IQ-test-like puzzles that require inductive reasoning [27]. The initial version, ARC-AGI-1, presents challenges as input-output grid pairs that force AI systems to extract and generalize abstract rules from just a few examples. Each task provides a few input-output example pairs (usually 2-3) and a test input. An AI model has two attempts to produce the correct output grid. Although some believe that mastering ARC-AGI would signal true artificial general intelligence, its primary purpose is to expose the current roadblocks in AGI progress. In fact, both conventional deep learning methods and CoT techniques have faced significant challenges with ARC-AGI-1, primarily because it requires the ability to generalize to entirely new tasks [28].\nAddressing the limitations identified in ARC-AGI-1, ARC-AGI-2 significantly expands the benchmark by providing a more comprehensive and carefully refined collection of tasks. These new tasks emphasize deeper compositional reasoning, multi-step logic, contextual rule application, and symbolic abstraction. Human calibration studies show these tasks are challenging but doable for people, while being much harder for current AI systems, offering a clearer measure of general reasoning abilities [29].\nFigure 6: **Left: Visualization of benchmark tasks. Right: Difficulty of _Sudoku-Extreme_ examples.**\nSudoku-ExtremeSudoku is a 9\\(\\times\\)9 logic puzzle, requiring each row, column, and 3\\(\\times\\)3 block to contain the digits 1-9 exactly once. A prediction is considered correct if it exactly matches the puzzle's unique solution. Sudoku's complex logical structure makes it a popular benchmark for evaluating logical reasoning in machine learning [62, 63, 64].\nThe most frequently used Sudoku dataset in research, namely the Kaggle dataset [65], can be fully solved using elementary single-digit techniques [66]. The minimal 17-clue puzzles [62], another widely-used collection, might seem more challenging due to its small number of clues. However, this perception is misleading--since 17 represents the minimum number of clues required to guarantee a unique Sudoku solution, these hints need to be highly orthogonal to each other. This orthogonal arrangement leads to many direct, easily-resolved solution paths [67].\nWe introduce _Sudoku-Extreme_, a more challenging dataset that is compiled from the aforementioned easy datasets as well as puzzles recognized by the Sudoku community as exceptionally difficult for human players:\n* Easy puzzles compiled from Kaggle, 17-clue, plus unbiased samples from the Sudoku puzzle distribution [67]: totaling \\(1\\,149\\,158\\) puzzles.\n* Challenging puzzles compiled from Magictour 1465, Forum-Hard and Forum-Extreme subsets: totaling \\(3\\,104\\,157\\) puzzles.\nThe compiled data then undergo a strict 90/10 train-test split, ensuring that the test set puzzles cannot be derived through equivalent transformations of any training samples. _Sudoku-Extreme_ is a down-sampled subset of this data containing 1000 training examples. We use _Sudoku-Extreme_ in our main experiments (Figure 1), which focuses on small-sample learning scenarios. To guarantee convergence and control overfitting effects in our analysis experiments (Figures 2, 3 and 5), we use the complete training data, _Sudoku-Extreme-Full_, containing \\(3\\,831\\,994\\) examples.\nWe measure puzzle difficulty by counting the number of search backtracks (\"guesses\") required by a smart Sudoku solver program _tdoku_, which uses propositional logic to reduce the number of guesses [67]. Our _Sudoku-Extreme_ dataset exhibits a mean difficulty of \\(22\\) backtracks per puzzle, significantly higher than existing datasets, including recent handmade puzzles Sudoku-Bench [68] which average just \\(0.45\\) backtracks per puzzle. These subset complexity levels are shown in Figure 6-(d).\nFor ARC-AGI challenge, we start with all input-output example pairs in the training and the evaluation sets. The dataset is augmented by applying translations, rotations, flips, and color permutations to the puzzles. Each task example is prepended with a learnable special token that represents the puzzle it belongs to. At test time, we proceed as follows for each test input in the evaluation set: (1) Generate and solve 1000 augmented variants and, for each, apply the inverse-augmentation transform to obtain a prediction. (2) Choose the two most popular predictions as the final outputs.3 All results are reported on the evaluation set.\nFootnote 3: The ARC-AGI allows two attempts for each test input.\nWe augment Sudoku puzzles by applying band and digit permutations, while data augmentation is disabled for Maze tasks. Both tasks undergo only a single inference pass.\nFor ARC-AGI, the scores of the CoT models are taken from the official leaderboard [29], while for Sudoku and Maze, the scores are obtained by evaluating through the corresponding API.\nIn Figure 1, the baselines are grouped based on whether they are pre-trained and use CoT, or neither. The \"Direct pred\" baseline means using \"direct prediction without CoT and pre-training\", which retains the exact training setup of HRM but swaps in a Transformer architecture. Interestingly, on ARC-AGI-1, \"Direct pred\" matches the performance of Liao and Gu [73], who built a carefully designed, domain-specific equivariant network for learning the ARC-AGI task from scratch, without pre-training. By substituting the Transformer architecture with HRM's hierarchical framework and implementing ACT, we achieve more than a twofold performance improvement.\nOn the _Sudoku-Extreme_ and _Maze-Hard_ benchmarks, the performance gap between HRM and the baseline methods is significant, as the baselines almost never manage to solve the tasks. These benchmarks that demand lengthy reasoning traces are particularly difficult for CoT-based methods. With only 1000 training examples, the \"Direct pred\" baseline--which employs an 8-layer Transformer identical in size to HRM--fails entirely on these challenging reasoning problems. When trained on the larger _Sudoku-Extreme-Full_ dataset, however, \"Direct pred\" can solve some easy Sudoku puzzles and reaches \\(16.9\\%\\) accuracy (see Figure 2). Lehnert et al. [71] showed that a large vanilla Transformer model with 175M parameters, trained on 1 million examples across multiple trials, achieved only marginal success on 30x30 Maze tasks, with accuracy below \\(20\\%\\) using the \\(pass@64\\) evaluation metric.", "level": 1, "page_numbers": [], "subsections": [], "references": [], "equations": [], "images": [], "tables": []}, {"section_id": "section_8", "title": "Visualization of intermediate timesteps", "content": "Although HRM demonstrates strong performance on complex reasoning tasks, it raises an intriguing question: what underlying reasoning algorithms does the HRM neural network actually implement? Addressing this question is important for enhancing model interpretability and developing a deeper understanding of the HRM solution space.\nWhile a definitive answer lies beyond our current scope, we begin our investigation by analyzing state trajectories and their corresponding solution evolution. More specifically, at each timestep \\(i\\) and given the low-level and high-level state pair (\\(z^{i}_{L}\\) and \\(z^{i}_{H}\\)) we perform a preliminary forward pass through the H-module to obtain \\(\\bar{z}^{i}=f_{H}(z^{i}_{H},z^{i}_{L};\\theta_{H})\\) and its corresponding decoded prediction \\(\\bar{y}^{i}=f_{O}(\\bar{z}^{i};\\theta_{O})\\). The prediction \\(\\bar{y}^{i}\\) is then visualized in Figure 7.\nIn the Maze task, HRM appears to initially explore several potential paths simultaneously, subsequently eliminating blocked or inefficient routes, then constructing a preliminary solution outline followed by multiple refinement iterations. In Sudoku, the strategy resembles a depth-first search\napproach, where the model appears to explore potential solutions and backtracks when it hits dead ends. HRM uses a different approach for ARC tasks, making incremental adjustments to the board and iteratively improving it until reaching a solution. Unlike Sudoku, which involves frequent backtracking, the ARC solution path follows a more consistent progression similar to hill-climbing optimization. Importantly, the model shows that it can adapt to different reasoning approaches, likely choosing an effective strategy for each particular task. Further research is needed to gain more comprehensive insights into these solution strategies.", "level": 1, "page_numbers": [], "subsections": [], "references": [], "equations": [], "images": [], "tables": []}, {"section_id": "section_9", "title": "4 Brain Correspondence", "content": "A key principle from systems neuroscience is that a brain region's functional repertoire--its ability to handle diverse and complex tasks--is closely linked to the dimensionality of its neural representations [75, 76]. Higher-order cortical areas, responsible for complex reasoning and decision-making, must handle a wide variety of tasks, demanding more flexible and context-dependent processing [77]. In dynamical systems, this flexibility is often realized through higher-dimensional state-space trajectories, which allow for a richer repertoire of potential computations [78]. This principle gives rise to an observable _dimensionality hierarchy_, where a region's position in the processing hierarchy correlates with its _effective dimensionality_. To quantify this phenomenon, we can examine the\nFigure 7: **Visualization of intermediate predictions by HRM on benchmark tasks.****Top:**_MazeHard_—blue cells indicate the predicted path. **Middle:**_Sudoku-Extreme_—bold cells represent initial givens; red highlights cells violating Sudoku constraints; grey shading indicates changes from the previous timestep. **Bottom:** ARC-AGI-2 Task—left: provided example input-output pair; right: intermediate steps solving the test input.\nFigure 8: **Hierarchical Dimensionality Organization in the HRM and Mouse Cortex.****(a,b)** are adapted from Posani et al. 74. (a) Anatomical illustration of mouse cortical areas, color-coded by functional modules. (b) Correlation between Participation Ratio (PR), a measure of effective neural dimensionality, and hierarchical position across different mouse cortical areas. Higher positions in the hierarchy (e.g., MOs, ACAd) exhibit significantly higher PR values compared to lower sensory areas (e.g., SSp-n), with a Spearman correlation coefficient of \\(\\rho\\) = 0.79 (P = 0.0003). (c,d) **Trained HRM.** (c) PR scaling of the trained HRM with task diversity. The dimensionality of the high-level module (\\(z_{H}\\)) scales with the number of unique tasks (trajectories) included in the analysis, indicating an adaptive expansion of its representational capacity. In contrast, the low-level module’s (\\(z_{L}\\)) dimensionality remains stable. (d) PR values for the low-level (\\(z_{L}\\), PR = 30.22) and high-level (\\(z_{H}\\), PR = 89.95) modules of the _trained_ HRM, computed from neural activity during 100 unique Sudoku-solving trajectories. A clear dimensionality hierarchy is observed, with the high-level module operating in a substantially higher-dimensional space. (e,f) **Analysis of Untrained Network.** To verify that the dimensionality hierarchy is an emergent property of training, the same analyses were performed on an _untrained_ HRM with random weights. (e) In contrast to the trained model’s scaling in (c), the dimensionality of both modules in the untrained model remains low and stable, failing to scale with the number of tasks. (f) Similarly, contrasting with the clear separation in (d), the PR values for the untrained model’s modules (\\(z_{L}\\), PR = 42.09; \\(z_{H}\\), PR = 40.75) are low and nearly identical, showing no evidence of hierarchical separation. This confirms that the observed hierarchical organization of dimensionality is a learned property that emerges through training, not an artifact of the model’s architecture.\nParticipation Ratio (PR), which serves as a standard measure of the effective dimensionality of a high-dimensional representation [79]. The PR is calculated using the formula\n\\[\\text{PR}=\\frac{(\\sum_{i}\\lambda_{i})^{2}}{\\sum_{i}\\lambda_{i}^{2}}\\,,\\]\nwhere \\(\\{\\lambda_{i}\\}\\) are the eigenvalues of the covariance matrix of neural trajectories. Intuitively, a higher PR value signifies that variance is distributed more evenly across many dimensions, corresponding to a higher-dimensional representation. Conversely, a lower PR value indicates that variance is concentrated in only a few principal components, reflecting a more compact, lower-dimensional structure.\nThe dimensionality hierarchy can be observed, for example, in the mouse cortex, where the PR of population activity increases monotonically from low-level sensory areas to high-level associative areas, supporting this link between dimensionality and functional complexity [74] (Figure 8 (a,b)).\nWe evaluated whether HRM reproduces this neuroscientific principle by calculating the PR for both recurrent modules after training on the _Sudoku-Extreme Full_ dataset. The PR computation used the covariance matrix derived from neural states gathered across multiple Sudoku-solving trajectories. The results show a striking parallel to the biological findings. The low-level module's state (\\(z_{L}\\)) occupies a relatively small subspace with a participation ratio of 30.22, whereas the high-level module's state (\\(z_{H}\\)) operates in a substantially larger subspace with a participation ratio of 89.95, as shown in Figure 8(c). Furthermore, Figure 8(d) shows that increasing the number of unique tasks (trajectories) from 10 to 100 causes \\(z_{H}\\) dimensionality to scale up accordingly, while \\(z_{L}\\) dimensionality remains stable. These results suggest an _emergent_ separation of representational capacity between the modules that parallels their functional roles.\nTo confirm that this hierarchical organization is an emergent property of training, and not an artifact of the network's architecture, we performed a control analysis using an identical but untrained network with random weights.\nWe initialized an identical HRM architecture with random weights and, without any training, measured the PR of its modules as the network processed the same task-specific inputs given to the trained model.\nThe results, shown in Figure 8(e,f), reveal a stark contrast: the high-level and low-level modules of the untrained network exhibit no hierarchical separation, with their PR values remaining low and nearly indistinguishable from each other. This control analysis validates that the dimensionality hierarchy is an _emergent property_ that arises as the model learns to perform complex reasoning.\nThe high-to-low PR ratio in HRM (\\(z_{H}/z_{L}\\approx 2.98\\)) closely matches that measured in the mouse cortex (\\(\\approx 2.25\\)). In contrast, conventional deep networks often exhibit _neural collapse_, where last-layer features converge to a low-dimensional subspace [80, 81, 82]. HRM therefore departs from the collapse pattern and instead fosters a high-dimensional representation in its higher module. This is significant because such representations are considered crucial for cognitive flexibility and are a hallmark of higher-order brain regions like the prefrontal cortex (PFC), which is central to complex reasoning.\nThis structural parallel suggests the model has discovered a fundamental organizational principle. By learning to partition its representations into a high-capacity, high-dimensional subspace (\\(z_{H}\\)) and a more specialized, low-dimensional one (\\(z_{L}\\)), HRM autonomously discovers an organizational\nprinciple that is thought to be fundamental for achieving robust and flexible reasoning in biological systems. This provides a potential mechanistic explanation for the model's success on complex, long-horizon tasks that are intractable for models lacking such a differentiated internal structure. We emphasize, however, that this evidence is correlational. While a causal link could be tested via intervention (e.g., by constraining the H-module's dimensionality), such methods are difficult to interpret in deep learning due to potential confounding effects on the training process itself. Thus, the causal necessity of this emergent hierarchy remains an important question for future investigation.", "level": 1, "page_numbers": [], "subsections": [], "references": [], "equations": [], "images": [], "tables": []}, {"section_id": "section_10", "title": "5.0.1 Reasoning and algorithm learning", "content": "Given the central role of reasoning problems and their close relation to algorithms, researchers have long explored neural architectures that enable algorithm learning from training instances. This line of work includes Neural Turing Machines (NTM)[83], the Differentiable Neural Computer (DNC)[84], and Neural GPUs[85]-all of which construct iterative neural architectures that mimic computational hardware for algorithm execution, and are trained to learn algorithms from data. Another notable work in this area is Recurrent Relational Networks (RRN)[62], which executes algorithms on graph representations through graph neural networks.\nRecent studies have integrated algorithm learning approaches with Transformer-based architectures. Universal Transformers extend the standard Transformer model by introducing a recurrent loop over the layers and implementing an adaptive halting mechanism. Geiping et al.[86] demonstrate that looped Transformers can generalize to a larger number of recurrent steps during inference than what they were trained on. Shen et al.[16] propose adding continuous recurrent reasoning tokens to the Transformer. Finally, TransNAR[8] combine recurrent graph neural networks with language models.\nBuilding on the success of CoT-based reasoning, a line of work have introduced fine-tuning methods that use reasoning paths from search algorithms (like A*) as SFT targets[87, 71, 70].\nWe also mention adaptive halting mechanisms designed to allocate additional computational resources to more challenging problems. This includes the Adaptive Computation Time (ACT) for RNNs[88] and follow-up research like PonderNet[89], which aims to improve the stability of this allocation process.\nHRM further pushes the boundary of algorithm learning through a brain-inspired computational architecture that achieves exceptional data efficiency and model expressiveness, successfully discovering complex and diverse algorithms from just 1000 training examples.", "level": 1, "page_numbers": [], "subsections": [], "references": [], "equations": [], "images": [], "tables": []}, {"section_id": "section_11", "title": "5.0.2 Brain-inspired reasoning architectures", "content": "Developing a model with the reasoning power of the brain has long been a goal in brain-inspired computing. Spaun[90] is one notable example, which uses spiking neural networks to create distinct modules corresponding to brain regions like the visual cortex and prefrontal cortex. This design enables an architecture to perform a range of cognitive tasks, from memory recall to simple reasoning puzzles. However, its reasoning relies on hand-designed algorithms, which may limit its ability to learn new tasks. Another significant model is the Tolman-Eichenbaum Machine (TEM)[91], which is inspired by the hippocampal-entorhinal system's role in spatial and relational memory tasks. TEM proposes that medial entorhinal cells create a basis for structural knowledge, while hippocampal cells link this basis to sensory information. This allows TEM to generalize and explains the emergence of various cell types like grid, border, and\nplace cells. Another approach involves neural sampling models [92], which view the neural signaling process as inference over a distribution, functioning similarly to a Boltzmann machine. These models often require hand-made rules to be set up for solving a specific reasoning task. In essence, while prior models are restricted to simple reasoning problems, HRM is designed to solve complex tasks that are hard for even advanced LLMs, without pre-training or task-specific manual design.\n**Hierarchical memory**: The hierarchical multi-timescale structure also plays an important role in how the brain processes memory. Models such as Hierarchical Sequential Models [93] and Clockwork RNN [94] use multiple recurrent modules that operate at varying time scales to more effectively capture long-range dependencies within sequences, thereby mitigating the forgetting issue in RNNs.\nSimilar mechanisms have also been adopted in linear attention methods for memorizing long contexts (see the Discussions section). Since HRM focuses on reasoning, full attention is applied for simplicity. Incorporating hierarchical memory into HRM could be a promising future direction.", "level": 1, "page_numbers": [], "subsections": [], "references": [], "equations": [], "images": [], "tables": []}, {"section_id": "section_12", "title": "6 Discussions", "content": "**Turing-completeness of HRM**: Like earlier neural reasoning algorithms including the Universal Transformer [95], HRM is computationally universal when given sufficient memory and time constraints. In other words, it falls into the category of models that can simulate any Turing machine, overcoming the computational limitations of standard Transformers discussed previously in the introduction. Given that earlier neural algorithm reasoners were trained as recurrent neural networks, they suffer from premature convergence and memory intensive BPTT. Therefore, in practice, their effective computational depth remains limited, though still deeper than that of a standard Transformer. By resolving these two challenges and being equipped with adaptive computation, HRM could be trained on long reasoning processes, solve complex puzzles requiring intensive depth-first search and backtracking, and move closer to practical Turing-completeness.\n**Reinforcement learning with chain-of-thought**: Beyond fine-tuning using human-annotated CoT, reinforcement learning (RL) represents another widely adopted training methodology. However, recent evidence suggests that RL primarily unlocks existing CoT-like capabilities rather than discovering fundamentally new reasoning mechanisms [96, 97, 98, 99]. Additionally, CoT-training with RL is known for its instability and data inefficiency, often requiring extensive exploration and careful reward design. In contrast, HRM takes feedback from dense gradient-based supervision rather than relying on a sparse reward signal. Moreover, HRM operates naturally in a continuous space, which is biologically plausible and avoids allocating same computational resources to each token, even though tokens vary in their reasoning and planning complexity [16].\n**Linear attention**: Recurrence has been explored not only for its capability in universal computation, but also as a means to replace the attention mechanism in Transformers, which suffers from quadratic time and memory complexity [100]. Recurrent alternatives offer a more efficient design by processing input tokens sequentially and predicting the next token at each time step, similar to early RNN-based language models.\nSome linear-attention variants, such as Log-linear Attention [101], share an RNN-like state-update that can be interpreted as propagating multi-timescale summary statistics, thereby retaining long-range context without the quadratic memory growth of standard self-attention. However, substituting the attention mechanism alone does not change the fact that Transformers are still fixed-depth, and require CoT as a compensatory mechanism. Notably, linear attention can operate with a reduced\nkey-value cache over extended contexts, making them more suitable for deployment on resource-constrained edge devices.", "level": 1, "page_numbers": [], "subsections": [], "references": [], "equations": [], "images": [], "tables": []}, {"section_id": "section_13", "title": "7 Conclusion", "content": "This work introduces the Hierarchical Reasoning Model, a brain-inspired architecture that leverages hierarchical structure and multi-timescale processing to achieve substantial computational depth without sacrificing training stability or efficiency. With only 27M parameters and training on just 1000 examples, HRM effectively solves challenging reasoning problems such as ARC, Sudoku, and complex maze navigation-tasks that typically pose significant difficulties for contemporary LLM and chain-of-thought models.\nAlthough the brain relies heavily on hierarchical structures to enable most cognitive processes, these concepts have largely remained confined to academic literature rather than being translated into practical applications. The prevailing AI approach continues to favor non-hierarchical models. Our results challenge this established paradigm and suggest that the Hierarchical Reasoning Model represents a viable alternative to the currently dominant chain-of-thought reasoning methods, advancing toward a foundational framework capable of Turing-complete universal computation.\n**Acknowledgements** We thank Mingli Yuan, Ahmed Murtadha Hasan Mahyoub and Hengshuai Yao for their insightful discussions and valuable feedback throughout the course of this work.", "level": 1, "page_numbers": [], "subsections": [], "references": [], "equations": [], "images": [], "tables": []}, {"section_id": "section_14", "title": "References", "content": "* [1] Ian Goodfellow, Yoshua Bengio, and Aaron Courville. _Deep Learning_. MIT Press, 2016. http://www.deeplearningbook.org.\n* [2] Kaiming He, X. Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. _2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 770-778, 2015.\n* [3] Lena Strobl. Average-hard attention transformers are constant-depth uniform threshold circuits, 2023.\n* [4] Tom Bylander. Complexity results for planning. In _Proceedings of the 12th International Joint Conference on Artificial Intelligence - Volume 1_, IJCAI'91, page 274-279, San Francisco, CA, USA, 1991. Morgan Kaufmann Publishers Inc. ISBN 1558601600.\n* [5] William Merrill and Ashish Sabharwal. A logic for expressing log-precision transformers. In _Neural Information Processing Systems_, 2023.\n* [6] David Chiang. Transformers in DLOGTIME-uniform TC\\({}^{0}\\). _Transactions on Machine Learning Research_, 2025.\n* [7] Lucas Lehnert, Sainbayar Sukhbaatar, DiJia Su, Qinqing Zheng, Paul McVay, Michael Rabbat, and Yuandong Tian. Beyond a*: Better planning with transformers via search dynamics bootstrapping. In _First Conference on Language Modeling_, 2024.\n* [8] Wilfried Bounsi, Borja Ibarz, Andrew Dudzik, Jessica B. Hamrick, Larisa Markeeva, Alex Vitvitskyi, Razvan Pascanu, and Petar Velivckovic'c. Transformers meet neural algorithmic reasoners. _ArXiv_, abs/2406.09308, 2024.\n* [9] William Merrill and Ashish Sabharwal. The parallelism tradeoff: Limitations of log-precision transformers. _Transactions of the Association for Computational Linguistics_, 11:531-545, 2023. doi: 10.1162/tacl_a_00562.\n* [10] Jason Wei, Yi Tay, et al. Chain-of-thought prompting elicits reasoning in large language models, 2022. arXiv preprint arXiv:2201.11903.\n* [11] William Merrill and Ashish Sabharwal. The expressive power of transformers with chain of thought. In _ICLR_, 2024.\n* [12] Xinyun Chen, Ryan A. Chi, Xuezhi Wang, and Denny Zhou. Premise order matters in reasoning with large language models. _ArXiv_, abs/2402.08939, 2024.\n* [13] Rongwu Xu, Zehan Qi, and Wei Xu. Preemptive answer \"attacks\" on chain-of-thought reasoning. In _Annual Meeting of the Association for Computational Linguistics_, 2024.\n* [14] Pablo Villalobos, Anson Ho, Jaime Sevilla, Tamay Besiroglu, Lennart Heim, and Marius Hobbhahn. Will we run out of data? limits of llm scaling based on human-generated data. _arXiv preprint arXiv:2211.04325_, 2022.\n* [15] Xinghao Chen, Anhao Zhao, Heming Xia, Xuan Lu, Hanlin Wang, Yanjun Chen, Wei Zhang, Jian Wang, Wenjie Li, and Xiaoyu Shen. Reasoning beyond language: A comprehensive survey on latent chain-of-thought reasoning, 2025.\n* [16] Xuan Shen, Yizhou Wang, Xiangxi Shi, Yanzhi Wang, Pu Zhao, and Jiuxiang Gu. Training large language models to reason in a continuous latent space. _arXiv preprint arXiv:2412.07423_, 2024.\n* Fedorenko et al. [2024] Evelina Fedorenko, Steven T Piantadosi, and Edward AF Gibson. Language is primarily a tool for communication rather than thought. _Nature_, 630(8017):575-586, 2024.\n* Wang et al. [2024] Hongyu Wang, Shuming Ma, Li Dong, Shaohan Huang, Dongdong Zhang, and Furu Wei. Deepnet: Scaling transformers to 1,000 layers. _IEEE Transactions on Pattern Analysis and Machine Intelligence_, 2024.\n* Lillicrap and Santoro [2019] Timothy P Lillicrap and Adam Santoro. Backpropagation through time and the brain. _Current Opinion in Neurobiology_, 55:82-89, 2019. ISSN 0959-4388. doi: https://doi.org/10.1016/j.conb.2019.01.011.\n* Murray et al. [2014] John D Murray, Alberto Bernacchia, David J Freedman, Ranulfo Romo, Jonathan D Wallis, Xinying Cai, Camillo Padoa-Schioppa, Tatiana Pasternak, Hyojung Seo, Daeyeol Lee, et al. A hierarchy of intrinsic timescales across primate cortex. _Nature neuroscience_, 17(12):1661-1663, 2014.\n* Zeraati et al. [2023] Roxana Zeraati, Yan-Liang Shi, Nicholas A Steinmetz, Marc A Gieselmann, Alexander Thiele, Tirin Moore, Anna Levina, and Tatiana A Engel. Intrinsic timescales in the visual cortex change with selective attention and reflect spatial connectivity. _Nature communications_, 14(1):1858, 2023.\n* Huntenburg et al. [2018] Julia M Huntenburg, Pierre-Louis Bazin, and Daniel S Margulies. Large-scale gradients in human cortical organization. _Trends in cognitive sciences_, 22(1):21-31, 2018.\n* Lamme and Roelfsema [2000] Victor AF Lamme and Pieter R Roelfsema. The distinct modes of vision offered by feedforward and recurrent processing. _Trends in neurosciences_, 23(11):571-579, 2000.\n* Bastos et al. [2012] Andre M Bastos, W Martin Usrey, Rick A Adams, George R Mangun, Pascal Fries, and Karl J Friston. Canonical microcircuits for predictive coding. _Neuron_, 76(4):695-711, 2012.\n* Kaleb et al. [2024] Klara Kaleb, Barbara Feulner, Juan Gallego, and Claudia Clopath. Feedback control guides credit assignment in recurrent neural networks. _Advances in Neural Information Processing Systems_, 37:5122-5144, 2024.\n* Lillicrap et al. [2020] Timothy P Lillicrap, Adam Santoro, Luke Marris, Colin J Akerman, and Geoffrey Hinton. Backpropagation and the brain. _Nature Reviews Neuroscience_, 21(6):335-346, 2020.\n* Chollet [2019] Francois Chollet. On the measure of intelligence (abstraction and reasoning corpus), 2019. arXiv preprint arXiv:1911.01547.\n* Chollet et al. [2024] Francois Chollet, Mike Knoop, Gregory Kamradt, and Bryan Landers. Arc prize 2024: Technical report. _ArXiv_, abs/2412.04604, 2024.\n* Chollet et al. [2025] Francois Chollet, Mike Knoop, Gregory Kamradt, Bryan Landers, and Henry Pinkard. Arcagi-2: A new challenge for frontier ai reasoning systems. _arXiv preprint arXiv:2505.11831_, 2025.\n* Buzsaki [2000] Gyorgy Buzsaki. Gamma, alpha, delta, and theta oscillations govern cognitive processes. _International Journal of Psychophysiology_, 39:241-248, 2000.\n* Buzsaki [2006] Gyorgy Buzsaki. _Rhythms of the Brain_. Oxford university press, 2006.\n* Pahor and Jausovec [2014] Anja Pahor and Norbert Jausovec. Theta-gamma cross-frequency coupling relates to the level of human intelligence. _Intelligence_, 46:283-290, 2014.\n* Tort et al. [2009] Adriano BL Tort, Robert W Komorowski, Joseph R Manns, Nancy J Kopell, and Howard Eichenbaum. Theta-gamma coupling increases during the learning of item-context associations. _Proceedings of the National Academy of Sciences_, 106(49):20942-20947, 2009.\n* [34] Benjamin Scellier and Yoshua Bengio. Equilibrium propagation: Bridging the gap between energy-based models and backpropagation. _Frontiers in Computational Neuroscience_, 11, 2016.\n* [35] Guillaume Bellec, Franz Scherr, Anand Subramoney, Elias Hajek, Darjan Salaj, Robert Legenstein, and Wolfgang Maass. A solution to the learning dilemma for recurrent networks of spiking neurons. _Nature Communications_, 11, 07 2020. doi: 10.1038/s41467-020-17236-y.\n* [36] Shaojie Bai, J Zico Kolter, and Vladlen Koltun. Deep equilibrium models. In _Advances in Neural Information Processing Systems_, pages 690-701, 2019.\n* [37] Zhengyang Geng, Xinyu Zhang, Shaojie Bai, Yisen Wang, and Zhouchen Lin. On training implicit models. _ArXiv_, abs/2111.05177, 2021.\n* [38] Katarina Begus and Elizabeth Bonawitz. The rhythm of learning: Theta oscillations as an index of active learning in infancy. _Developmental Cognitive Neuroscience_, 45:100810, 2020. ISSN 1878-9293. doi: https://doi.org/10.1016/j.dcn.2020.100810.\n* [39] Shaojie Bai, Zhengyang Geng, Yash Savani, and J. Zico Kolter. Deep Equilibrium Optical Flow Estimation . In _2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 610-620, 2022.\n* [40] Zaccharie Ramzi, Florian Mannel, Shaojie Bai, Jean-Luc Starck, Philippe Ciuciu, and Thomas Moreau. Shine: Sharing the inverse estimate from the forward pass for bi-level optimization and implicit models. _ArXiv_, abs/2106.00553, 2021.\n* [41] Shaojie Bai, Vladlen Koltun, and J. Zico Kolter. Stabilizing equilibrium models by jacobian regularization. In _International Conference on Machine Learning_, 2021.\n* [42] Daniel Kahneman and P Egan. Thinking, fast and slow (farrar, straus and giroux, new york), 2011.\n* [43] Matthew D Lieberman. Social cognitive neuroscience: a review of core processes. _Annu. Rev. Psychol._, 58(1):259-289, 2007.\n* [44] Randy L Buckner, Jessica R Andrews-Hanna, and Daniel L Schacter. The brain's default network: anatomy, function, and relevance to disease. _Annals of the new York Academy of Sciences_, 1124(1):1-38, 2008.\n* [45] Marcus E Raichle. The brain's default mode network. _Annual review of neuroscience_, 38(1):433-447, 2015.\n* [46] Andrew Westbrook and Todd S Braver. Cognitive effort: A neuroeconomic approach. _Cognitive, Affective, & Behavioral Neuroscience_, 15:395-415, 2015.\n* [47] Richard S. Sutton and Andrew G. Barto. _Reinforcement Learning: An Introduction_. MIT Press, Cambridge, MA, 2018.\n* [48] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and Martin A. Riedmiller. Playing atari with deep reinforcement learning. _ArXiv_, abs/1312.5602, 2013.\n* [49] Matteo Gallici, Mattie Fellows, Benjamin Ellis, Bartomeu Pou, Ivan Masmitja, Jakob Nicolaus Foerster, and Mario Martin. Simplifying deep temporal difference learning, 2025.\n* [50] Shuo Xie and Zhiyuan Li. Implicit bias of adamw: L inf norm constrained optimization. _ArXiv_, abs/2404.04454, 2024.\n* [51] Lucas Prieto, Melih Barsbey, Pedro A. M. Mediano, and Tolga Birdal. Grokking at the edge of numerical stability. In _The Thirteenth International Conference on Learning Representations_, 2025.\n* [52] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In _Advances in neural information processing systems_, pages 5998-6008, 2017.\n* [53] Meta AI. Llama 3: State-of-the-art open weight language models. Technical report, Meta, 2024. URL https://ai.meta.com/llama/.\n* [54] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. _Neurocomputing_, 568:127063, 2024.\n* [55] Noam M. Shazeer. Glu variants improve transformer. _ArXiv_, abs/2002.05202, 2020.\n* [56] Biao Zhang and Rico Sennrich. Root mean square layer normalization. _ArXiv_, abs/1910.07467, 2019.\n* [57] Gunter Klambauer, Thomas Unterthiner, Andreas Mayr, and Sepp Hochreiter. Self-normalizing neural networks. In _Neural Information Processing Systems_, 2017.\n* [58] JAX Developers. _jax.nn.initializers.lecun_normal_. Google Research, 2025. URL https://docs.jax.dev/en/latest/_autosummary/jax.nn.initializers.lecun_normal.html. Accessed June 22, 2025.\n* [59] Yann LeCun, Leon Bottou, Genevieve B Orr, and Klaus-Robert Muller. Efficient backprop. In _Neural networks: Tricks of the trade_, pages 9-50. Springer, 2002.\n* [60] Katie E Everett, Lechao Xiao, Mitchell Wortsman, Alexander A Alemi, Roman Novak, Peter J Liu, Izzeddin Gur, Jascha Sohl-Dickstein, Leslie Pack Kaelbling, Jaehoon Lee, and Jeffrey Pennington. Scaling exponents across parameterizations and optimizers. In _Forty-first International Conference on Machine Learning_, 2024.\n* [61] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization, 2017.\n* [62] Rasmus Berg Palm, Ulrich Paquet, and Ole Winther. Recurrent relational networks. In _Neural Information Processing Systems_, 2017.\n* [63] Jieyi Long. Large language model guided tree-of-thought. _ArXiv_, abs/2305.08291, 2023.\n* [64] Yilun Du, Jiayuan Mao, and Josh Tenenbaum. Learning iterative reasoning through energy diffusion. _ArXiv_, abs/2406.11179, 2024.\n* [65] Kyubyong Park. Can convolutional neural networks crack sudoku puzzles? https://github.com/Kyubyong/sudoku, 2018.\n* [66] Single-digit techniques. https://hodoku.sourceforge.net/en/tech_singles.php. Accessed: 2025-06-16.\n* [67] Tom Dillion. Tdoku: A fast sudoku solver and generator. https://t-dillon.github.io/tdoku/, 2025.\n* [68] Jeffrey Seely, Yuki Imajuku, Tianyu Zhao, Edoardo Cetin, and Llion Jones. Sudoku-bench: Evaluating creative reasoning with sudoku variants. _arXiv preprint arXiv:2505.16135_, 2025.\n* [69] Luke Darlow, Ciaran Regan, Sebastian Risi, Jeffrey Seely, and Llion Jones. Continuous thought machines. _arXiv preprint arXiv:2505.05522_, 2025.\n* Su et al. [2025] DiJia Su, Sainbayar Sukhbaatar, Michael Rabbat, Yuandong Tian, and Qinqing Zheng. Dualformer: Controllable fast and slow thinking by learning with randomized reasoning traces, 2025.\n* Lehnert et al. [2024] Lucas Lehnert, Sainbayar Sukhbaatar, DiJia Su, Qinqing Zheng, Paul McVay, Michael Rabbat, and Yuandong Tian. Beyond a*: Better planning with transformers via search dynamics bootstrapping. In _First Conference on Language Modeling_, 2024.\n* Kapadia et al. [2013] Mubbasir Kapadia, Francisco Garcia, Cory D. Boatright, and Norman I. Badler. Dynamic search on the gpu. In _2013 IEEE/RSJ International Conference on Intelligent Robots and Systems_, pages 3332-3337, 2013. doi: 10.1109/IROS.2013.6696830.\n* Liao and Gu [2025] Isaac Liao and Albert Gu. Arc-agi without pretraining, 2025. URL https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/arc_agi_without_pretraining.html.\n* Posani et al. [2025] Lorenzo Posani, Shuqi Wang, Samuel P Muscinelli, Liam Paninski, and Stefano Fusi. Rarely categorical, always high-dimensional: how the neural code changes along the cortical hierarchy. _bioRxiv_, pages 2024-11, 2025.\n* Rigotti et al. [2013] Mattia Rigotti, Omri Barak, Melissa R. Warden, Xiao-Jing Wang, Nathaniel D. Daw, Earl K. Miller, and Stefano Fusi. The importance of mixed selectivity in complex cognitive tasks. _Nature_, 497:585-590, 2013. doi: 10.1038/nature12160.\n* Mante et al. [2013] Valerio Mante, David Sussillo, Krishna V. Shenoy, and William T. Newsome. Context-dependent computation by recurrent dynamics in prefrontal cortex. _Nature_, 503(7474):78-84, 2013. doi: 10.1038/nature12742.\n* Miller and Cohen [2001] Earl K. Miller and Jonathan D. Cohen. An integrative theory of prefrontal cortex function. _Annual Review of Neuroscience_, 24(1):167-202, 2001. doi: 10.1146/annurev.neuro.24.1.167.\n* Maass [2002] Wolfgang Maass. Real-time computing without stable states: a new framework for neural computation based on perturbations. _Neural Computation_, 14(11):2531-2560, 2002. doi: 10.1162/089976602760407955.\n* Altan et al. [2021] Ege Altan, Sara A. Solla, Lee E. Miller, and Eric J. Perreault. Estimating the dimensionality of the manifold underlying multi-electrode neural recordings. _PLoS Computational Biology_, 17(11):e1008591, 2021. doi: 10.1371/journal.pcbi.1008591.\n* Papyan et al. [2020] Vardan Papyan, X. Y. Han, and David L. Donoho. Prevalence of neural collapse during the terminal phase of deep learning training. _Proceedings of the National Academy of Sciences_, 117(40):24652-24663, 2020. doi: 10.1073/pnas.2015509117.\n* Fang et al. [2021] Cong Fang, Hangfeng He, Qi Long, and Weijie J. Su. Exploring deep neural networks via layer-peeled model: Minority collapse in imbalanced training. _Proceedings of the National Academy of Sciences_, 118(43):e2103091118, 2021. doi: 10.1073/pnas.2103091118.\n* Zhu et al. [2021] Zhihui Zhu, Tianyu Ding, Jinxin Zhou, Xiao Li, Chong You, Jeremias Sulam, and Qing Qu. A geometric analysis of neural collapse with unconstrained features. In _Advances in Neural Information Processing Systems_, volume 34 of _NeurIPS_, pages 29820-29834, 2021.\n* Graves et al. [2014] Alex Graves, Greg Wayne, and Ivo Danihelka. Neural turing machines, 2014.\n* Graves et al. [2016] Alex Graves, Greg Wayne, Malcolm Reynolds, Tim Harley, Ivo Danihelka, Agnieszka Grabska-Barwinska, Sergio Gomez Colmenarejo, Edward Grefenstette, Tiago Ramalho, John Agapiou, et al. Hybrid computing using a neural network with dynamic external memory. _Nature_, 538(7626):471-476, 2016.\n* [85] Lukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In _ICLR_, 2016.\n* [86] Jonas Geiping, Sean McLeish, Neel Jain, John Kirchenbauer, Siddharth Singh, Brian R. Bartoldson, Bhavya Kailkhura, Abhinav Bhatele, and Tom Goldstein. Scaling up test-time compute with latent reasoning: A recurrent depth approach, 2025.\n* [87] Tiedong Liu and Kian Hsiang Low. Goat: Fine-tuned llama outperforms gpt-4 on arithmetic tasks. _ArXiv_, abs/2305.14201, 2023.\n* [88] Alex Graves. Adaptive computation time for recurrent neural networks. _ArXiv_, abs/1603.08983, 2016.\n* [89] Andrea Banino, Jan Balaguer, and Charles Blundell. Pondernet: Learning to ponder. _ArXiv_, abs/2107.05407, 2021.\n* [90] Chris Eliasmith, Terrence C Stewart, Xuan Choo, Trevor Bekolay, Travis DeWolf, Yichuan Tang, and Daniel Rasmussen. A large-scale model of the functioning brain. _science_, 338 (6111):1202-1205, 2012.\n* [91] James CR Whittington, Timothy H Muller, Shirley Mark, Guifen Chen, Caswell Barry, Neil Burgess, and Timothy EJ Behrens. The tolman-eichenbaum machine: unifying space and relational memory through generalization in the hippocampal formation. _Cell_, 183(5):1249-1263, 2020.\n* [92] Lars Buesing, Johannes Bill, Bernhard Nessler, and Wolfgang Maass. Neural dynamics as sampling: a model for stochastic computation in recurrent networks of spiking neurons. _PLoS computational biology_, 7(11):e1002211, 2011.\n* [93] Salah Hihi and Yoshua Bengio. Hierarchical recurrent neural networks for long-term dependencies. In D. Touretzky, M.C. Mozer, and M. Hasselmo, editors, _Advances in Neural Information Processing Systems_, volume 8. MIT Press, 1995.\n* [94] Jan Koutnik, Klaus Greff, Faustino J. Gomez, and Jurgen Schmidhuber. A clockwork rnn. In _International Conference on Machine Learning_, 2014.\n* [95] Mostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob Uszkoreit, and Lukasz Kaiser. Universal transformers, 2018. arXiv preprint arXiv:1807.03819.\n* [96] Yiping Wang, Qing Yang, Zhiyuan Zeng, Liliang Ren, Lucas Liu, Baolin Peng, Hao Cheng, Xuehai He, Kuan Wang, Jianfeng Gao, Weizhu Chen, Shuohang Wang, Simon Shaolei Du, and Yelong Shen. Reinforcement learning for reasoning in large language models with one training example, 2025. URL https://arxiv.org/abs/2504.20571.\n* [97] Niklas Muennighoff. s1: Simple test-time scaling. _arXiv preprint arXiv:2502.23456_, 2025.\n* [98] Liang Wen, Yunke Cai, Fenrui Xiao, Xin He, Qi An, Zhenyu Duan, Yimin Du, Junchen Liu, Lifu Tang, Xiaowei Lv, Haosheng Zou, Yongchao Deng, Shousheng Jia, and Xiangzheng Zhang. Light-r1: Curriculum sft, dpo and rl for long cot from scratch and beyond, 2025.\n* [99] Xuefeng Li, Haoyang Zou, and Pengfei Liu. Limr: Less is more for rl scaling, 2025.\n* [100] Tri Dao and Albert Gu. Transformers are ssms: Generalized models and efficient algorithms through structured state space duality. _ArXiv_, abs/2405.21060, 2024.\n* [101] Han Guo, Songlin Yang, Tarushii Goel, Eric P Xing, Tri Dao, and Yoon Kim. Log-linear attention. _arXiv preprint arXiv:2506.04761_, 2025.", "level": 1, "page_numbers": [], "subsections": [], "references": [], "equations": [], "images": [], "tables": []}], "images": [], "tables": [], "metadata": null, "processing_time": 1505.7261962890625, "error_message": null, "confidence_score": 1.0}, {"parser_name": "PyMuPDF", "success": true, "quality": "excellent", "content": "# Hierarchical Reasoning Model\n\nGuan Wang [1] _[,][†]_, Jin Li [1], Yuhao Sun [1], Xing Chen [1], Changling Liu [1],\nYue Wu [1], Meng Lu [1] _[,][†]_, Sen Song [2] _[,][†]_, Yasin Abbasi Yadkori [1] _[,][†]_\n\n\n1 Sapient Intelligence, Singapore\n\n\n**Abstract**\n\n\nReasoning, the process of devising and executing complex goal-oriented action sequences,\nremains a critical challenge in AI. Current large language models (LLMs) primarily employ\nChain-of-Thought (CoT) techniques, which suffer from brittle task decomposition, extensive\ndata requirements, and high latency. Inspired by the hierarchical and multi-timescale processing in the human brain, we propose the Hierarchical Reasoning Model (HRM), a novel\nrecurrent architecture that attains significant computational depth while maintaining both training stability and efficiency. HRM executes sequential reasoning tasks in a single forward pass\nwithout explicit supervision of the intermediate process, through two interdependent recurrent\nmodules: a high-level module responsible for slow, abstract planning, and a low-level module handling rapid, detailed computations. With only 27 million parameters, HRM achieves\nexceptional performance on complex reasoning tasks using only 1000 training samples. The\nmodel operates without pre-training or CoT data, yet achieves nearly perfect performance on\nchallenging tasks including complex Sudoku puzzles and optimal path finding in large mazes.\nFurthermore, HRM outperforms much larger models with significantly longer context windows\non the Abstraction and Reasoning Corpus (ARC), a key benchmark for measuring artificial\ngeneral intelligence capabilities. These results underscore HRM’s potential as a transformative\nadvancement toward universal computation and general-purpose reasoning systems.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1: **Left:** HRM is inspired by hierarchical processing and temporal separation in the brain. It\nhas two recurrent networks operating at different timescales to collaboratively solve tasks. **Right:**\nWith only about 1000 training examples, the HRM (~27M parameters) surpasses state-of-the-art\nCoT models on inductive benchmarks (ARC-AGI) and challenging symbolic tree-search puzzles\n( _Sudoku-Extreme_, _Maze-Hard_ ) where CoT models failed completely. The HRM was randomly\ninitialized, and it solved the tasks directly from inputs without chain of thoughts.\n\n\n2 Tsinghua University _†_ Corresponding author. Contact: `research@sapient.inc` .\nCode available at: `[github.com/sapientinc/HRM](https://github.com/sapientinc/HRM)`\n\n\n1\n\n\n### **1 Introduction**\n\nDeep learning, as its name suggests, emerged from the idea of stacking more layers to achieve\nincreased representation power and improved performance [1][,][2] . However, despite the remarkable\nsuccess of large language models, their core architecture is paradoxically shallow [3] . This imposes\na fundamental constraint on their most sought-after capability: reasoning. The fixed depth of standard Transformers places them in computational complexity classes such as _AC_ [0] or _TC_ [0][ 4], preventing them from solving problems that require polynomial time [5][,][6] . LLMs are not Turing-complete\nand thus they cannot, at least in a purely end-to-end manner, execute complex algorithmic reasoning that is necessary for deliberate planning or symbolic manipulation tasks [7][,][8] . For example,\nour results on the Sudoku task show that increasing Transformer model depth _can_ improve performance, [1] but performance remains far from optimal even with very deep models (see Figure 2),\nwhich supports the conjectured limitations of the LLM scaling paradigm [9] .\n\n\nThe LLMs literature has relied largely on Chain-of-Thought (CoT) prompting for reasoning [10] .\nCoT externalizes reasoning into token-level language by breaking down complex tasks into simpler intermediate steps, sequentially generating text using a shallow model [11] . However, CoT for\nreasoning is a crutch, not a satisfactory solution. It relies on brittle, human-defined decompositions\nwhere a single misstep or a misorder of the steps can derail the reasoning process entirely [12][,][13] . This\ndependency on explicit linguistic steps tethers reasoning to patterns at the token level. As a result,\nCoT reasoning often requires significant amount of training data and generates a large number of\ntokens for complex reasoning tasks, resulting in slow response times. A more efficient approach is\nneeded to minimize these data requirements [14] .\n\n\nTowards this goal, we explore “latent reasoning”, where the model conducts computations within\nits internal hidden state space [15][,][16] . This aligns with the understanding that language is a tool for\nhuman communication, not the substrate of thought itself [17] ; the brain sustains lengthy, coherent\nchains of reasoning with remarkable efficiency in a latent space, without constant translation back\nto language. However, the power of latent reasoning is still fundamentally constrained by a model’s\n_effective computational depth_ . Naively stacking layers is notoriously difficult due to vanishing gradients, which plague training stability and effectiveness [1][,][18] . Recurrent architectures, a natural alternative for sequential tasks, often suffer from early convergence, rendering subsequent computational steps inert, and rely on the biologically implausible, computationally expensive and memory\nintensive Backpropagation Through Time (BPTT) for training [19] .\n\n\nThe human brain provides a compelling blueprint for achieving the effective computational depth\nthat contemporary artificial models lack. It organizes computation hierarchically across cortical regions operating at different timescales, enabling deep, multi-stage reasoning [20][,][21][,][22] . Recurrent feedback loops iteratively refine internal representations, allowing slow, higher-level areas to\nguide, and fast, lower-level circuits to execute—subordinate processing while preserving global\ncoherence [23][,][24][,][25] . Notably, the brain achieves such depth without incurring the prohibitive creditassignment costs that typically hamper recurrent networks from backpropagation through time [19][,][26] .\n\n\nInspired by this hierarchical and multi-timescale biological architecture, we propose the Hierarchical Reasoning Model (HRM). HRM is designed to significantly increase the effective computational depth. It features two coupled recurrent modules: a high-level (H) module for abstract,\ndeliberate reasoning, and a low-level (L) module for fast, detailed computations. This structure\n\n\n1 Simply increasing the model width does not improve performance here.\n\n\n2\n\n\n|Col1|Scaling Width - 8 layers fixed<br>Scaling Depth - 512 hidden size fixed|Col3|Col4|Col5|Col6|Col7|Col8|\n|---|---|---|---|---|---|---|---|\n|||||||||\n|||||||||\n|||||||||\n|||||||||\n\n\n|Col1|Transformer<br>Recurrent Transformer<br>HRM|Col3|Col4|Col5|Col6|Col7|Col8|Col9|\n|---|---|---|---|---|---|---|---|---|\n||||||||||\n||||||||||\n||||||||||\n||||||||||\n\n\n\nFigure 2: **The necessity of depth for complex reasoning. Left:** On _Sudoku-Extreme Full_, which\nrequire extensive tree-search and backtracking, increasing a Transformer’s width yields no performance gain, while increasing depth is critical. **Right:** Standard architectures saturates, failing to\nbenefit from increased depth. HRM overcomes this fundamental limitation, effectively using its\ncomputational depth to achieve near-perfect accuracy.\n\n\navoids the rapid convergence of standard recurrent models through a process we term “hierarchical convergence.” The slow-updating H-module advances only after the fast-updating L-module\nhas completed multiple computational steps and reached a local equilibrium, at which point the\nL-module is reset to begin a new computational phase.\n\n\nFurthermore, we propose a one-step gradient approximation for training HRM, which offers improved efficiency and eliminates the requirement for BPTT. This design maintains a constant memory footprint ( _O_ (1) compared to BPTT’s _O_ ( _T_ ) for _T_ timesteps) throughout the backpropagation\nprocess, making it scalable and more biologically plausible.\n\n\nLeveraging its enhanced effective depth, HRM excels at tasks that demand extensive search and\nbacktracking. **Using only 1,000 input-output examples, without pre-training or CoT supervi-**\n**sion**, HRM learns to solve problems that are intractable for even the most advanced LLMs. For\nexample, it achieves near-perfect accuracy in complex Sudoku puzzles ( _Sudoku-Extreme Full_ ) and\noptimal pathfinding in 30x30 mazes, where state-of-the-art CoT methods completely fail (0% accuracy). In the Abstraction and Reasoning Corpus (ARC) AGI Challenge [27][,][28][,][29] - a benchmark\nof inductive reasoning - HRM, trained from scratch with only the official dataset (~1000 examples), with only 27M parameters and a 30x30 grid context (900 tokens), achieves a performance\nof **40.3%**, which substantially surpasses leading CoT-based models like o3-mini-high (34.5%)\nand Claude 3.7 8K context (21.2%), despite their considerably larger parameter sizes and context lengths, as shown in Figure 1. This represents a promising direction toward the development\nof next-generation AI reasoning systems with universal computational capabilities.\n\n### **2 Hierarchical Reasoning Model**\n\n\nWe present the HRM, inspired by three fundamental principles of neural computation observed in\nthe brain:\n\n\n- **Hierarchical processing:** The brain processes information across a hierarchy of cortical areas. Higher-level areas integrate information over longer timescales and form abstract representations, while lower-level areas handle more immediate, detailed sensory and motor processing [20][,][22][,][21] .\n\n\n3\n\n\n- **Temporal Separation:** These hierarchical levels in the brain operate at distinct intrinsic timescales,\nreflected in neural rhythms (e.g., slow theta waves, 4–8 Hz and fast gamma waves, 30–100\nHz) [30][,][31] . This separation allows for stable, high-level guidance of rapid, low-level computations [32][,][33] .\n\n\n- **Recurrent Connectivity:** The brain features extensive recurrent connections. These feedback\nloops enable iterative refinement, yielding more accurate and context-sensitive representations\nat the cost of additional processing time. Additionally, the brain largely avoids the problematic\ndeep credit assignment problem associated with BPTT [19] .\n\n\nThe HRM model consists of four learnable components: an input network _f_ _I_ ( _·_ ; _θ_ _I_ ), a low-level recurrent module _f_ _L_ ( _·_ ; _θ_ _L_ ), a high-level recurrent module _f_ _H_ ( _·_ ; _θ_ _H_ ), and an output network _f_ _O_ ( _·_ ; _θ_ _O_ ).\nThe model’s dynamics unfold over _N_ high-level cycles of _T_ low-level timesteps each [2] . We index\nthe total timesteps of one forward pass by _i_ = 1 _, . . ., N × T_ . The modules _f_ _L_ and _f_ _H_ each keep a\nhidden state— _z_ _L_ _[i]_ [for] _[ f]_ _[L]_ [ and] _[ z]_ _H_ _[i]_ [for] _[ f]_ _[H]_ [—which are initialized with the vectors] _[ z]_ _L_ [0] [and] _[ z]_ _H_ [0] [, respec-]\ntively.\n\n\nThe HRM maps an input vector _x_ to an output prediction vector ˆ _y_ as follows. First, the input _x_ is\nprojected into a working representation ˜ _x_ by the input network:\n\n\n˜\n_x_ = _f_ _I_ ( _x_ ; _θ_ _I_ ) _._\n\n\nAt each timestep _i_, the L-module updates its state conditioned on its own previous state, the Hmodule’s current state (which remains fixed throughout the cycle), and the input representation.\nThe H-module only updates once per cycle (i.e., every _T_ timesteps) using the L-module’s final\nstate at the end of that cycle:\n\n\n_z_ _L_ _[i]_ [=] _[ f]_ _[L]_ � _z_ _L_ _[i][−]_ [1] _[, z]_ _H_ _[i][−]_ [1] _[,]_ [ ˜] _[x]_ [;] _[ θ]_ _[L]_ � _,_\n\n\n\n_z_ _H_ _[i]_ [=]\n\n\n\n_f_ _H_ � _z_ _H_ _[i][−]_ [1] _[, z]_ _L_ _[i][−]_ [1] [;] _[ θ]_ _[H]_ � if _i ≡_ 0 (mod _T_ ) _,_\n_z_ _[i][−]_ [1] otherwise _._\n� _H_\n\n\n\nFinally, after _N_ full cycles, a prediction ˆ _y_ is extracted from the hidden state of the H-module:\n\n\nˆ\n_y_ = _f_ _O_ ( _z_ _H_ _[NT]_ [;] _[ θ]_ _[O]_ [)] _[ .]_\n\n\nThis entire _NT_ -timestep process represents a single forward pass of the HRM. A halting mechanism (detailed later in this section) determines whether the model should terminate, in which case\n\nˆ\n_y_ will be used as the final prediction, or continue with an additional forward pass.\n\n\n**Hierarchical convergence** Although convergence is crucial for recurrent networks, standard RNNs\nare fundamentally limited by their tendency to converge too early. As the hidden state settles toward\na fixed point, update magnitudes shrink, effectively stalling subsequent computation and capping\nthe network’s effective depth. To preserve computational power, we actually want convergence to\nproceed very slowly–but engineering that gradual approach is difficult, since pushing convergence\ntoo far edges the system toward instability.\n\n\n2 While inspired by temporal separation in the brain, our model’s “high-level” and “low-level” modules are conceptual abstractions and do not map directly to specific neural oscillation frequencies.\n\n\n4\n\n\nFigure 3: Comparison of forward residuals and PCA trajectories. HRM shows hierarchical convergence: the H-module steadily converges, while the L-module repeatedly converges within cycles\nbefore being reset by H, resulting in residual spikes. The recurrent neural network exhibits rapid\nconvergence with residuals quickly approaching zero. In contrast, the deep neural network experiences vanishing gradients, with significant residuals primarily in the initial (input) and final layers.\n\n\nHRM is explicitly designed to counteract this premature convergence through a process we term\n_hierarchical convergence_ . During each cycle, the L-module (an RNN) exhibits stable convergence\nto a _local equilibrium_ . This equilibrium, however, depends on the high-level state _z_ _H_ supplied\nduring that cycle. After completing the _T_ steps, the H-module incorporates the sub-computation’s\noutcome (the final state _z_ _L_ ) and performs its own update. This _z_ _H_ update establishes a fresh context\nfor the L-module, essentially “restarting” its computational path and initiating a new convergence\nphase toward a different local equilibrium.\n\n\nThis process allows the HRM to perform a sequence of distinct, stable, nested computations, where\nthe H-module directs the overall problem-solving strategy and the L-module executes the intensive\nsearch or refinement required for each step. Although a standard RNN may approach convergence\nwithin _T_ iterations, the hierarchical convergence benefits from an enhanced effective depth of _NT_\nsteps. As empirically shown in Figure 3, this mechanism allows HRM both to maintain high\ncomputational activity (forward residual) over many steps (in contrast to a standard RNN, whose\nactivity rapidly decays) and to enjoy stable convergence. This translates into better performance at\nany computation depth, as illustrated in Figure 2.\n\n\n**Approximate gradient** Recurrent models typically use BPTT to compute gradients. However,\nBPTT requires storing the hidden states from the forward pass and then combining them with\ngradients during the backward pass, which demands _O_ ( _T_ ) memory for T timesteps. This heavy\nmemory burden forces smaller batch sizes and leads to poor GPU utilization, especially for largescale networks. Additionally, because retaining the full history trace through time is biologically\nimplausible, it is unlikely that the brain implements BPTT [19] .\n\n\nFortunately, if a recurrent neural network converges to a fixed point, we can avoid unrolling its state\nsequence by applying backpropagation in a single step at that equilibrium point. Moreover, such a\nmechanism could plausibly be implemented in the brain using only local learning rules [34][,][35] . Based\n\n\n5\n\n\non this finding, we propose a one-step approximation of the HRM gradient–using the gradient of\nthe last state of each module and treating other states as constant. The gradient path is, therefore,\n\n\nOutput head → final state of the H-module → final state of the L-module → input embedding\n\n\nThe above method needs _O_ (1) memory, does not require unrolling through time, and can be easily\nimplemented with an autograd framework such as PyTorch, as shown in Figure 4. Given that\neach module only needs to back-propagate errors through its most recent local synaptic activity,\nthis approach aligns well with the perspective that cortical credit assignment relies on short-range,\ntemporally local mechanisms rather than on a global replay of activity patterns.\n\n\n\nThe one-step gradient approximation is theoretically\ngrounded in the mathematics of Deep Equilibrium Models (DEQ) [36] which employs the Implicit Function Theorem (IFT) to bypass BPTT, as detailed next. Consider an\nidealized HRM behavior where, during high-level cycle\n_k_, the L-module repeatedly updates until its state _z_ _L_ converges to a local fixed point _z_ _L_ _[⋆]_ [. This fixed point, given]\nthe current high-level state _z_ _H_ _[k][−]_ [1] [, can be expressed as]\n\n\n_z_ _L_ _[⋆]_ [=] _[ f]_ _[L]_ [(] _[z]_ _L_ _[⋆]_ _[, z]_ _H_ _[k][−]_ [1] _[,]_ [ ˜] _[x]_ [;] _[ θ]_ _[L]_ [)] _[ .]_\n\n\nThe H-module then performs a single update using this\nconverged L-state:\n\n\n_z_ _H_ _[k]_ [=] _[ f]_ _[H]_ [(] _[z]_ _H_ _[k][−]_ [1] _[, z]_ _L_ _[⋆]_ [;] _[ θ]_ _[H]_ [)] _[ .]_\n\n\nWith a proper mapping _F_, the updates to the high-level\nstate can be written in a more compact form as _z_ _H_ _[k]_ [=]\n_F_ ( _z_ _H_ _[k][−]_ [1] [; ˜] _[x, θ]_ [)][, where] _[ θ]_ [ = (] _[θ]_ _[I]_ _[, θ]_ _[L]_ [)][, and the fixed-point]\n_∂F_\ncan be written as _z_ _H_ _[⋆]_ [=] _[ F]_ [(] _[z]_ _H_ _[⋆]_ [; ˜] _[x, θ]_ [)][. Let] _[ J]_ _[F]_ [ =] _∂z_ _H_ [be]\nthe Jacobian of _F_, and assume that the matrix _I −_ _J_ _F_ is\ninvertible at _z_ _H_ _[⋆]_ [and that the mapping] _[ F]_ [ is continuously]\ndifferentiable. The Implicit Function Theorem then allows us to calculate the exact gradient of fixed point _z_ _H_ _[⋆]_\nwith respect to the parameters _θ_ without explicit backpropagation:\n\n\n```\ndef hrm(z, x, N=2, T=2):\n  x = input_embedding(x)\n  zH, zL = z\n\n  with torch.no_grad():\n    for _i in range(N ∗T −1):\n      zL = L_net(zL, zH, x)\n      if (_i + 1) % T == 0:\n        zH = H_net(zH, zL)\n\n  # 1 − step grad\n  zL = L_net(zL, zH, x)\n  zH = H_net(zH, zL)\n  return (zH, zL), output_head(zH)\n\n# Deep Supervision\nfor x, y_true in train_dataloader:\n  z = z_init\n  for step in range(N_supervision):\n    z, y_hat = hrm(z, x)\n\n    loss = softmax_cross_entropy(y_hat, y_true)\n    z = z.detach()\n\n    loss.backward()\n    opt.step()\n    opt.zero_grad()\n\n```\n\nFigure 4: **Top:** Diagram of HRM with\napproximate gradient. **Bottom:** Pseudocode of HRM with deep supervision\ntraining in PyTorch.\n\n\n\n_._ (1)\n���� _z_ _H_ _[⋆]_\n\n\n\n_∂z∂θ_ _H_ _[⋆]_ [=] � _I −_ _J_ _F_ �� _z_ _H_ _[⋆]_\n\n\n\n� _−_ 1 _∂∂θF_\n\n\n\nCalculating the above gradient requires evaluating and inverting matrix ( _I −_ _J_ _F_ ) that can be computationally expensive. Given the Neumann series expansion,\n\n\n( _I −_ _J_ _F_ ) _[−]_ [1] = _I_ + _J_ _F_ + _J_ _F_ [2] [+] _[ J]_ _F_ [3] [+] _[ . . .,]_\n\n\nthe so-called _1-step gradient_ [37] approximates the series by considering only its first term, i.e. ( _I −_\n_J_ _F_ ) _[−]_ [1] _≈_ _I_, and leads to the following approximation of Equation (1):\n\n\n\n_∂z_ _H_ _[∗]_\n_≈_ _[∂][f]_ _[H]_\n_∂θ_ _H_ _∂θ_ _H_\n\n\n\n_∂z_ _H_ _[∗]_\n\n_[∂]_ _∂θ_ _[f]_ _H_ _[H]_ _,_ _∂θ_ _L_ _≈_ _[∂]_ _∂z_ _[f]_ _[H][∗]_\n\n\n\n\n_[∂]_ _∂z_ _[f]_ _[H]_ _L_ _[∗]_ _·_ _∂θ_ _[∂z]_ _LL_ _[∗]_ _,_ _∂z∂θ_ _H_ _[∗]_ _I_ _≈_ _[∂]_ _∂z_ _[f]_ _[H]_ _L_ _[∗]_\n\n\n\n\n_[∂][f]_ _[H]_ _·_ _[∂z]_ _L_ _[∗]_ _._ (2)\n\n_∂z_ _L_ _[∗]_ _∂θ_ _I_\n\n\n\n6\n\n\nThe gradients of the low-level fixed point, _∂θ∂z_ _LL_ _[∗]_ [and] _∂z∂θ_ _L_ _[∗]_ _I_ [, can also be approximated using another]\napplication of the 1-step gradient:\n\n\n\n_∂z_ _L_ _[∗]_\n_≈_ _[∂][f]_ _[L]_\n_∂θ_ _L_ _∂θ_ _L_\n\n\n\n_∂z_ _L_ _[∗]_\n\n_[∂]_ _∂θ_ _[f]_ _L_ _[L]_ _,_ _∂θ_ _I_ _≈_ _[∂]_ _∂θ_ _[f]_ _[L]_ _I_\n\n\n\n_._ (3)\n_∂θ_ _I_\n\n\n\nBy substituting Equation (3) back into Equation (2), we arrive at the final simplified gradients.\n\n\nBefore defining our loss function, we must first introduce two key elements of our proposed\nmethod: _deep supervision_ and _adaptive computational time_ .\n\n\n**Deep supervision** Inspired by the principle that periodic neural oscillations regulate when learning\noccurs in the brain [38], we incorporate a deep supervision mechanism into HRM, as detailed next.\n\n\nGiven a data sample ( _x, y_ ), we run multiple forward passes of the HRM model, each of which we\nrefer to as a _segment_ . Let _M_ denote the total number of segments executed before termination.\nFor each segment _m ∈{_ 1 _, . . ., M_ _}_, let _z_ _[m]_ = ( _z_ _H_ _[mNT]_ _, z_ _L_ _[mNT]_ ) represent the hidden state at the\nconclusion of segment _m_, encompassing both high-level and low-level state components.\n\n\nAt each segment _m_, we apply a deep supervision step as follows:\n\n\n1. Given the state _z_ _[m][−]_ [1] from the previous segment, compute the next state _z_ _[m]_ and its associated\noutput ˆ _y_ _[m]_ through a forward pass in the HRM model:\n\n\n( _z_ _[m]_ _,_ ˆ _y_ _[m]_ ) _←_ HRM( _z_ _[m][−]_ [1] _, x_ ; _θ_ )\n\n\n2. Compute the loss for the current segment:\n\n\n_L_ _[m]_ _←_ L OSS (ˆ _y_ _[m]_ _, y_ )\n\n\n3. Update parameters:\n\n\n_θ ←_ O PTIMIZER S TEP ( _θ, ∇_ _θ_ _L_ _[m]_ )\n\n\nThe crucial aspect of this procedure is that the hidden state _z_ _[m]_ is “detached” from the computation graph before being used as the input state for the next segment. Consequently, gradients from\nsegment _m_ + 1 do not propagate back through segment _m_, effectively creating a 1-step approximation of the gradient of the recursive deep supervision process [39][,][40] . This approach provides more\nfrequent feedback to the H-module and serves as a regularization mechanism, demonstrating superior empirical performance and enhanced stability in deep equilibrium models when compared to\nmore complex, Jacobian-based regularization techniques [39][,][41] . Figure 4 shows pseudocode of deep\nsupervision training.\n\n\n**Adaptive computational time (ACT)** The brain dynamically alternates between automatic thinking (“System 1”) and deliberate reasoning (“System 2”) [42] . Neuroscientific evidence shows that\nthese cognitive modes share overlapping neural circuits, particularly within regions such as the\nprefrontal cortex and the default mode network [43][,][44] . This indicates that the brain dynamically modulates the “runtime” of these circuits according to task complexity and potential rewards [45][,][46] .\n\n\nInspired by the above mechanism, we incorporate an adaptive halting strategy into HRM that enables “thinking, fast and slow”. This integration leverages deep supervision and uses the Q-learning\n\n\n7\n\n\nalgorithm [47] to adaptively determine the number of segments. A Q-head uses the final state of the\nH-module to predict the Q-values _Q_ [ˆ] _[m]_ = ( _Q_ [ˆ] _[m]_ halt _[,]_ [ ˆ] _[Q]_ continue _[m]_ [)][ of the “halt” and “continue” actions:]\n\n\nˆ\n_Q_ _[m]_ = _σ_ ( _θ_ _Q_ _[⊤]_ _[z]_ _H_ _[mNT]_ ) _,_\n\n\nwhere _σ_ denotes the sigmoid function applied element-wise. The halt or continue action is chosen\nusing a randomized strategy as detailed next. Let _M_ max denote the maximum number of segments\n(a fixed hyperparameter) and _M_ min denote the minimum number of segments (a random variable).\nThe value of _M_ min is determined stochastically: with probability _ε_, it is sampled uniformly from the\nset _{_ 2 _, · · ·, M_ max _}_ (to encourage longer thinking), and with probability 1 _−_ _ε_, it is set to 1. The halt\naction is selected under two conditions: when the segment count surpasses the maximum threshold\n_M_ max, or when the estimated halt value _Q_ [ˆ] halt exceeds the estimated continue value _Q_ [ˆ] continue and the\nsegment count has reached at least the minimum threshold _M_ min .\n\n\nThe Q-head is updated through a Q-learning algorithm, which is defined on the following episodic\nMarkov Decision Process (MDP). The state of the MDP at segment _m_ is _z_ _[m]_, and the action space\nis _{_ halt _,_ continue _}_ . Choosing the action “halt” terminates the episode and returns a binary reward\n\nˆ\nindicating prediction correctness, i.e., **1** _{y_ _[m]_ = _y}_ . Choosing “continue” yields a reward of 0 and\nthe state transitions to _z_ _[m]_ [+1] . Thus, the Q-learning targets for the two actions _G_ [ˆ] _[m]_ = ( _G_ [ˆ] _[m]_ halt _[,]_ [ ˆ] _[G]_ _[m]_ continue [)]\nare given by\n\n\n_G_ ˆ _[m]_ halt [=] **[ 1]** _[{][y]_ [ˆ] _[m]_ [ =] _[ y][}][,]_\n\n\n\n_G_ ˆ _[m]_ continue [=]\n\n\n\n\n\n\n\n\n\n\nˆ\n_Q_ _[m]_ halt [+1] _[,]_ if _m ≥_ _N_ max _,_\n\n\n\nmax( _Q_ [ˆ] _[m]_ halt [+1] _[,]_ [ ˆ] _[Q]_ _[m]_ continue [+1] [)] _[,]_ otherwise _._\n\n\n\nWe can now define the loss function of our learning procedure. The overall loss for each supervision\nsegment combines both the Q-head loss and the sequence-to-sequence loss:\n\n\n_L_ _[m]_ ACT [=][ L] [OSS] [(ˆ] _[y]_ _[m]_ _[, y]_ [) +][ B] [INARY] [C] [ROSS] [E] [NTROPY] [( ˆ] _[Q]_ _[m]_ _[,]_ [ ˆ] _[G]_ _[m]_ [)] _[ .]_\n\n\nMinimizing the above loss enables both accurate predictions and nearly optimal stopping decisions.\n\n\nSelecting the “halt” action ends the supervision loop. In practice, sequences are processed in\nbatches, which can be easily handled by substituting any halted sample in the batch with a fresh\nsample from the dataloader.\n\n\nFigure 5 presents a performance comparison between two HRM variants: one incorporating ACT\nand another employing a fixed computational step count equivalent to ACT’s _M_ max parameter. It\nshows that ACT effectively adapts its computational resources based on task complexity, achieving\nsignificant computational savings with minimal impact on performance.\n\n\n**Inference-time scaling** An effective neural model should exploit additional computational resources during inference to enhance performance. As illustrated in Figure 5-(c), HRM seamlessly\nachieves inference-time scaling by simply increasing the computational limit parameter, _M_ max\nwithout requiring further training or architectural modifications.\n\n\nAdditional compute is especially effective for tasks that demand deeper reasoning. On Sudoku—\na problem that often requires long-term planning—HRM exhibits strong inference-time scaling.\nOn the other hand, we find that extra computational resources yield minimal gains in ARC-AGI\nchallenge, as solutions generally require only a few transformations.\n\n\n8\n\n\n|7 8|(a) ACT Compute Spent Fixed M ACT ( limit)|\n|---|---|\n|3<br>4<br>5<br>6<br>|ACT (Mmax limit)|\n|2<br>4<br>8<br>1<br>2|2<br>4<br>8<br>1<br>2|\n\n\n\nFigure 5: **Effectiveness of Adaptive Computation Time (ACT)** on the _Sudoku-Extreme-Full_ . **(a)**\nMean compute steps used by models with ACT versus models with a fixed number of compute steps\n( _M_ ). ACT maintains a low and stable number of average compute steps even as the maximum limit\n( _M_ max ) increases. **(b)** Accuracy comparison. The ACT model achieves performance comparable\nto the fixed-compute model while utilizing substantially fewer computational steps on average. **(c)**\nInference-time scalability. Models trained with a specific _M_ max can generalize to higher computational limits during inference, leading to improved accuracy. For example, a model trained with\n_M_ max = 8 continues to see accuracy gains when run with _M_ max = 16 during inference.\n\n\n\n**Stability of Q-learning in ACT** The deep Q-learning that underpins our ACT mechanism is\nknown to be prone to instability, often requiring stabilization techniques such as replay buffers\nand target networks [48], which are absent in our design. Our approach, however, achieves stability\nthrough the intrinsic properties of our model and training procedure. Recent theoretical work by\nGallici et al. [49] shows that Q-learning can achieve convergence if network parameters are bounded,\nweight decay is incorporated during training, and post-normalization layers are implemented. Our\nmodel satisfies these conditions through its Post-Norm architecture that employs RMSNorm (a\nlayer normalization variant) and the AdamW optimizer. AdamW has been shown to solve an _L_ _∞_ constrained optimization problem, ensuring that model parameters remain bounded by 1 _/λ_ [50] .\n\n\n**Architectural details** We employ a sequence-to-sequence architecture for HRM. Both input and\noutput are represented as token sequences: _x_ = ( _x_ 1 _, . . ., x_ _l_ ) and _y_ = ( _y_ 1 _, . . ., y_ _l_ _′_ ) respectively.\nThe model includes an embedding layer _f_ _I_ that converts discrete tokens into vector representations, and an output head _f_ _O_ ( _z_ ; _θ_ _O_ ) = softmax( _θ_ _O_ _z_ ) that transforms hidden states into token probability distributions ˆ _y_ . For small-sample experiments, we replace softmax with stablemax [51] to\nimprove generalization performance. The sequence-to-sequence loss is averaged over all tokens,\n\n_l_ _′_\n\nL OSS (ˆ _y, y_ ) = [1] _[′]_ � _i_ =1 [log] _[ p]_ [(] _[y]_ _[i]_ [)][, where] _[ p]_ [(] _[y]_ _[i]_ [)][ is the probability that distribution][ ˆ] _[y]_ _[i]_ [ assigns to token]\n\n\n\n_l_ [1] _[′]_ � _li_ _′_\n\n\n\nL OSS (ˆ _y, y_ ) = _l_ [1] _[′]_ � _i_ =1 [log] _[ p]_ [(] _[y]_ _[i]_ [)][, where] _[ p]_ [(] _[y]_ _[i]_ [)][ is the probability that distribution][ ˆ] _[y]_ _[i]_ [ assigns to token]\n\n_y_ _i_ . The initial hidden states _z_ [0] are initialized by sampling from a truncated normal distribution with\nstandard deviation of 1, truncation of 2, and kept fixed throughout training.\n\n\nBoth the low-level and high-level recurrent modules _f_ _L_ and _f_ _H_ are implemented using encoderonly Transformer [52] blocks with identical architectures and dimensions. These modules take multiple inputs, and we use straightforward element-wise addition to combine them, though more\nsophisticated merging techniques such as gating mechanisms could potentially improve performance and is left for future work. For all Transformer blocks in this work—including those in\nthe baseline models—we incorporate the enhancements found in modern LLMs (based on Llama [53]\n\narchitectures). These improvements include Rotary Positional Encoding [54], Gated Linear Units [55],\nRMSNorm [56], and the removal of bias terms from linear layers.\n\n\nFurthermore, both HRM and recurrent Transformer models implement a Post-Norm architecture\n\n\n\n9\n\n\n|Col1|8|4|Col4|Col5|5|Col7|6|Col9|\n|---|---|---|---|---|---|---|---|---|\n|||||8||7|||\n|3||||4|||||\n||3|8|4||||2||\n|||6|||3|||8|\n|9||||||||6|\n||||5||||||\n||||||2|||1|\n||2|5||3|||8||\n\n\n|8|4|1|2|5|9|6|\n|---|---|---|---|---|---|---|\n|6|1|3|8|9|7|4|\n|5|9|6|4|7|8|1|\n|3|8|4|9|6|1|2|\n|1|6|2|7|3|5|9|\n|7|2|8|5|1|4|3|\n|9|3|5|1|8|2|7|\n|4|7|9|6|2|3|5|\n\n\n\n(a) ARC-AGI\n\n\n\n1 2 5 7 3 4 6 8 9\n\n\n(b) Sudoku-Hard (c) Maze navigation (d) _Sudoku-Extreme_ subset difficulty\n\n\n\nFigure 6: **Left:** Visualization of benchmark tasks. **Right:** Difficulty of _Sudoku-Extreme_ examples.\n\n\nwith weights initialized via truncated LeCun Normal initialization [57][,][58][,][59], while the scale and bias\nparameters are excluded from RMSNorm. All parameters are optimized using the Adam-atan2 optimizer [60], a scale-invariant variant of Adam [61], combined with a constant learning rate that includes\nlinear warm-up.\n\n### **3 Results**\n\n\nThis section begins by describing the ARC-AGI, Sudoku, and Maze benchmarks, followed by an\noverview of the baseline models and their results. Figure 6-(a,b,c) presents a visual representation of the three benchmark tasks, which are selected to evaluate various reasoning abilities in AI\nmodels.\n\n#### **3.1 Benchmarks**\n\n\n**ARC-AGI Challenge** The ARC-AGI benchmark evaluates general fluid intelligence through IQtest-like puzzles that require inductive reasoning [27] . The initial version, ARC-AGI-1, presents challenges as input-output grid pairs that force AI systems to extract and generalize abstract rules from\njust a few examples. Each task provides a few input–output example pairs (usually 2–3) and a test\ninput. An AI model has two attempts to produce the correct output grid. Although some believe\nthat mastering ARC-AGI would signal true artificial general intelligence, its primary purpose is\nto expose the current roadblocks in AGI progress. In fact, both conventional deep learning methods and CoT techniques have faced significant challenges with ARC-AGI-1, primarily because it\nrequires the ability to generalize to entirely new tasks [28] .\n\n\nAddressing the limitations identified in ARC-AGI-1, ARC-AGI-2 significantly expands the benchmark by providing a more comprehensive and carefully refined collection of tasks. These new\ntasks emphasize deeper compositional reasoning, multi-step logic, contextual rule application, and\nsymbolic abstraction. Human calibration studies show these tasks are challenging but doable for\npeople, while being much harder for current AI systems, offering a clearer measure of general\nreasoning abilities [29] .\n\n\n10\n\n\n**Sudoku-Extreme** Sudoku is a 9 _×_ 9 logic puzzle, requiring each row, column, and 3 _×_ 3 block to\ncontain the digits 1–9 exactly once. A prediction is considered correct if it exactly matches the\npuzzle’s unique solution. Sudoku’s complex logical structure makes it a popular benchmark for\nevaluating logical reasoning in machine learning [62][,][63][,][64] .\n\n\nThe most frequently used Sudoku dataset in research, namely the Kaggle dataset [65], can be fully\nsolved using elementary single-digit techniques [66] . The minimal 17-clue puzzles [62], another widelyused collection, might seem more challenging due to its small number of clues. However, this\nperception is misleading—since 17 represents the minimum number of clues required to guarantee\na unique Sudoku solution, these hints need to be highly orthogonal to each other. This orthogonal\narrangement leads to many direct, easily-resolved solution paths [67] .\n\n\nWe introduce _Sudoku-Extreme_, a more challenging dataset that is compiled from the aforementioned easy datasets as well as puzzles recognized by the Sudoku community as exceptionally\ndifficult for human players:\n\n\n- Easy puzzles compiled from Kaggle, 17-clue, plus unbiased samples from the Sudoku puzzle\ndistribution [67] : totaling 1 149 158 puzzles.\n\n\n- Challenging puzzles compiled from Magictour 1465, Forum-Hard and Forum-Extreme subsets:\ntotaling 3 104 157 puzzles.\n\n\nThe compiled data then undergo a strict 90/10 train-test split, ensuring that the test set puzzles\ncannot be derived through equivalent transformations of any training samples. _Sudoku-Extreme_ is\na down-sampled subset of this data containing 1000 training examples. We use _Sudoku-Extreme_ in\nour main experiments (Figure 1), which focuses on small-sample learning scenarios. To guarantee\nconvergence and control overfitting effects in our analysis experiments (Figures 2, 3 and 5), we use\nthe complete training data, _Sudoku-Extreme-Full_, containing 3 831 994 examples.\n\n\nWe measure puzzle difficulty by counting the number of search backtracks (“guesses”) required\nby a smart Sudoku solver program _tdoku_, which uses propositional logic to reduce the number of\nguesses [67] . Our _Sudoku-Extreme_ dataset exhibits a mean difficulty of 22 backtracks per puzzle, significantly higher than existing datasets, including recent handmade puzzles Sudoku-Bench [68] which\naverage just 0 _._ 45 backtracks per puzzle. These subset complexity levels are shown in Figure 6-(d).\n\n\n**Maze-Hard** This task involves finding the optimal path in a 30 _×_ 30 maze, making it interpretable\nand frequently used for training LLMs in search tasks [69][,][70][,][71] . We adopt the instance generation\nprocedure of Lehnert et al. [71], but introduce an additional filter to retain only those instances whose\ndifficulty exceeds 110. Here, “difficulty” is defined as the length of the shortest path, which aligns\nwith the linear time complexity of the wavefront breadth-first search algorithm on GPUs [72] . A path\nis considered correct if it is valid and optimal—that is, the shortest route from the start to the goal.\nThe training and test set both include 1000 examples.\n\n#### **3.2 Evaluation Details**\n\n\nFor all benchmarks, HRM models were initialized with random weights and trained in the sequenceto-sequence setup using the input-output pairs. The two-dimensional input and output grids were\nflattened and then padded to the maximum sequence length. The resulting performance is shown in\nFigure 1. Remarkably, HRM attains these results with just ~1000 training examples per task—and\n**without pretraining or CoT labels** .\n\n\n11\n\n\nFor ARC-AGI challenge, we start with all input-output example pairs in the training and the evaluation sets. The dataset is augmented by applying translations, rotations, flips, and color permutations\nto the puzzles. Each task example is prepended with a learnable special token that represents the\npuzzle it belongs to. At test time, we proceed as follows for each test input in the evaluation set: (1)\nGenerate and solve 1000 augmented variants and, for each, apply the inverse-augmentation transform to obtain a prediction. (2) Choose the two most popular predictions as the final outputs. [3] All\nresults are reported on the evaluation set.\n\n\nWe augment Sudoku puzzles by applying band and digit permutations, while data augmentation is\ndisabled for Maze tasks. Both tasks undergo only a single inference pass.\n\n\nFor ARC-AGI, the scores of the CoT models are taken from the official leaderboard [29], while for\nSudoku and Maze, the scores are obtained by evaluating through the corresponding API.\n\n\nIn Figure 1, the baselines are grouped based on whether they are pre-trained and use CoT, or neither.\nThe “Direct pred” baseline means using “direct prediction without CoT and pre-training”, which\nretains the exact training setup of HRM but swaps in a Transformer architecture. Interestingly, on\nARC-AGI-1, “Direct pred” matches the performance of Liao and Gu [73], who built a carefully designed, domain-specific equivariant network for learning the ARC-AGI task from scratch, without\npre-training. By substituting the Transformer architecture with HRM’s hierarchical framework and\nimplementing ACT, we achieve more than a twofold performance improvement.\n\n\nOn the _Sudoku-Extreme_ and _Maze-Hard_ benchmarks, the performance gap between HRM and the\nbaseline methods is significant, as the baselines almost never manage to solve the tasks. These\nbenchmarks that demand lengthy reasoning traces are particularly difficult for CoT-based methods.\nWith only 1000 training examples, the “Direct pred” baseline—which employs an 8-layer Transformer identical in size to HRM—fails entirely on these challenging reasoning problems. When\ntrained on the larger _Sudoku-Extreme-Full_ dataset, however, “Direct pred” can solve some easy\nSudoku puzzles and reaches 16 _._ 9% accuracy (see Figure 2). Lehnert et al. [71] showed that a large\nvanilla Transformer model with 175M parameters, trained on 1 million examples across multiple\ntrials, achieved only marginal success on 30x30 Maze tasks, with accuracy below 20% using the\n_pass_ @64 evaluation metric.\n\n#### **3.3 Visualization of intermediate timesteps**\n\n\nAlthough HRM demonstrates strong performance on complex reasoning tasks, it raises an intriguing question: what underlying reasoning algorithms does the HRM neural network actually implement? Addressing this question is important for enhancing model interpretability and developing a\ndeeper understanding of the HRM solution space.\n\n\nWhile a definitive answer lies beyond our current scope, we begin our investigation by analyzing\nstate trajectories and their corresponding solution evolution. More specifically, at each timestep\n_i_ and given the low-level and high-level state pair ( _z_ _L_ _[i]_ [and] _[ z]_ _H_ _[i]_ [) we perform a preliminary forward]\npass through the H-module to obtain ¯ _z_ _[i]_ = _f_ _H_ ( _z_ _H_ _[i]_ _[, z]_ _L_ _[i]_ [;] _[ θ]_ _[H]_ [)][ and its corresponding decoded prediction]\n_y_ ¯ _[i]_ = _f_ _O_ (¯ _z_ _[i]_ ; _θ_ _O_ ). The prediction ¯ _y_ _[i]_ is then visualized in Figure 7.\n\n\nIn the Maze task, HRM appears to initially explore several potential paths simultaneously, subsequently eliminating blocked or inefficient routes, then constructing a preliminary solution outline\nfollowed by multiple refinement iterations. In Sudoku, the strategy resembles a depth-first search\n\n\n3 The ARC-AGI allows two attempts for each test input.\n\n\n12\n\n\nFigure 7: **Visualization of intermediate predictions by HRM on benchmark tasks. Top:** _Maze-_\n_Hard_ —blue cells indicate the predicted path. **Middle:** _Sudoku-Extreme_ —bold cells represent initial givens; red highlights cells violating Sudoku constraints; grey shading indicates changes from\nthe previous timestep. **Bottom:** ARC-AGI-2 Task—left: provided example input-output pair; right:\nintermediate steps solving the test input.\n\n\napproach, where the model appears to explore potential solutions and backtracks when it hits dead\nends. HRM uses a different approach for ARC tasks, making incremental adjustments to the board\nand iteratively improving it until reaching a solution. Unlike Sudoku, which involves frequent\nbacktracking, the ARC solution path follows a more consistent progression similar to hill-climbing\noptimization.\n\n\nImportantly, the model shows that it can adapt to different reasoning approaches, likely choosing an\neffective strategy for each particular task. Further research is needed to gain more comprehensive\ninsights into these solution strategies.\n\n### **4 Brain Correspondence**\n\n\nA key principle from systems neuroscience is that a brain region’s functional repertoire—its ability\nto handle diverse and complex tasks—is closely linked to the dimensionality of its neural representations [75][,][76] . Higher-order cortical areas, responsible for complex reasoning and decision-making,\nmust handle a wide variety of tasks, demanding more flexible and context-dependent processing [77] .\nIn dynamical systems, this flexibility is often realized through higher-dimensional state-space trajectories, which allow for a richer repertoire of potential computations [78] . This principle gives rise\nto an observable _dimensionality hierarchy_, where a region’s position in the processing hierarchy\ncorrelates with its _effective dimensionality_ . To quantify this phenomenon, we can examine the\n\n\n13\n\n\n(e)\n\n\n(f)\n\n\n\n(a)\n\n\n(b)\n\n\n\n(c)\n\n\n\n5.0\n\n\n4.5\n\n\n4.0\n\n\n3.5\n\n\n3.0\n\n\n2.5\n\n\n2.0\n\n\n0 20 40\n\n\nPosition in the hierarchy\n\n\nFigure 8: **Hierarchical Dimensionality Organization in the HRM and Mouse Cortex.** (a,b) are\nadapted from Posani et al. [74] . (a) Anatomical illustration of mouse cortical areas, color-coded by\nfunctional modules. (b) Correlation between Participation Ratio (PR), a measure of effective neural\ndimensionality, and hierarchical position across different mouse cortical areas. Higher positions in\nthe hierarchy (e.g., MOs, ACAd) exhibit significantly higher PR values compared to lower sensory\nareas (e.g., SSp-n), with a Spearman correlation coefficient of _ρ_ = 0.79 (P = 0.0003). (c,d) **Trained**\n**HRM.** (c) PR scaling of the trained HRM with task diversity. The dimensionality of the highlevel module ( _z_ _H_ ) scales with the number of unique tasks (trajectories) included in the analysis,\nindicating an adaptive expansion of its representational capacity. In contrast, the low-level module’s\n( _z_ _L_ ) dimensionality remains stable. (d) PR values for the low-level ( _z_ _L_, PR = 30.22) and highlevel ( _z_ _H_, PR = 89.95) modules of the _trained_ HRM, computed from neural activity during 100\nunique Sudoku-solving trajectories. A clear dimensionality hierarchy is observed, with the highlevel module operating in a substantially higher-dimensional space. (e,f) **Analysis of Untrained**\n**Network.** To verify that the dimensionality hierarchy is an emergent property of training, the same\nanalyses were performed on an _untrained_ HRM with random weights. (e) In contrast to the trained\nmodel’s scaling in (c), the dimensionality of both modules in the untrained model remains low and\nstable, failing to scale with the number of tasks. (f) Similarly, contrasting with the clear separation\nin (d), the PR values for the untrained model’s modules ( _z_ _L_, PR = 42.09; _z_ _H_, PR = 40.75) are\nlow and nearly identical, showing no evidence of hierarchical separation. This confirms that the\nobserved hierarchical organization of dimensionality is a learned property that emerges through\ntraining, not an artifact of the model’s architecture.\n\n\n14\n\n\nParticipation Ratio (PR), which serves as a standard measure of the effective dimensionality of a\nhigh-dimensional representation [79] . The PR is calculated using the formula\n\n\n\n\n[�] _i_ _[λ]_ _[i]_ [)] [2]\n\n~~�~~ _i_ _[λ]_ _i_ [2]\n\n\n\nPR = [(][�]\n\n\n\n_,_\n_i_ _[λ]_ _i_ [2]\n\n\n\nwhere _{λ_ _i_ _}_ are the eigenvalues of the covariance matrix of neural trajectories. Intuitively, a higher\nPR value signifies that variance is distributed more evenly across many dimensions, corresponding\nto a higher-dimensional representation. Conversely, a lower PR value indicates that variance is\nconcentrated in only a few principal components, reflecting a more compact, lower-dimensional\n\nstructure.\n\n\nThe dimensionality hierarchy can be observed, for example, in the mouse cortex, where the PR of\npopulation activity increases monotonically from low-level sensory areas to high-level associative\nareas, supporting this link between dimensionality and functional complexity [74] (Figure 8 (a,b)).\n\n\nWe evaluated whether HRM reproduces this neuroscientific principle by calculating the PR for\nboth recurrent modules after training on the _Sudoku-Extreme Full_ dataset. The PR computation\nused the covariance matrix derived from neural states gathered across multiple Sudoku-solving\ntrajectories. The results show a striking parallel to the biological findings. The low-level module’s\nstate ( _z_ _L_ ) occupies a relatively small subspace with a participation ratio of 30.22, whereas the highlevel module’s state ( _z_ _H_ ) operates in a substantially larger subspace with a participation ratio of\n89.95, as shown in Figure 8(c). Furthermore, Figure 8(d) shows that increasing the number of\nunique tasks (trajectories) from 10 to 100 causes _z_ _H_ dimensionality to scale up accordingly, while\n_z_ _L_ dimensionality remains stable. These results suggest an _emergent_ separation of representational\ncapacity between the modules that parallels their functional roles.\n\n\nTo confirm that this hierarchical organization is an emergent property of training, and not an artifact\nof the network’s architecture, we performed a control analysis using an identical but untrained\nnetwork with random weights.\n\n\nWe initialized an identical HRM architecture with random weights and, without any training, measured the PR of its modules as the network processed the same task-specific inputs given to the\ntrained model.\n\n\nThe results, shown in Figure 8(e,f), reveal a stark contrast: the high-level and low-level modules of\nthe untrained network exhibit no hierarchical separation, with their PR values remaining low and\nnearly indistinguishable from each other. This control analysis validates that the dimensionality\nhierarchy is an _emergent property_ that arises as the model learns to perform complex reasoning.\n\n\nThe high-to-low PR ratio in HRM ( _z_ _H_ _/z_ _L_ _≈_ 2 _._ 98) closely matches that measured in the mouse\ncortex ( _≈_ 2 _._ 25). In contrast, conventional deep networks often exhibit _neural collapse_, where\nlast-layer features converge to a low-dimensional subspace [80][,][81][,][82] . HRM therefore departs from the\ncollapse pattern and instead fosters a high-dimensional representation in its higher module. This\nis significant because such representations are considered crucial for cognitive flexibility and are a\nhallmark of higher-order brain regions like the prefrontal cortex (PFC), which is central to complex\nreasoning.\n\n\nThis structural parallel suggests the model has discovered a fundamental organizational principle.\nBy learning to partition its representations into a high-capacity, high-dimensional subspace ( _z_ _H_ )\nand a more specialized, low-dimensional one ( _z_ _L_ ), HRM autonomously discovers an organizational\n\n\n15\n\n\nprinciple that is thought to be fundamental for achieving robust and flexible reasoning in biological\nsystems. This provides a potential mechanistic explanation for the model’s success on complex,\nlong-horizon tasks that are intractable for models lacking such a differentiated internal structure.\nWe emphasize, however, that this evidence is correlational. While a causal link could be tested\nvia intervention (e.g., by constraining the H-module’s dimensionality), such methods are difficult\nto interpret in deep learning due to potential confounding effects on the training process itself.\nThus, the causal necessity of this emergent hierarchy remains an important question for future\ninvestigation.\n\n### **5 Related Work**\n\n\n**Reasoning and algorithm learning** Given the central role of reasoning problems and their close\nrelation to algorithms, researchers have long explored neural architectures that enable algorithm\nlearning from training instances. This line of work includes Neural Turing Machines (NTM) [83],\nthe Differentiable Neural Computer (DNC) [84], and Neural GPUs [85] –all of which construct iterative\nneural architectures that mimic computational hardware for algorithm execution, and are trained to\nlearn algorithms from data. Another notable work in this area is Recurrent Relational Networks\n(RRN) [62], which executes algorithms on graph representations through graph neural networks.\n\n\nRecent studies have integrated algorithm learning approaches with Transformer-based architectures. Universal Transformers extend the standard Transformer model by introducing a recurrent\nloop over the layers and implementing an adaptive halting mechanism. Geiping et al. [86] demonstrate\nthat looped Transformers can generalize to a larger number of recurrent steps during inference than\nwhat they were trained on. Shen et al. [16] propose adding continuous recurrent reasoning tokens\nto the Transformer. Finally, TransNAR [8] combine recurrent graph neural networks with language\nmodels.\n\n\nBuilding on the success of CoT-based reasoning, a line of work have introduced fine-tuning methods that use reasoning paths from search algorithms (like A*) as SFT targets [87][,][71][,][70] .\n\n\nWe also mention adaptive halting mechanisms designed to allocate additional computational resources to more challenging problems. This includes the Adaptive Computation Time (ACT) for\nRNNs [88] and follow-up research like PonderNet [89], which aims to improve the stability of this allocation process.\n\n\nHRM further pushes the boundary of algorithm learning through a brain-inspired computational\narchitecture that achieves exceptional data efficiency and model expressiveness, successfully discovering complex and diverse algorithms from just 1000 training examples.\n\n\n**Brain-inspired reasoning architectures** Developing a model with the reasoning power of the\nbrain has long been a goal in brain-inspired computing. Spaun [90] is one notable example, which uses\nspiking neural networks to create distinct modules corresponding to brain regions like the visual\ncortex and prefrontal cortex. This design enables an architecture to perform a range of cognitive\ntasks, from memory recall to simple reasoning puzzles. However, its reasoning relies on handdesigned algorithms, which may limit its ability to learn new tasks. Another significant model is the\nTolman-Eichenbaum Machine (TEM) [91], which is inspired by the hippocampal-entorhinal system’s\nrole in spatial and relational memory tasks. TEM proposes that medial entorhinal cells create a\nbasis for structural knowledge, while hippocampal cells link this basis to sensory information. This\nallows TEM to generalize and explains the emergence of various cell types like grid, border, and\n\n\n16\n\n\nplace cells. Another approach involves neural sampling models [92], which view the neural signaling\nprocess as inference over a distribution, functioning similarly to a Boltzmann machine. These\nmodels often require hand-made rules to be set up for solving a specific reasoning task. In essence,\nwhile prior models are restricted to simple reasoning problems, HRM is designed to solve complex\ntasks that are hard for even advanced LLMs, without pre-training or task-specific manual design.\n\n\n**Hierarchical memory** The hierarchical multi-timescale structure also plays an important role in\nhow the brain processes memory. Models such as Hierarchical Sequential Models [93] and Clockwork\nRNN [94] use multiple recurrent modules that operate at varying time scales to more effectively capture long-range dependencies within sequences, thereby mitigating the forgetting issue in RNNs.\n\n\nSimilar mechanisms have also been adopted in linear attention methods for memorizing long contexts (see the Discussions section). Since HRM focuses on reasoning, full attention is applied for\nsimplicity. Incorporating hierarchical memory into HRM could be a promising future direction.\n\n### **6 Discussions**\n\n\n**Turing-completeness of HRM** Like earlier neural reasoning algorithms including the Universal\nTransformer [95], HRM is computationally universal when given sufficient memory and time constraints. In other words, it falls into the category of models that can simulate any Turing machine,\novercoming the computational limitations of standard Transformers discussed previously in the introduction. Given that earlier neural algorithm reasoners were trained as recurrent neural networks,\nthey suffer from premature convergence and memory intensive BPTT. Therefore, in practice, their\neffective computational depth remains limited, though still deeper than that of a standard Transformer. By resolving these two challenges and being equipped with adaptive computation, HRM\ncould be trained on long reasoning processes, solve complex puzzles requiring intensive depth-first\nsearch and backtracking, and move closer to practical Turing-completeness.\n\n\n**Reinforcement learning with chain-of-thought** Beyond fine-tuning using human-annotated CoT,\nreinforcement learning (RL) represents another widely adopted training methodology. However,\nrecent evidence suggests that RL primarily unlocks existing CoT-like capabilities rather than discovering fundamentally new reasoning mechanisms [96][,][97][,][98][,][99] . Additionally, CoT-training with RL\nis known for its instability and data inefficiency, often requiring extensive exploration and careful\nreward design. In contrast, HRM takes feedback from dense gradient-based supervision rather than\nrelying on a sparse reward signal. Moreover, HRM operates naturally in a continuous space, which\nis biologically plausible and avoids allocating same computational resources to each token, even\nthough tokens vary in their reasoning and planning complexity [16] .\n\n\n**Linear attention** Recurrence has been explored not only for its capability in universal computation, but also as a means to replace the attention mechanism in Transformers, which suffers from\nquadratic time and memory complexity [100] . Recurrent alternatives offer a more efficient design by\nprocessing input tokens sequentially and predicting the next token at each time step, similar to early\nRNN-based language models.\n\n\nSome linear-attention variants, such as Log-linear Attention [101], share an RNN-like state-update that\ncan be interpreted as propagating multi-timescale summary statistics, thereby retaining long-range\ncontext without the quadratic memory growth of standard self-attention. However, substituting the\nattention mechanism alone does not change the fact that Transformers are still fixed-depth, and\nrequire CoT as a compensatory mechanism. Notably, linear attention can operate with a reduced\n\n\n17\n\n\nkey-value cache over extended contexts, making them more suitable for deployment on resourceconstrained edge devices.\n\n### **7 Conclusion**\n\n\nThis work introduces the Hierarchical Reasoning Model, a brain-inspired architecture that leverages hierarchical structure and multi-timescale processing to achieve substantial computational\ndepth without sacrificing training stability or efficiency. With only 27M parameters and training on just 1000 examples, HRM effectively solves challenging reasoning problems such as ARC,\nSudoku, and complex maze navigation–tasks that typically pose significant difficulties for contemporary LLM and chain-of-thought models.\n\n\nAlthough the brain relies heavily on hierarchical structures to enable most cognitive processes,\nthese concepts have largely remained confined to academic literature rather than being translated\ninto practical applications. The prevailing AI approach continues to favor non-hierarchical models.\nOur results challenge this established paradigm and suggest that the Hierarchical Reasoning Model\nrepresents a viable alternative to the currently dominant chain-of-thought reasoning methods, advancing toward a foundational framework capable of Turing-complete universal computation.\n\n\n**Acknowledgements** We thank Mingli Yuan, Ahmed Murtadha Hasan Mahyoub and Hengshuai\nYao for their insightful discussions and valuable feedback throughout the course of this work.\n\n\n18\n\n\n### **References**\n\n1. Ian Goodfellow, Yoshua Bengio, and Aaron Courville. _Deep Learning_ . MIT Press, 2016.\n\n`[http://www.deeplearningbook.org](http://www.deeplearningbook.org)` .\n\n2. Kaiming He, X. Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image\nrecognition. _2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_,\npages 770–778, 2015.\n\n3. Lena Strobl. Average-hard attention transformers are constant-depth uniform threshold\ncircuits, 2023.\n\n4. Tom Bylander. Complexity results for planning. In _Proceedings of the 12th International Joint_\n_Conference on Artificial Intelligence - Volume 1_, IJCAI’91, page 274–279, San Francisco,\nCA, USA, 1991. Morgan Kaufmann Publishers Inc. ISBN 1558601600.\n\n5. William Merrill and Ashish Sabharwal. A logic for expressing log-precision transformers. In\n_Neural Information Processing Systems_, 2023.\n6. David Chiang. Transformers in DLOGTIME-uniform TC [0] . _Transactions on Machine_\n_Learning Research_, 2025.\n\n7. Lucas Lehnert, Sainbayar Sukhbaatar, DiJia Su, Qinqing Zheng, Paul McVay, Michael\nRabbat, and Yuandong Tian. Beyond a*: Better planning with transformers via search\ndynamics bootstrapping. In _First Conference on Language Modeling_, 2024.\n\n8. Wilfried Bounsi, Borja Ibarz, Andrew Dudzik, Jessica B. Hamrick, Larisa Markeeva, Alex\nVitvitskyi, Razvan Pascanu, and Petar Velivckovi’c. Transformers meet neural algorithmic\nreasoners. _ArXiv_, abs/2406.09308, 2024.\n\n9. William Merrill and Ashish Sabharwal. The parallelism tradeoff: Limitations of log-precision\ntransformers. _Transactions of the Association for Computational Linguistics_, 11:531–545,\n2023. doi: 10.1162/tacl_a_00562.\n\n10. Jason Wei, Yi Tay, et al. Chain-of-thought prompting elicits reasoning in large language\nmodels, 2022. arXiv preprint arXiv:2201.11903.\n\n11. William Merrill and Ashish Sabharwal. The expressive power of transformers with chain of\nthought. In _ICLR_, 2024.\n\n12. Xinyun Chen, Ryan A. Chi, Xuezhi Wang, and Denny Zhou. Premise order matters in\nreasoning with large language models. _ArXiv_, abs/2402.08939, 2024.\n\n13. Rongwu Xu, Zehan Qi, and Wei Xu. Preemptive answer \"attacks\" on chain-of-thought\nreasoning. In _Annual Meeting of the Association for Computational Linguistics_, 2024.\n\n14. Pablo Villalobos, Anson Ho, Jaime Sevilla, Tamay Besiroglu, Lennart Heim, and Marius\nHobbhahn. Will we run out of data? limits of llm scaling based on human-generated data.\n_arXiv preprint arXiv:2211.04325_, 2022.\n\n15. Xinghao Chen, Anhao Zhao, Heming Xia, Xuan Lu, Hanlin Wang, Yanjun Chen, Wei Zhang,\nJian Wang, Wenjie Li, and Xiaoyu Shen. Reasoning beyond language: A comprehensive\nsurvey on latent chain-of-thought reasoning, 2025.\n\n16. Xuan Shen, Yizhou Wang, Xiangxi Shi, Yanzhi Wang, Pu Zhao, and Jiuxiang Gu.\nTraining large language models to reason in a continuous latent space. _arXiv preprint_\n_arXiv:2412.07423_, 2024.\n\n\n19\n\n\n17. Evelina Fedorenko, Steven T Piantadosi, and Edward AF Gibson. Language is primarily a\ntool for communication rather than thought. _Nature_, 630(8017):575–586, 2024.\n\n18. Hongyu Wang, Shuming Ma, Li Dong, Shaohan Huang, Dongdong Zhang, and Furu Wei.\nDeepnet: Scaling transformers to 1,000 layers. _IEEE Transactions on Pattern Analysis and_\n_Machine Intelligence_, 2024.\n\n19. Timothy P Lillicrap and Adam Santoro. Backpropagation through time and the brain. _Current_\n_Opinion in Neurobiology_, 55:82–89, 2019. ISSN 0959-4388. doi: https://doi.org/10.1016/j.\nconb.2019.01.011.\n\n20. John D Murray, Alberto Bernacchia, David J Freedman, Ranulfo Romo, Jonathan D Wallis,\nXinying Cai, Camillo Padoa-Schioppa, Tatiana Pasternak, Hyojung Seo, Daeyeol Lee, et al.\nA hierarchy of intrinsic timescales across primate cortex. _Nature neuroscience_, 17(12):1661–\n1663, 2014.\n\n21. Roxana Zeraati, Yan-Liang Shi, Nicholas A Steinmetz, Marc A Gieselmann, Alexander\nThiele, Tirin Moore, Anna Levina, and Tatiana A Engel. Intrinsic timescales in the\nvisual cortex change with selective attention and reflect spatial connectivity. _Nature_\n_communications_, 14(1):1858, 2023.\n\n22. Julia M Huntenburg, Pierre-Louis Bazin, and Daniel S Margulies. Large-scale gradients in\nhuman cortical organization. _Trends in cognitive sciences_, 22(1):21–31, 2018.\n\n23. Victor AF Lamme and Pieter R Roelfsema. The distinct modes of vision offered by\nfeedforward and recurrent processing. _Trends in neurosciences_, 23(11):571–579, 2000.\n\n24. Andre M Bastos, W Martin Usrey, Rick A Adams, George R Mangun, Pascal Fries, and Karl J\nFriston. Canonical microcircuits for predictive coding. _Neuron_, 76(4):695–711, 2012.\n\n25. Klara Kaleb, Barbara Feulner, Juan Gallego, and Claudia Clopath. Feedback control guides\ncredit assignment in recurrent neural networks. _Advances in Neural Information Processing_\n_Systems_, 37:5122–5144, 2024.\n\n26. Timothy P Lillicrap, Adam Santoro, Luke Marris, Colin J Akerman, and Geoffrey Hinton.\nBackpropagation and the brain. _Nature Reviews Neuroscience_, 21(6):335–346, 2020.\n\n27. François Chollet. On the measure of intelligence (abstraction and reasoning corpus), 2019.\narXiv preprint arXiv:1911.01547.\n\n28. Francois Chollet, Mike Knoop, Gregory Kamradt, and Bryan Landers. Arc prize 2024:\nTechnical report. _ArXiv_, abs/2412.04604, 2024.\n\n29. Francois Chollet, Mike Knoop, Gregory Kamradt, Bryan Landers, and Henry Pinkard. Arcagi-2: A new challenge for frontier ai reasoning systems. _arXiv preprint arXiv:2505.11831_,\n2025.\n\n30. György Buzsáki. Gamma, alpha, delta, and theta oscillations govern cognitive processes.\n_International Journal of Psychophysiology_, 39:241–248, 2000.\n\n31. György Buzsáki. _Rhythms of the Brain_ . Oxford university press, 2006.\n\n32. Anja Pahor and Norbert Jaušovec. Theta–gamma cross-frequency coupling relates to the level\nof human intelligence. _Intelligence_, 46:283–290, 2014.\n\n33. Adriano BL Tort, Robert W Komorowski, Joseph R Manns, Nancy J Kopell, and Howard\nEichenbaum. Theta–gamma coupling increases during the learning of item–context\nassociations. _Proceedings of the National Academy of Sciences_, 106(49):20942–20947, 2009.\n\n\n20\n\n\n34. Benjamin Scellier and Yoshua Bengio. Equilibrium propagation: Bridging the gap between\nenergy-based models and backpropagation. _Frontiers in Computational Neuroscience_, 11,\n2016.\n\n35. Guillaume Bellec, Franz Scherr, Anand Subramoney, Elias Hajek, Darjan Salaj, Robert\nLegenstein, and Wolfgang Maass. A solution to the learning dilemma for recurrent\nnetworks of spiking neurons. _Nature Communications_, 11, 07 2020. doi: 10.1038/\ns41467-020-17236-y.\n\n36. Shaojie Bai, J Zico Kolter, and Vladlen Koltun. Deep equilibrium models. In _Advances in_\n_Neural Information Processing Systems_, pages 690–701, 2019.\n\n37. Zhengyang Geng, Xinyu Zhang, Shaojie Bai, Yisen Wang, and Zhouchen Lin. On training\nimplicit models. _ArXiv_, abs/2111.05177, 2021.\n\n38. Katarina Begus and Elizabeth Bonawitz. The rhythm of learning: Theta oscillations as an\nindex of active learning in infancy. _Developmental Cognitive Neuroscience_, 45:100810, 2020.\nISSN 1878-9293. doi: https://doi.org/10.1016/j.dcn.2020.100810.\n\n39. Shaojie Bai, Zhengyang Geng, Yash Savani, and J. Zico Kolter. Deep Equilibrium\nOptical Flow Estimation . In _2022 IEEE/CVF Conference on Computer Vision and Pattern_\n_Recognition (CVPR)_, pages 610–620, 2022.\n\n40. Zaccharie Ramzi, Florian Mannel, Shaojie Bai, Jean-Luc Starck, Philippe Ciuciu, and\nThomas Moreau. Shine: Sharing the inverse estimate from the forward pass for bi-level\noptimization and implicit models. _ArXiv_, abs/2106.00553, 2021.\n\n41. Shaojie Bai, Vladlen Koltun, and J. Zico Kolter. Stabilizing equilibrium models by jacobian\nregularization. In _International Conference on Machine Learning_, 2021.\n\n42. Daniel Kahneman and P Egan. Thinking, fast and slow (farrar, straus and giroux, new york),\n2011.\n\n43. Matthew D Lieberman. Social cognitive neuroscience: a review of core processes. _Annu. Rev._\n_Psychol._, 58(1):259–289, 2007.\n\n44. Randy L Buckner, Jessica R Andrews-Hanna, and Daniel L Schacter. The brain’s default\nnetwork: anatomy, function, and relevance to disease. _Annals of the new York Academy of_\n_Sciences_, 1124(1):1–38, 2008.\n\n45. Marcus E Raichle. The brain’s default mode network. _Annual review of neuroscience_, 38(1):\n433–447, 2015.\n\n46. Andrew Westbrook and Todd S Braver. Cognitive effort: A neuroeconomic approach.\n_Cognitive, Affective, & Behavioral Neuroscience_, 15:395–415, 2015.\n\n47. Richard S. Sutton and Andrew G. Barto. _Reinforcement Learning: An Introduction_ . MIT\nPress, Cambridge, MA, 2018.\n\n48. Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan\nWierstra, and Martin A. Riedmiller. Playing atari with deep reinforcement learning. _ArXiv_,\nabs/1312.5602, 2013.\n\n49. Matteo Gallici, Mattie Fellows, Benjamin Ellis, Bartomeu Pou, Ivan Masmitja,\nJakob Nicolaus Foerster, and Mario Martin. Simplifying deep temporal difference learning,\n2025.\n\n\n21\n\n\n50. Shuo Xie and Zhiyuan Li. Implicit bias of adamw: L inf norm constrained optimization.\n_ArXiv_, abs/2404.04454, 2024.\n\n51. Lucas Prieto, Melih Barsbey, Pedro A. M. Mediano, and Tolga Birdal. Grokking at the edge of\nnumerical stability. In _The Thirteenth International Conference on Learning Representations_,\n2025.\n\n52. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\nŁukasz Kaiser, and Illia Polosukhin. Attention is all you need. In _Advances in neural_\n_information processing systems_, pages 5998–6008, 2017.\n\n53. Meta AI. Llama 3: State-of-the-art open weight language models. Technical report, Meta,\n2024. URL `[https://ai.meta.com/llama/](https://ai.meta.com/llama/)` .\n\n54. Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer:\nEnhanced transformer with rotary position embedding. _Neurocomputing_, 568:127063, 2024.\n\n55. Noam M. Shazeer. Glu variants improve transformer. _ArXiv_, abs/2002.05202, 2020.\n\n56. Biao Zhang and Rico Sennrich. Root mean square layer normalization. _ArXiv_,\nabs/1910.07467, 2019.\n\n57. Günter Klambauer, Thomas Unterthiner, Andreas Mayr, and Sepp Hochreiter. Selfnormalizing neural networks. In _Neural Information Processing Systems_, 2017.\n\n58. JAX Developers. _jax.nn.initializers.lecun_normal_ . Google Research, 2025. URL\n```\n  https://docs.jax.dev/en/latest/_autosummary/jax.nn.initializers.lecun_\n```\n\n`[normal.html](https://docs.jax.dev/en/latest/_autosummary/jax.nn.initializers.lecun_normal.html)` . Accessed June 22, 2025.\n\n59. Yann LeCun, Léon Bottou, Genevieve B Orr, and Klaus-Robert Müller. Efficient backprop.\nIn _Neural networks: Tricks of the trade_, pages 9–50. Springer, 2002.\n\n60. Katie E Everett, Lechao Xiao, Mitchell Wortsman, Alexander A Alemi, Roman Novak,\nPeter J Liu, Izzeddin Gur, Jascha Sohl-Dickstein, Leslie Pack Kaelbling, Jaehoon Lee, and\nJeffrey Pennington. Scaling exponents across parameterizations and optimizers. In _Forty-first_\n_International Conference on Machine Learning_, 2024.\n\n61. Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization, 2017.\n\n62. Rasmus Berg Palm, Ulrich Paquet, and Ole Winther. Recurrent relational networks. In _Neural_\n_Information Processing Systems_, 2017.\n\n63. Jieyi Long. Large language model guided tree-of-thought. _ArXiv_, abs/2305.08291, 2023.\n\n64. Yilun Du, Jiayuan Mao, and Josh Tenenbaum. Learning iterative reasoning through energy\ndiffusion. _ArXiv_, abs/2406.11179, 2024.\n\n65. Kyubyong Park. Can convolutional neural networks crack sudoku puzzles? `[https:](https://github.com/Kyubyong/sudoku)`\n`[//github.com/Kyubyong/sudoku](https://github.com/Kyubyong/sudoku)`, 2018.\n\n66. Single-digit techniques. `[https://hodoku.sourceforge.net/en/tech_singles.php](https://hodoku.sourceforge.net/en/tech_singles.php)` .\nAccessed: 2025-06-16.\n\n67. Tom Dillion. Tdoku: A fast sudoku solver and generator. `[https://t-dillon.github.io/](https://t-dillon.github.io/tdoku/)`\n`[tdoku/](https://t-dillon.github.io/tdoku/)`, 2025.\n\n68. Jeffrey Seely, Yuki Imajuku, Tianyu Zhao, Edoardo Cetin, and Llion Jones. Sudoku-bench:\nEvaluating creative reasoning with sudoku variants. _arXiv preprint arXiv:2505.16135_, 2025.\n\n69. Luke Darlow, Ciaran Regan, Sebastian Risi, Jeffrey Seely, and Llion Jones. Continuous\nthought machines. _arXiv preprint arXiv:2505.05522_, 2025.\n\n\n22\n\n\n70. DiJia Su, Sainbayar Sukhbaatar, Michael Rabbat, Yuandong Tian, and Qinqing Zheng.\nDualformer: Controllable fast and slow thinking by learning with randomized reasoning\ntraces, 2025.\n\n71. Lucas Lehnert, Sainbayar Sukhbaatar, DiJia Su, Qinqing Zheng, Paul McVay, Michael\nRabbat, and Yuandong Tian. Beyond a*: Better planning with transformers via search\ndynamics bootstrapping. In _First Conference on Language Modeling_, 2024.\n\n72. Mubbasir Kapadia, Francisco Garcia, Cory D. Boatright, and Norman I. Badler. Dynamic\nsearch on the gpu. In _2013 IEEE/RSJ International Conference on Intelligent Robots and_\n_Systems_, pages 3332–3337, 2013. doi: 10.1109/IROS.2013.6696830.\n\n73. Isaac Liao and Albert Gu. Arc-agi without pretraining, 2025. URL `[https:](https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/arc_agi_without_pretraining.html)`\n```\n  //iliao2345.github.io/blog_posts/arc_agi_without_pretraining/arc_agi_\n```\n\n`[without_pretraining.html](https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/arc_agi_without_pretraining.html)` .\n\n74. Lorenzo Posani, Shuqi Wang, Samuel P Muscinelli, Liam Paninski, and Stefano Fusi.\nRarely categorical, always high-dimensional: how the neural code changes along the cortical\nhierarchy. _bioRxiv_, pages 2024–11, 2025.\n\n75. Mattia Rigotti, Omri Barak, Melissa R. Warden, Xiao-Jing Wang, Nathaniel D. Daw, Earl K.\nMiller, and Stefano Fusi. The importance of mixed selectivity in complex cognitive tasks.\n_Nature_, 497:585–590, 2013. doi: 10.1038/nature12160.\n\n76. Valerio Mante, David Sussillo, Krishna V. Shenoy, and William T. Newsome. Contextdependent computation by recurrent dynamics in prefrontal cortex. _Nature_, 503(7474):78–84,\n2013. doi: 10.1038/nature12742.\n\n77. Earl K. Miller and Jonathan D. Cohen. An integrative theory of prefrontal cortex function.\n_Annual Review of Neuroscience_, 24(1):167–202, 2001. doi: 10.1146/annurev.neuro.24.1.167.\n\n78. Wolfgang Maass. Real-time computing without stable states: a new framework for neural\ncomputation based on perturbations. _Neural Computation_, 14(11):2531–2560, 2002. doi:\n10.1162/089976602760407955.\n\n79. Ege Altan, Sara A. Solla, Lee E. Miller, and Eric J. Perreault. Estimating the dimensionality\nof the manifold underlying multi-electrode neural recordings. _PLoS Computational Biology_,\n17(11):e1008591, 2021. doi: 10.1371/journal.pcbi.1008591.\n\n80. Vardan Papyan, X. Y. Han, and David L. Donoho. Prevalence of neural collapse during the\nterminal phase of deep learning training. _Proceedings of the National Academy of Sciences_,\n117(40):24652–24663, 2020. doi: 10.1073/pnas.2015509117.\n\n81. Cong Fang, Hangfeng He, Qi Long, and Weijie J. Su. Exploring deep neural networks via\nlayer–peeled model: Minority collapse in imbalanced training. _Proceedings of the National_\n_Academy of Sciences_, 118(43):e2103091118, 2021. doi: 10.1073/pnas.2103091118.\n\n82. Zhihui Zhu, Tianyu Ding, Jinxin Zhou, Xiao Li, Chong You, Jeremias Sulam, and Qing Qu.\nA geometric analysis of neural collapse with unconstrained features. In _Advances in Neural_\n_Information Processing Systems_, volume 34 of _NeurIPS_, pages 29820–29834, 2021.\n\n83. Alex Graves, Greg Wayne, and Ivo Danihelka. Neural turing machines, 2014.\n\n84. Alex Graves, Greg Wayne, Malcolm Reynolds, Tim Harley, Ivo Danihelka, Agnieszka\nGrabska-Barwi´nska, Sergio Gómez Colmenarejo, Edward Grefenstette, Tiago Ramalho, John\nAgapiou, et al. Hybrid computing using a neural network with dynamic external memory.\n_Nature_, 538(7626):471–476, 2016.\n\n\n23\n\n\n85. Lukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In _ICLR_, 2016.\n\n86. Jonas Geiping, Sean McLeish, Neel Jain, John Kirchenbauer, Siddharth Singh, Brian R.\nBartoldson, Bhavya Kailkhura, Abhinav Bhatele, and Tom Goldstein. Scaling up test-time\ncompute with latent reasoning: A recurrent depth approach, 2025.\n\n87. Tiedong Liu and Kian Hsiang Low. Goat: Fine-tuned llama outperforms gpt-4 on arithmetic\ntasks. _ArXiv_, abs/2305.14201, 2023.\n\n88. Alex Graves. Adaptive computation time for recurrent neural networks. _ArXiv_,\nabs/1603.08983, 2016.\n\n89. Andrea Banino, Jan Balaguer, and Charles Blundell. Pondernet: Learning to ponder. _ArXiv_,\nabs/2107.05407, 2021.\n\n90. Chris Eliasmith, Terrence C Stewart, Xuan Choo, Trevor Bekolay, Travis DeWolf, Yichuan\nTang, and Daniel Rasmussen. A large-scale model of the functioning brain. _science_, 338\n(6111):1202–1205, 2012.\n\n91. James CR Whittington, Timothy H Muller, Shirley Mark, Guifen Chen, Caswell Barry, Neil\nBurgess, and Timothy EJ Behrens. The tolman-eichenbaum machine: unifying space and\nrelational memory through generalization in the hippocampal formation. _Cell_, 183(5):1249–\n1263, 2020.\n\n92. Lars Buesing, Johannes Bill, Bernhard Nessler, and Wolfgang Maass. Neural dynamics as\nsampling: a model for stochastic computation in recurrent networks of spiking neurons. _PLoS_\n_computational biology_, 7(11):e1002211, 2011.\n\n93. Salah Hihi and Yoshua Bengio. Hierarchical recurrent neural networks for long-term\ndependencies. In D. Touretzky, M.C. Mozer, and M. Hasselmo, editors, _Advances in Neural_\n_Information Processing Systems_, volume 8. MIT Press, 1995.\n\n94. Jan Koutník, Klaus Greff, Faustino J. Gomez, and Jürgen Schmidhuber. A clockwork rnn. In\n_International Conference on Machine Learning_, 2014.\n\n95. Mostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob Uszkoreit, and Lukasz Kaiser.\nUniversal transformers, 2018. arXiv preprint arXiv:1807.03819.\n\n96. Yiping Wang, Qing Yang, Zhiyuan Zeng, Liliang Ren, Lucas Liu, Baolin Peng, Hao Cheng,\nXuehai He, Kuan Wang, Jianfeng Gao, Weizhu Chen, Shuohang Wang, Simon Shaolei Du,\nand Yelong Shen. Reinforcement learning for reasoning in large language models with one\ntraining example, 2025. URL `[https://arxiv.org/abs/2504.20571](https://arxiv.org/abs/2504.20571)` .\n\n97. Niklas Muennighoff. s1: Simple test-time scaling. _arXiv preprint arXiv:2502.23456_, 2025.\n\n98. Liang Wen, Yunke Cai, Fenrui Xiao, Xin He, Qi An, Zhenyu Duan, Yimin Du, Junchen Liu,\nLifu Tang, Xiaowei Lv, Haosheng Zou, Yongchao Deng, Shousheng Jia, and Xiangzheng\nZhang. Light-r1: Curriculum sft, dpo and rl for long cot from scratch and beyond, 2025.\n\n99. Xuefeng Li, Haoyang Zou, and Pengfei Liu. Limr: Less is more for rl scaling, 2025.\n\n100. Tri Dao and Albert Gu. Transformers are ssms: Generalized models and efficient algorithms\nthrough structured state space duality. _ArXiv_, abs/2405.21060, 2024.\n\n101. Han Guo, Songlin Yang, Tarushii Goel, Eric P Xing, Tri Dao, and Yoon Kim. Log-linear\nattention. _arXiv preprint arXiv:2506.04761_, 2025.\n\n\n24\n\n\n", "sections": [{"section_id": "section_0", "title": "Hierarchical Reasoning Model", "content": "Guan Wang [1] _[,][†]_, Jin Li [1], Yuhao Sun [1], Xing Chen [1], Changling Liu [1],\nYue Wu [1], Meng Lu [1] _[,][†]_, Sen Song [2] _[,][†]_, Yasin Abbasi Yadkori [1] _[,][†]_\n1 Sapient Intelligence, Singapore\n**Abstract**\nReasoning, the process of devising and executing complex goal-oriented action sequences,\nremains a critical challenge in AI. Current large language models (LLMs) primarily employ\nChain-of-Thought (CoT) techniques, which suffer from brittle task decomposition, extensive\ndata requirements, and high latency. Inspired by the hierarchical and multi-timescale processing in the human brain, we propose the Hierarchical Reasoning Model (HRM), a novel\nrecurrent architecture that attains significant computational depth while maintaining both training stability and efficiency. HRM executes sequential reasoning tasks in a single forward pass\nwithout explicit supervision of the intermediate process, through two interdependent recurrent\nmodules: a high-level module responsible for slow, abstract planning, and a low-level module handling rapid, detailed computations. With only 27 million parameters, HRM achieves\nexceptional performance on complex reasoning tasks using only 1000 training samples. The\nmodel operates without pre-training or CoT data, yet achieves nearly perfect performance on\nchallenging tasks including complex Sudoku puzzles and optimal path finding in large mazes.\nFurthermore, HRM outperforms much larger models with significantly longer context windows\non the Abstraction and Reasoning Corpus (ARC), a key benchmark for measuring artificial\ngeneral intelligence capabilities. These results underscore HRM’s potential as a transformative\nadvancement toward universal computation and general-purpose reasoning systems.\nFigure 1: **Left:** HRM is inspired by hierarchical processing and temporal separation in the brain. It\nhas two recurrent networks operating at different timescales to collaboratively solve tasks. **Right:**\nWith only about 1000 training examples, the HRM (~27M parameters) surpasses state-of-the-art\nCoT models on inductive benchmarks (ARC-AGI) and challenging symbolic tree-search puzzles\n( _Sudoku-Extreme_, _Maze-Hard_ ) where CoT models failed completely. The HRM was randomly\ninitialized, and it solved the tasks directly from inputs without chain of thoughts.\n2 Tsinghua University _†_ Corresponding author. Contact: `research@sapient.inc` .\nCode available at: `[github.com/sapientinc/HRM](https://github.com/sapientinc/HRM)`\n1", "level": 1, "page_numbers": [], "subsections": [], "references": [], "equations": [], "images": [], "tables": []}, {"section_id": "section_1", "title": "**1 Introduction**", "content": "Deep learning, as its name suggests, emerged from the idea of stacking more layers to achieve\nincreased representation power and improved performance [1][,][2] . However, despite the remarkable\nsuccess of large language models, their core architecture is paradoxically shallow [3] . This imposes\na fundamental constraint on their most sought-after capability: reasoning. The fixed depth of standard Transformers places them in computational complexity classes such as _AC_ [0] or _TC_ [0][ 4], preventing them from solving problems that require polynomial time [5][,][6] . LLMs are not Turing-complete\nand thus they cannot, at least in a purely end-to-end manner, execute complex algorithmic reasoning that is necessary for deliberate planning or symbolic manipulation tasks [7][,][8] . For example,\nour results on the Sudoku task show that increasing Transformer model depth _can_ improve performance, [1] but performance remains far from optimal even with very deep models (see Figure 2),\nwhich supports the conjectured limitations of the LLM scaling paradigm [9] .\nThe LLMs literature has relied largely on Chain-of-Thought (CoT) prompting for reasoning [10] .\nCoT externalizes reasoning into token-level language by breaking down complex tasks into simpler intermediate steps, sequentially generating text using a shallow model [11] . However, CoT for\nreasoning is a crutch, not a satisfactory solution. It relies on brittle, human-defined decompositions\nwhere a single misstep or a misorder of the steps can derail the reasoning process entirely [12][,][13] . This\ndependency on explicit linguistic steps tethers reasoning to patterns at the token level. As a result,\nCoT reasoning often requires significant amount of training data and generates a large number of\ntokens for complex reasoning tasks, resulting in slow response times. A more efficient approach is\nneeded to minimize these data requirements [14] .\nTowards this goal, we explore “latent reasoning”, where the model conducts computations within\nits internal hidden state space [15][,][16] . This aligns with the understanding that language is a tool for\nhuman communication, not the substrate of thought itself [17] ; the brain sustains lengthy, coherent\nchains of reasoning with remarkable efficiency in a latent space, without constant translation back\nto language. However, the power of latent reasoning is still fundamentally constrained by a model’s\n_effective computational depth_ . Naively stacking layers is notoriously difficult due to vanishing gradients, which plague training stability and effectiveness [1][,][18] . Recurrent architectures, a natural alternative for sequential tasks, often suffer from early convergence, rendering subsequent computational steps inert, and rely on the biologically implausible, computationally expensive and memory\nintensive Backpropagation Through Time (BPTT) for training [19] .\nThe human brain provides a compelling blueprint for achieving the effective computational depth\nthat contemporary artificial models lack. It organizes computation hierarchically across cortical regions operating at different timescales, enabling deep, multi-stage reasoning [20][,][21][,][22] . Recurrent feedback loops iteratively refine internal representations, allowing slow, higher-level areas to\nguide, and fast, lower-level circuits to execute—subordinate processing while preserving global\ncoherence [23][,][24][,][25] . Notably, the brain achieves such depth without incurring the prohibitive creditassignment costs that typically hamper recurrent networks from backpropagation through time [19][,][26] .\nInspired by this hierarchical and multi-timescale biological architecture, we propose the Hierarchical Reasoning Model (HRM). HRM is designed to significantly increase the effective computational depth. It features two coupled recurrent modules: a high-level (H) module for abstract,\ndeliberate reasoning, and a low-level (L) module for fast, detailed computations. This structure\n1 Simply increasing the model width does not improve performance here.\n2\n|Col1|Scaling Width - 8 layers fixed<br>Scaling Depth - 512 hidden size fixed|Col3|Col4|Col5|Col6|Col7|Col8|\n|---|---|---|---|---|---|---|---|\n|||||||||\n|||||||||\n|||||||||\n|||||||||\n|Col1|Transformer<br>Recurrent Transformer<br>HRM|Col3|Col4|Col5|Col6|Col7|Col8|Col9|\n|---|---|---|---|---|---|---|---|---|\n||||||||||\n||||||||||\n||||||||||\n||||||||||\nFigure 2: **The necessity of depth for complex reasoning. Left:** On _Sudoku-Extreme Full_, which\nrequire extensive tree-search and backtracking, increasing a Transformer’s width yields no performance gain, while increasing depth is critical. **Right:** Standard architectures saturates, failing to\nbenefit from increased depth. HRM overcomes this fundamental limitation, effectively using its\ncomputational depth to achieve near-perfect accuracy.\navoids the rapid convergence of standard recurrent models through a process we term “hierarchical convergence.” The slow-updating H-module advances only after the fast-updating L-module\nhas completed multiple computational steps and reached a local equilibrium, at which point the\nL-module is reset to begin a new computational phase.\nFurthermore, we propose a one-step gradient approximation for training HRM, which offers improved efficiency and eliminates the requirement for BPTT. This design maintains a constant memory footprint ( _O_ (1) compared to BPTT’s _O_ ( _T_ ) for _T_ timesteps) throughout the backpropagation\nprocess, making it scalable and more biologically plausible.\nLeveraging its enhanced effective depth, HRM excels at tasks that demand extensive search and\nbacktracking. **Using only 1,000 input-output examples, without pre-training or CoT supervi-**\n**sion**, HRM learns to solve problems that are intractable for even the most advanced LLMs. For\nexample, it achieves near-perfect accuracy in complex Sudoku puzzles ( _Sudoku-Extreme Full_ ) and\noptimal pathfinding in 30x30 mazes, where state-of-the-art CoT methods completely fail (0% accuracy). In the Abstraction and Reasoning Corpus (ARC) AGI Challenge [27][,][28][,][29] - a benchmark\nof inductive reasoning - HRM, trained from scratch with only the official dataset (~1000 examples), with only 27M parameters and a 30x30 grid context (900 tokens), achieves a performance\nof **40.3%**, which substantially surpasses leading CoT-based models like o3-mini-high (34.5%)\nand Claude 3.7 8K context (21.2%), despite their considerably larger parameter sizes and context lengths, as shown in Figure 1. This represents a promising direction toward the development\nof next-generation AI reasoning systems with universal computational capabilities.", "level": 3, "page_numbers": [], "subsections": [], "references": [], "equations": [], "images": [], "tables": []}, {"section_id": "section_2", "title": "**2 Hierarchical Reasoning Model**", "content": "We present the HRM, inspired by three fundamental principles of neural computation observed in\nthe brain:\n- **Hierarchical processing:** The brain processes information across a hierarchy of cortical areas. Higher-level areas integrate information over longer timescales and form abstract representations, while lower-level areas handle more immediate, detailed sensory and motor processing [20][,][22][,][21] .\n3\n- **Temporal Separation:** These hierarchical levels in the brain operate at distinct intrinsic timescales,\nreflected in neural rhythms (e.g., slow theta waves, 4–8 Hz and fast gamma waves, 30–100\nHz) [30][,][31] . This separation allows for stable, high-level guidance of rapid, low-level computations [32][,][33] .\n- **Recurrent Connectivity:** The brain features extensive recurrent connections. These feedback\nloops enable iterative refinement, yielding more accurate and context-sensitive representations\nat the cost of additional processing time. Additionally, the brain largely avoids the problematic\ndeep credit assignment problem associated with BPTT [19] .\nThe HRM model consists of four learnable components: an input network _f_ _I_ ( _·_ ; _θ_ _I_ ), a low-level recurrent module _f_ _L_ ( _·_ ; _θ_ _L_ ), a high-level recurrent module _f_ _H_ ( _·_ ; _θ_ _H_ ), and an output network _f_ _O_ ( _·_ ; _θ_ _O_ ).\nThe model’s dynamics unfold over _N_ high-level cycles of _T_ low-level timesteps each [2] . We index\nthe total timesteps of one forward pass by _i_ = 1 _, . . ., N × T_ . The modules _f_ _L_ and _f_ _H_ each keep a\nhidden state— _z_ _L_ _[i]_ [for] _[ f]_ _[L]_ [ and] _[ z]_ _H_ _[i]_ [for] _[ f]_ _[H]_ [—which are initialized with the vectors] _[ z]_ _L_ [0] [and] _[ z]_ _H_ [0] [, respec-]\ntively.\nThe HRM maps an input vector _x_ to an output prediction vector ˆ _y_ as follows. First, the input _x_ is\nprojected into a working representation ˜ _x_ by the input network:\n˜\n_x_ = _f_ _I_ ( _x_ ; _θ_ _I_ ) _._\nAt each timestep _i_, the L-module updates its state conditioned on its own previous state, the Hmodule’s current state (which remains fixed throughout the cycle), and the input representation.\nThe H-module only updates once per cycle (i.e., every _T_ timesteps) using the L-module’s final\nstate at the end of that cycle:\n_z_ _L_ _[i]_ [=] _[ f]_ _[L]_ � _z_ _L_ _[i][−]_ [1] _[, z]_ _H_ _[i][−]_ [1] _[,]_ [ ˜] _[x]_ [;] _[ θ]_ _[L]_ � _,_\n_z_ _H_ _[i]_ [=]\n_f_ _H_ � _z_ _H_ _[i][−]_ [1] _[, z]_ _L_ _[i][−]_ [1] [;] _[ θ]_ _[H]_ � if _i ≡_ 0 (mod _T_ ) _,_\n_z_ _[i][−]_ [1] otherwise _._\n� _H_\nFinally, after _N_ full cycles, a prediction ˆ _y_ is extracted from the hidden state of the H-module:\nˆ\n_y_ = _f_ _O_ ( _z_ _H_ _[NT]_ [;] _[ θ]_ _[O]_ [)] _[ .]_\nThis entire _NT_ -timestep process represents a single forward pass of the HRM. A halting mechanism (detailed later in this section) determines whether the model should terminate, in which case\nˆ\n_y_ will be used as the final prediction, or continue with an additional forward pass.\n**Hierarchical convergence** Although convergence is crucial for recurrent networks, standard RNNs\nare fundamentally limited by their tendency to converge too early. As the hidden state settles toward\na fixed point, update magnitudes shrink, effectively stalling subsequent computation and capping\nthe network’s effective depth. To preserve computational power, we actually want convergence to\nproceed very slowly–but engineering that gradual approach is difficult, since pushing convergence\ntoo far edges the system toward instability.\n2 While inspired by temporal separation in the brain, our model’s “high-level” and “low-level” modules are conceptual abstractions and do not map directly to specific neural oscillation frequencies.\n4\nFigure 3: Comparison of forward residuals and PCA trajectories. HRM shows hierarchical convergence: the H-module steadily converges, while the L-module repeatedly converges within cycles\nbefore being reset by H, resulting in residual spikes. The recurrent neural network exhibits rapid\nconvergence with residuals quickly approaching zero. In contrast, the deep neural network experiences vanishing gradients, with significant residuals primarily in the initial (input) and final layers.\nHRM is explicitly designed to counteract this premature convergence through a process we term\n_hierarchical convergence_ . During each cycle, the L-module (an RNN) exhibits stable convergence\nto a _local equilibrium_ . This equilibrium, however, depends on the high-level state _z_ _H_ supplied\nduring that cycle. After completing the _T_ steps, the H-module incorporates the sub-computation’s\noutcome (the final state _z_ _L_ ) and performs its own update. This _z_ _H_ update establishes a fresh context\nfor the L-module, essentially “restarting” its computational path and initiating a new convergence\nphase toward a different local equilibrium.\nThis process allows the HRM to perform a sequence of distinct, stable, nested computations, where\nthe H-module directs the overall problem-solving strategy and the L-module executes the intensive\nsearch or refinement required for each step. Although a standard RNN may approach convergence\nwithin _T_ iterations, the hierarchical convergence benefits from an enhanced effective depth of _NT_\nsteps. As empirically shown in Figure 3, this mechanism allows HRM both to maintain high\ncomputational activity (forward residual) over many steps (in contrast to a standard RNN, whose\nactivity rapidly decays) and to enjoy stable convergence. This translates into better performance at\nany computation depth, as illustrated in Figure 2.\n**Approximate gradient** Recurrent models typically use BPTT to compute gradients. However,\nBPTT requires storing the hidden states from the forward pass and then combining them with\ngradients during the backward pass, which demands _O_ ( _T_ ) memory for T timesteps. This heavy\nmemory burden forces smaller batch sizes and leads to poor GPU utilization, especially for largescale networks. Additionally, because retaining the full history trace through time is biologically\nimplausible, it is unlikely that the brain implements BPTT [19] .\nFortunately, if a recurrent neural network converges to a fixed point, we can avoid unrolling its state\nsequence by applying backpropagation in a single step at that equilibrium point. Moreover, such a\nmechanism could plausibly be implemented in the brain using only local learning rules [34][,][35] . Based\n5\non this finding, we propose a one-step approximation of the HRM gradient–using the gradient of\nthe last state of each module and treating other states as constant. The gradient path is, therefore,\nOutput head → final state of the H-module → final state of the L-module → input embedding\nThe above method needs _O_ (1) memory, does not require unrolling through time, and can be easily\nimplemented with an autograd framework such as PyTorch, as shown in Figure 4. Given that\neach module only needs to back-propagate errors through its most recent local synaptic activity,\nthis approach aligns well with the perspective that cortical credit assignment relies on short-range,\ntemporally local mechanisms rather than on a global replay of activity patterns.\nThe one-step gradient approximation is theoretically\ngrounded in the mathematics of Deep Equilibrium Models (DEQ) [36] which employs the Implicit Function Theorem (IFT) to bypass BPTT, as detailed next. Consider an\nidealized HRM behavior where, during high-level cycle\n_k_, the L-module repeatedly updates until its state _z_ _L_ converges to a local fixed point _z_ _L_ _[⋆]_ [. This fixed point, given]\nthe current high-level state _z_ _H_ _[k][−]_ [1] [, can be expressed as]\n_z_ _L_ _[⋆]_ [=] _[ f]_ _[L]_ [(] _[z]_ _L_ _[⋆]_ _[, z]_ _H_ _[k][−]_ [1] _[,]_ [ ˜] _[x]_ [;] _[ θ]_ _[L]_ [)] _[ .]_\nThe H-module then performs a single update using this\nconverged L-state:\n_z_ _H_ _[k]_ [=] _[ f]_ _[H]_ [(] _[z]_ _H_ _[k][−]_ [1] _[, z]_ _L_ _[⋆]_ [;] _[ θ]_ _[H]_ [)] _[ .]_\nWith a proper mapping _F_, the updates to the high-level\nstate can be written in a more compact form as _z_ _H_ _[k]_ [=]\n_F_ ( _z_ _H_ _[k][−]_ [1] [; ˜] _[x, θ]_ [)][, where] _[ θ]_ [ = (] _[θ]_ _[I]_ _[, θ]_ _[L]_ [)][, and the fixed-point]\n_∂F_\ncan be written as _z_ _H_ _[⋆]_ [=] _[ F]_ [(] _[z]_ _H_ _[⋆]_ [; ˜] _[x, θ]_ [)][. Let] _[ J]_ _[F]_ [ =] _∂z_ _H_ [be]\nthe Jacobian of _F_, and assume that the matrix _I −_ _J_ _F_ is\ninvertible at _z_ _H_ _[⋆]_ [and that the mapping] _[ F]_ [ is continuously]\ndifferentiable. The Implicit Function Theorem then allows us to calculate the exact gradient of fixed point _z_ _H_ _[⋆]_\nwith respect to the parameters _θ_ without explicit backpropagation:\n```\ndef hrm(z, x, N=2, T=2):\nx = input_embedding(x)\nzH, zL = z\nwith torch.no_grad():\nfor _i in range(N ∗T −1):\nzL = L_net(zL, zH, x)\nif (_i + 1) % T == 0:\nzH = H_net(zH, zL)", "level": 3, "page_numbers": [], "subsections": [], "references": [], "equations": [], "images": [], "tables": []}, {"section_id": "section_3", "title": "1 − step grad", "content": "zL = L_net(zL, zH, x)\nzH = H_net(zH, zL)\nreturn (zH, zL), output_head(zH)", "level": 1, "page_numbers": [], "subsections": [], "references": [], "equations": [], "images": [], "tables": []}, {"section_id": "section_4", "title": "Deep Supervision", "content": "for x, y_true in train_dataloader:\nz = z_init\nfor step in range(N_supervision):\nz, y_hat = hrm(z, x)\nloss = softmax_cross_entropy(y_hat, y_true)\nz = z.detach()\nloss.backward()\nopt.step()\nopt.zero_grad()\n```\nFigure 4: **Top:** Diagram of HRM with\napproximate gradient. **Bottom:** Pseudocode of HRM with deep supervision\ntraining in PyTorch.\n_._ (1)\n���� _z_ _H_ _[⋆]_\n_∂z∂θ_ _H_ _[⋆]_ [=] � _I −_ _J_ _F_ �� _z_ _H_ _[⋆]_\n� _−_ 1 _∂∂θF_\nCalculating the above gradient requires evaluating and inverting matrix ( _I −_ _J_ _F_ ) that can be computationally expensive. Given the Neumann series expansion,\n( _I −_ _J_ _F_ ) _[−]_ [1] = _I_ + _J_ _F_ + _J_ _F_ [2] [+] _[ J]_ _F_ [3] [+] _[ . . .,]_\nthe so-called _1-step gradient_ [37] approximates the series by considering only its first term, i.e. ( _I −_\n_J_ _F_ ) _[−]_ [1] _≈_ _I_, and leads to the following approximation of Equation (1):\n_∂z_ _H_ _[∗]_\n_≈_ _[∂][f]_ _[H]_\n_∂θ_ _H_ _∂θ_ _H_\n_∂z_ _H_ _[∗]_\n_[∂]_ _∂θ_ _[f]_ _H_ _[H]_ _,_ _∂θ_ _L_ _≈_ _[∂]_ _∂z_ _[f]_ _[H][∗]_\n_[∂]_ _∂z_ _[f]_ _[H]_ _L_ _[∗]_ _·_ _∂θ_ _[∂z]_ _LL_ _[∗]_ _,_ _∂z∂θ_ _H_ _[∗]_ _I_ _≈_ _[∂]_ _∂z_ _[f]_ _[H]_ _L_ _[∗]_\n_[∂][f]_ _[H]_ _·_ _[∂z]_ _L_ _[∗]_ _._ (2)\n_∂z_ _L_ _[∗]_ _∂θ_ _I_\n6\nThe gradients of the low-level fixed point, _∂θ∂z_ _LL_ _[∗]_ [and] _∂z∂θ_ _L_ _[∗]_ _I_ [, can also be approximated using another]\napplication of the 1-step gradient:\n_∂z_ _L_ _[∗]_\n_≈_ _[∂][f]_ _[L]_\n_∂θ_ _L_ _∂θ_ _L_\n_∂z_ _L_ _[∗]_\n_[∂]_ _∂θ_ _[f]_ _L_ _[L]_ _,_ _∂θ_ _I_ _≈_ _[∂]_ _∂θ_ _[f]_ _[L]_ _I_\n_._ (3)\n_∂θ_ _I_\nBy substituting Equation (3) back into Equation (2), we arrive at the final simplified gradients.\nBefore defining our loss function, we must first introduce two key elements of our proposed\nmethod: _deep supervision_ and _adaptive computational time_ .\n**Deep supervision** Inspired by the principle that periodic neural oscillations regulate when learning\noccurs in the brain [38], we incorporate a deep supervision mechanism into HRM, as detailed next.\nGiven a data sample ( _x, y_ ), we run multiple forward passes of the HRM model, each of which we\nrefer to as a _segment_ . Let _M_ denote the total number of segments executed before termination.\nFor each segment _m ∈{_ 1 _, . . ., M_ _}_, let _z_ _[m]_ = ( _z_ _H_ _[mNT]_ _, z_ _L_ _[mNT]_ ) represent the hidden state at the\nconclusion of segment _m_, encompassing both high-level and low-level state components.\nAt each segment _m_, we apply a deep supervision step as follows:\n1. Given the state _z_ _[m][−]_ [1] from the previous segment, compute the next state _z_ _[m]_ and its associated\noutput ˆ _y_ _[m]_ through a forward pass in the HRM model:\n( _z_ _[m]_ _,_ ˆ _y_ _[m]_ ) _←_ HRM( _z_ _[m][−]_ [1] _, x_ ; _θ_ )\n2. Compute the loss for the current segment:\n_L_ _[m]_ _←_ L OSS (ˆ _y_ _[m]_ _, y_ )\n3. Update parameters:\n_θ ←_ O PTIMIZER S TEP ( _θ, ∇_ _θ_ _L_ _[m]_ )\nThe crucial aspect of this procedure is that the hidden state _z_ _[m]_ is “detached” from the computation graph before being used as the input state for the next segment. Consequently, gradients from\nsegment _m_ + 1 do not propagate back through segment _m_, effectively creating a 1-step approximation of the gradient of the recursive deep supervision process [39][,][40] . This approach provides more\nfrequent feedback to the H-module and serves as a regularization mechanism, demonstrating superior empirical performance and enhanced stability in deep equilibrium models when compared to\nmore complex, Jacobian-based regularization techniques [39][,][41] . Figure 4 shows pseudocode of deep\nsupervision training.\n**Adaptive computational time (ACT)** The brain dynamically alternates between automatic thinking (“System 1”) and deliberate reasoning (“System 2”) [42] . Neuroscientific evidence shows that\nthese cognitive modes share overlapping neural circuits, particularly within regions such as the\nprefrontal cortex and the default mode network [43][,][44] . This indicates that the brain dynamically modulates the “runtime” of these circuits according to task complexity and potential rewards [45][,][46] .\nInspired by the above mechanism, we incorporate an adaptive halting strategy into HRM that enables “thinking, fast and slow”. This integration leverages deep supervision and uses the Q-learning\n7\nalgorithm [47] to adaptively determine the number of segments. A Q-head uses the final state of the\nH-module to predict the Q-values _Q_ [ˆ] _[m]_ = ( _Q_ [ˆ] _[m]_ halt _[,]_ [ ˆ] _[Q]_ continue _[m]_ [)][ of the “halt” and “continue” actions:]\nˆ\n_Q_ _[m]_ = _σ_ ( _θ_ _Q_ _[⊤]_ _[z]_ _H_ _[mNT]_ ) _,_\nwhere _σ_ denotes the sigmoid function applied element-wise. The halt or continue action is chosen\nusing a randomized strategy as detailed next. Let _M_ max denote the maximum number of segments\n(a fixed hyperparameter) and _M_ min denote the minimum number of segments (a random variable).\nThe value of _M_ min is determined stochastically: with probability _ε_, it is sampled uniformly from the\nset _{_ 2 _, · · ·, M_ max _}_ (to encourage longer thinking), and with probability 1 _−_ _ε_, it is set to 1. The halt\naction is selected under two conditions: when the segment count surpasses the maximum threshold\n_M_ max, or when the estimated halt value _Q_ [ˆ] halt exceeds the estimated continue value _Q_ [ˆ] continue and the\nsegment count has reached at least the minimum threshold _M_ min .\nThe Q-head is updated through a Q-learning algorithm, which is defined on the following episodic\nMarkov Decision Process (MDP). The state of the MDP at segment _m_ is _z_ _[m]_, and the action space\nis _{_ halt _,_ continue _}_ . Choosing the action “halt” terminates the episode and returns a binary reward\nˆ\nindicating prediction correctness, i.e., **1** _{y_ _[m]_ = _y}_ . Choosing “continue” yields a reward of 0 and\nthe state transitions to _z_ _[m]_ [+1] . Thus, the Q-learning targets for the two actions _G_ [ˆ] _[m]_ = ( _G_ [ˆ] _[m]_ halt _[,]_ [ ˆ] _[G]_ _[m]_ continue [)]\nare given by\n_G_ ˆ _[m]_ halt [=] **[ 1]** _[{][y]_ [ˆ] _[m]_ [ =] _[ y][}][,]_\n_G_ ˆ _[m]_ continue [=]\n\n\n\nˆ\n_Q_ _[m]_ halt [+1] _[,]_ if _m ≥_ _N_ max _,_\nmax( _Q_ [ˆ] _[m]_ halt [+1] _[,]_ [ ˆ] _[Q]_ _[m]_ continue [+1] [)] _[,]_ otherwise _._\nWe can now define the loss function of our learning procedure. The overall loss for each supervision\nsegment combines both the Q-head loss and the sequence-to-sequence loss:\n_L_ _[m]_ ACT [=][ L] [OSS] [(ˆ] _[y]_ _[m]_ _[, y]_ [) +][ B] [INARY] [C] [ROSS] [E] [NTROPY] [( ˆ] _[Q]_ _[m]_ _[,]_ [ ˆ] _[G]_ _[m]_ [)] _[ .]_\nMinimizing the above loss enables both accurate predictions and nearly optimal stopping decisions.\nSelecting the “halt” action ends the supervision loop. In practice, sequences are processed in\nbatches, which can be easily handled by substituting any halted sample in the batch with a fresh\nsample from the dataloader.\nFigure 5 presents a performance comparison between two HRM variants: one incorporating ACT\nand another employing a fixed computational step count equivalent to ACT’s _M_ max parameter. It\nshows that ACT effectively adapts its computational resources based on task complexity, achieving\nsignificant computational savings with minimal impact on performance.\n**Inference-time scaling** An effective neural model should exploit additional computational resources during inference to enhance performance. As illustrated in Figure 5-(c), HRM seamlessly\nachieves inference-time scaling by simply increasing the computational limit parameter, _M_ max\nwithout requiring further training or architectural modifications.\nAdditional compute is especially effective for tasks that demand deeper reasoning. On Sudoku—\na problem that often requires long-term planning—HRM exhibits strong inference-time scaling.\nOn the other hand, we find that extra computational resources yield minimal gains in ARC-AGI\nchallenge, as solutions generally require only a few transformations.\n8\n|7 8|(a) ACT Compute Spent Fixed M ACT ( limit)|\n|---|---|\n|3<br>4<br>5<br>6<br>|ACT (Mmax limit)|\n|2<br>4<br>8<br>1<br>2|2<br>4<br>8<br>1<br>2|\nFigure 5: **Effectiveness of Adaptive Computation Time (ACT)** on the _Sudoku-Extreme-Full_ . **(a)**\nMean compute steps used by models with ACT versus models with a fixed number of compute steps\n( _M_ ). ACT maintains a low and stable number of average compute steps even as the maximum limit\n( _M_ max ) increases. **(b)** Accuracy comparison. The ACT model achieves performance comparable\nto the fixed-compute model while utilizing substantially fewer computational steps on average. **(c)**\nInference-time scalability. Models trained with a specific _M_ max can generalize to higher computational limits during inference, leading to improved accuracy. For example, a model trained with\n_M_ max = 8 continues to see accuracy gains when run with _M_ max = 16 during inference.\n**Stability of Q-learning in ACT** The deep Q-learning that underpins our ACT mechanism is\nknown to be prone to instability, often requiring stabilization techniques such as replay buffers\nand target networks [48], which are absent in our design. Our approach, however, achieves stability\nthrough the intrinsic properties of our model and training procedure. Recent theoretical work by\nGallici et al. [49] shows that Q-learning can achieve convergence if network parameters are bounded,\nweight decay is incorporated during training, and post-normalization layers are implemented. Our\nmodel satisfies these conditions through its Post-Norm architecture that employs RMSNorm (a\nlayer normalization variant) and the AdamW optimizer. AdamW has been shown to solve an _L_ _∞_ constrained optimization problem, ensuring that model parameters remain bounded by 1 _/λ_ [50] .\n**Architectural details** We employ a sequence-to-sequence architecture for HRM. Both input and\noutput are represented as token sequences: _x_ = ( _x_ 1 _, . . ., x_ _l_ ) and _y_ = ( _y_ 1 _, . . ., y_ _l_ _′_ ) respectively.\nThe model includes an embedding layer _f_ _I_ that converts discrete tokens into vector representations, and an output head _f_ _O_ ( _z_ ; _θ_ _O_ ) = softmax( _θ_ _O_ _z_ ) that transforms hidden states into token probability distributions ˆ _y_ . For small-sample experiments, we replace softmax with stablemax [51] to\nimprove generalization performance. The sequence-to-sequence loss is averaged over all tokens,\n_l_ _′_\nL OSS (ˆ _y, y_ ) = [1] _[′]_ � _i_ =1 [log] _[ p]_ [(] _[y]_ _[i]_ [)][, where] _[ p]_ [(] _[y]_ _[i]_ [)][ is the probability that distribution][ ˆ] _[y]_ _[i]_ [ assigns to token]\n_l_ [1] _[′]_ � _li_ _′_\nL OSS (ˆ _y, y_ ) = _l_ [1] _[′]_ � _i_ =1 [log] _[ p]_ [(] _[y]_ _[i]_ [)][, where] _[ p]_ [(] _[y]_ _[i]_ [)][ is the probability that distribution][ ˆ] _[y]_ _[i]_ [ assigns to token]\n_y_ _i_ . The initial hidden states _z_ [0] are initialized by sampling from a truncated normal distribution with\nstandard deviation of 1, truncation of 2, and kept fixed throughout training.\nBoth the low-level and high-level recurrent modules _f_ _L_ and _f_ _H_ are implemented using encoderonly Transformer [52] blocks with identical architectures and dimensions. These modules take multiple inputs, and we use straightforward element-wise addition to combine them, though more\nsophisticated merging techniques such as gating mechanisms could potentially improve performance and is left for future work. For all Transformer blocks in this work—including those in\nthe baseline models—we incorporate the enhancements found in modern LLMs (based on Llama [53]\narchitectures). These improvements include Rotary Positional Encoding [54], Gated Linear Units [55],\nRMSNorm [56], and the removal of bias terms from linear layers.\nFurthermore, both HRM and recurrent Transformer models implement a Post-Norm architecture\n9\n|Col1|8|4|Col4|Col5|5|Col7|6|Col9|\n|---|---|---|---|---|---|---|---|---|\n|||||8||7|||\n|3||||4|||||\n||3|8|4||||2||\n|||6|||3|||8|\n|9||||||||6|\n||||5||||||\n||||||2|||1|\n||2|5||3|||8||\n|8|4|1|2|5|9|6|\n|---|---|---|---|---|---|---|\n|6|1|3|8|9|7|4|\n|5|9|6|4|7|8|1|\n|3|8|4|9|6|1|2|\n|1|6|2|7|3|5|9|\n|7|2|8|5|1|4|3|\n|9|3|5|1|8|2|7|\n|4|7|9|6|2|3|5|\n(a) ARC-AGI\n1 2 5 7 3 4 6 8 9\n(b) Sudoku-Hard (c) Maze navigation (d) _Sudoku-Extreme_ subset difficulty\nFigure 6: **Left:** Visualization of benchmark tasks. **Right:** Difficulty of _Sudoku-Extreme_ examples.\nwith weights initialized via truncated LeCun Normal initialization [57][,][58][,][59], while the scale and bias\nparameters are excluded from RMSNorm. All parameters are optimized using the Adam-atan2 optimizer [60], a scale-invariant variant of Adam [61], combined with a constant learning rate that includes\nlinear warm-up.", "level": 1, "page_numbers": [], "subsections": [], "references": [], "equations": [], "images": [], "tables": []}, {"section_id": "section_5", "title": "**3 Results**", "content": "This section begins by describing the ARC-AGI, Sudoku, and Maze benchmarks, followed by an\noverview of the baseline models and their results. Figure 6-(a,b,c) presents a visual representation of the three benchmark tasks, which are selected to evaluate various reasoning abilities in AI\nmodels.", "level": 3, "page_numbers": [], "subsections": [], "references": [], "equations": [], "images": [], "tables": []}, {"section_id": "section_6", "title": "**3.1 Benchmarks**", "content": "**ARC-AGI Challenge** The ARC-AGI benchmark evaluates general fluid intelligence through IQtest-like puzzles that require inductive reasoning [27] . The initial version, ARC-AGI-1, presents challenges as input-output grid pairs that force AI systems to extract and generalize abstract rules from\njust a few examples. Each task provides a few input–output example pairs (usually 2–3) and a test\ninput. An AI model has two attempts to produce the correct output grid. Although some believe\nthat mastering ARC-AGI would signal true artificial general intelligence, its primary purpose is\nto expose the current roadblocks in AGI progress. In fact, both conventional deep learning methods and CoT techniques have faced significant challenges with ARC-AGI-1, primarily because it\nrequires the ability to generalize to entirely new tasks [28] .\nAddressing the limitations identified in ARC-AGI-1, ARC-AGI-2 significantly expands the benchmark by providing a more comprehensive and carefully refined collection of tasks. These new\ntasks emphasize deeper compositional reasoning, multi-step logic, contextual rule application, and\nsymbolic abstraction. Human calibration studies show these tasks are challenging but doable for\npeople, while being much harder for current AI systems, offering a clearer measure of general\nreasoning abilities [29] .\n10\n**Sudoku-Extreme** Sudoku is a 9 _×_ 9 logic puzzle, requiring each row, column, and 3 _×_ 3 block to\ncontain the digits 1–9 exactly once. A prediction is considered correct if it exactly matches the\npuzzle’s unique solution. Sudoku’s complex logical structure makes it a popular benchmark for\nevaluating logical reasoning in machine learning [62][,][63][,][64] .\nThe most frequently used Sudoku dataset in research, namely the Kaggle dataset [65], can be fully\nsolved using elementary single-digit techniques [66] . The minimal 17-clue puzzles [62], another widelyused collection, might seem more challenging due to its small number of clues. However, this\nperception is misleading—since 17 represents the minimum number of clues required to guarantee\na unique Sudoku solution, these hints need to be highly orthogonal to each other. This orthogonal\narrangement leads to many direct, easily-resolved solution paths [67] .\nWe introduce _Sudoku-Extreme_, a more challenging dataset that is compiled from the aforementioned easy datasets as well as puzzles recognized by the Sudoku community as exceptionally\ndifficult for human players:\n- Easy puzzles compiled from Kaggle, 17-clue, plus unbiased samples from the Sudoku puzzle\ndistribution [67] : totaling 1 149 158 puzzles.\n- Challenging puzzles compiled from Magictour 1465, Forum-Hard and Forum-Extreme subsets:\ntotaling 3 104 157 puzzles.\nThe compiled data then undergo a strict 90/10 train-test split, ensuring that the test set puzzles\ncannot be derived through equivalent transformations of any training samples. _Sudoku-Extreme_ is\na down-sampled subset of this data containing 1000 training examples. We use _Sudoku-Extreme_ in\nour main experiments (Figure 1), which focuses on small-sample learning scenarios. To guarantee\nconvergence and control overfitting effects in our analysis experiments (Figures 2, 3 and 5), we use\nthe complete training data, _Sudoku-Extreme-Full_, containing 3 831 994 examples.\nWe measure puzzle difficulty by counting the number of search backtracks (“guesses”) required\nby a smart Sudoku solver program _tdoku_, which uses propositional logic to reduce the number of\nguesses [67] . Our _Sudoku-Extreme_ dataset exhibits a mean difficulty of 22 backtracks per puzzle, significantly higher than existing datasets, including recent handmade puzzles Sudoku-Bench [68] which\naverage just 0 _._ 45 backtracks per puzzle. These subset complexity levels are shown in Figure 6-(d).\n**Maze-Hard** This task involves finding the optimal path in a 30 _×_ 30 maze, making it interpretable\nand frequently used for training LLMs in search tasks [69][,][70][,][71] . We adopt the instance generation\nprocedure of Lehnert et al. [71], but introduce an additional filter to retain only those instances whose\ndifficulty exceeds 110. Here, “difficulty” is defined as the length of the shortest path, which aligns\nwith the linear time complexity of the wavefront breadth-first search algorithm on GPUs [72] . A path\nis considered correct if it is valid and optimal—that is, the shortest route from the start to the goal.\nThe training and test set both include 1000 examples.", "level": 4, "page_numbers": [], "subsections": [], "references": [], "equations": [], "images": [], "tables": []}, {"section_id": "section_7", "title": "**3.2 Evaluation Details**", "content": "For all benchmarks, HRM models were initialized with random weights and trained in the sequenceto-sequence setup using the input-output pairs. The two-dimensional input and output grids were\nflattened and then padded to the maximum sequence length. The resulting performance is shown in\nFigure 1. Remarkably, HRM attains these results with just ~1000 training examples per task—and\n**without pretraining or CoT labels** .\n11\nFor ARC-AGI challenge, we start with all input-output example pairs in the training and the evaluation sets. The dataset is augmented by applying translations, rotations, flips, and color permutations\nto the puzzles. Each task example is prepended with a learnable special token that represents the\npuzzle it belongs to. At test time, we proceed as follows for each test input in the evaluation set: (1)\nGenerate and solve 1000 augmented variants and, for each, apply the inverse-augmentation transform to obtain a prediction. (2) Choose the two most popular predictions as the final outputs. [3] All\nresults are reported on the evaluation set.\nWe augment Sudoku puzzles by applying band and digit permutations, while data augmentation is\ndisabled for Maze tasks. Both tasks undergo only a single inference pass.\nFor ARC-AGI, the scores of the CoT models are taken from the official leaderboard [29], while for\nSudoku and Maze, the scores are obtained by evaluating through the corresponding API.\nIn Figure 1, the baselines are grouped based on whether they are pre-trained and use CoT, or neither.\nThe “Direct pred” baseline means using “direct prediction without CoT and pre-training”, which\nretains the exact training setup of HRM but swaps in a Transformer architecture. Interestingly, on\nARC-AGI-1, “Direct pred” matches the performance of Liao and Gu [73], who built a carefully designed, domain-specific equivariant network for learning the ARC-AGI task from scratch, without\npre-training. By substituting the Transformer architecture with HRM’s hierarchical framework and\nimplementing ACT, we achieve more than a twofold performance improvement.\nOn the _Sudoku-Extreme_ and _Maze-Hard_ benchmarks, the performance gap between HRM and the\nbaseline methods is significant, as the baselines almost never manage to solve the tasks. These\nbenchmarks that demand lengthy reasoning traces are particularly difficult for CoT-based methods.\nWith only 1000 training examples, the “Direct pred” baseline—which employs an 8-layer Transformer identical in size to HRM—fails entirely on these challenging reasoning problems. When\ntrained on the larger _Sudoku-Extreme-Full_ dataset, however, “Direct pred” can solve some easy\nSudoku puzzles and reaches 16 _._ 9% accuracy (see Figure 2). Lehnert et al. [71] showed that a large\nvanilla Transformer model with 175M parameters, trained on 1 million examples across multiple\ntrials, achieved only marginal success on 30x30 Maze tasks, with accuracy below 20% using the\n_pass_ @64 evaluation metric.", "level": 4, "page_numbers": [], "subsections": [], "references": [], "equations": [], "images": [], "tables": []}, {"section_id": "section_8", "title": "**3.3 Visualization of intermediate timesteps**", "content": "Although HRM demonstrates strong performance on complex reasoning tasks, it raises an intriguing question: what underlying reasoning algorithms does the HRM neural network actually implement? Addressing this question is important for enhancing model interpretability and developing a\ndeeper understanding of the HRM solution space.\nWhile a definitive answer lies beyond our current scope, we begin our investigation by analyzing\nstate trajectories and their corresponding solution evolution. More specifically, at each timestep\n_i_ and given the low-level and high-level state pair ( _z_ _L_ _[i]_ [and] _[ z]_ _H_ _[i]_ [) we perform a preliminary forward]\npass through the H-module to obtain ¯ _z_ _[i]_ = _f_ _H_ ( _z_ _H_ _[i]_ _[, z]_ _L_ _[i]_ [;] _[ θ]_ _[H]_ [)][ and its corresponding decoded prediction]\n_y_ ¯ _[i]_ = _f_ _O_ (¯ _z_ _[i]_ ; _θ_ _O_ ). The prediction ¯ _y_ _[i]_ is then visualized in Figure 7.\nIn the Maze task, HRM appears to initially explore several potential paths simultaneously, subsequently eliminating blocked or inefficient routes, then constructing a preliminary solution outline\nfollowed by multiple refinement iterations. In Sudoku, the strategy resembles a depth-first search\n3 The ARC-AGI allows two attempts for each test input.\n12\nFigure 7: **Visualization of intermediate predictions by HRM on benchmark tasks. Top:** _Maze-_\n_Hard_ —blue cells indicate the predicted path. **Middle:** _Sudoku-Extreme_ —bold cells represent initial givens; red highlights cells violating Sudoku constraints; grey shading indicates changes from\nthe previous timestep. **Bottom:** ARC-AGI-2 Task—left: provided example input-output pair; right:\nintermediate steps solving the test input.\napproach, where the model appears to explore potential solutions and backtracks when it hits dead\nends. HRM uses a different approach for ARC tasks, making incremental adjustments to the board\nand iteratively improving it until reaching a solution. Unlike Sudoku, which involves frequent\nbacktracking, the ARC solution path follows a more consistent progression similar to hill-climbing\noptimization.\nImportantly, the model shows that it can adapt to different reasoning approaches, likely choosing an\neffective strategy for each particular task. Further research is needed to gain more comprehensive\ninsights into these solution strategies.", "level": 4, "page_numbers": [], "subsections": [], "references": [], "equations": [], "images": [], "tables": []}, {"section_id": "section_9", "title": "**4 Brain Correspondence**", "content": "A key principle from systems neuroscience is that a brain region’s functional repertoire—its ability\nto handle diverse and complex tasks—is closely linked to the dimensionality of its neural representations [75][,][76] . Higher-order cortical areas, responsible for complex reasoning and decision-making,\nmust handle a wide variety of tasks, demanding more flexible and context-dependent processing [77] .\nIn dynamical systems, this flexibility is often realized through higher-dimensional state-space trajectories, which allow for a richer repertoire of potential computations [78] . This principle gives rise\nto an observable _dimensionality hierarchy_, where a region’s position in the processing hierarchy\ncorrelates with its _effective dimensionality_ . To quantify this phenomenon, we can examine the\n13\n(e)\n(f)\n(a)\n(b)\n(c)\n5.0\n4.5\n4.0\n3.5\n3.0\n2.5\n2.0\n0 20 40\nPosition in the hierarchy\nFigure 8: **Hierarchical Dimensionality Organization in the HRM and Mouse Cortex.** (a,b) are\nadapted from Posani et al. [74] . (a) Anatomical illustration of mouse cortical areas, color-coded by\nfunctional modules. (b) Correlation between Participation Ratio (PR), a measure of effective neural\ndimensionality, and hierarchical position across different mouse cortical areas. Higher positions in\nthe hierarchy (e.g., MOs, ACAd) exhibit significantly higher PR values compared to lower sensory\nareas (e.g., SSp-n), with a Spearman correlation coefficient of _ρ_ = 0.79 (P = 0.0003). (c,d) **Trained**\n**HRM.** (c) PR scaling of the trained HRM with task diversity. The dimensionality of the highlevel module ( _z_ _H_ ) scales with the number of unique tasks (trajectories) included in the analysis,\nindicating an adaptive expansion of its representational capacity. In contrast, the low-level module’s\n( _z_ _L_ ) dimensionality remains stable. (d) PR values for the low-level ( _z_ _L_, PR = 30.22) and highlevel ( _z_ _H_, PR = 89.95) modules of the _trained_ HRM, computed from neural activity during 100\nunique Sudoku-solving trajectories. A clear dimensionality hierarchy is observed, with the highlevel module operating in a substantially higher-dimensional space. (e,f) **Analysis of Untrained**\n**Network.** To verify that the dimensionality hierarchy is an emergent property of training, the same\nanalyses were performed on an _untrained_ HRM with random weights. (e) In contrast to the trained\nmodel’s scaling in (c), the dimensionality of both modules in the untrained model remains low and\nstable, failing to scale with the number of tasks. (f) Similarly, contrasting with the clear separation\nin (d), the PR values for the untrained model’s modules ( _z_ _L_, PR = 42.09; _z_ _H_, PR = 40.75) are\nlow and nearly identical, showing no evidence of hierarchical separation. This confirms that the\nobserved hierarchical organization of dimensionality is a learned property that emerges through\ntraining, not an artifact of the model’s architecture.\n14\nParticipation Ratio (PR), which serves as a standard measure of the effective dimensionality of a\nhigh-dimensional representation [79] . The PR is calculated using the formula\n[�] _i_ _[λ]_ _[i]_ [)] [2]\n~~�~~ _i_ _[λ]_ _i_ [2]\nPR = [(][�]\n_,_\n_i_ _[λ]_ _i_ [2]\nwhere _{λ_ _i_ _}_ are the eigenvalues of the covariance matrix of neural trajectories. Intuitively, a higher\nPR value signifies that variance is distributed more evenly across many dimensions, corresponding\nto a higher-dimensional representation. Conversely, a lower PR value indicates that variance is\nconcentrated in only a few principal components, reflecting a more compact, lower-dimensional\nstructure.\nThe dimensionality hierarchy can be observed, for example, in the mouse cortex, where the PR of\npopulation activity increases monotonically from low-level sensory areas to high-level associative\nareas, supporting this link between dimensionality and functional complexity [74] (Figure 8 (a,b)).\nWe evaluated whether HRM reproduces this neuroscientific principle by calculating the PR for\nboth recurrent modules after training on the _Sudoku-Extreme Full_ dataset. The PR computation\nused the covariance matrix derived from neural states gathered across multiple Sudoku-solving\ntrajectories. The results show a striking parallel to the biological findings. The low-level module’s\nstate ( _z_ _L_ ) occupies a relatively small subspace with a participation ratio of 30.22, whereas the highlevel module’s state ( _z_ _H_ ) operates in a substantially larger subspace with a participation ratio of\n89.95, as shown in Figure 8(c). Furthermore, Figure 8(d) shows that increasing the number of\nunique tasks (trajectories) from 10 to 100 causes _z_ _H_ dimensionality to scale up accordingly, while\n_z_ _L_ dimensionality remains stable. These results suggest an _emergent_ separation of representational\ncapacity between the modules that parallels their functional roles.\nTo confirm that this hierarchical organization is an emergent property of training, and not an artifact\nof the network’s architecture, we performed a control analysis using an identical but untrained\nnetwork with random weights.\nWe initialized an identical HRM architecture with random weights and, without any training, measured the PR of its modules as the network processed the same task-specific inputs given to the\ntrained model.\nThe results, shown in Figure 8(e,f), reveal a stark contrast: the high-level and low-level modules of\nthe untrained network exhibit no hierarchical separation, with their PR values remaining low and\nnearly indistinguishable from each other. This control analysis validates that the dimensionality\nhierarchy is an _emergent property_ that arises as the model learns to perform complex reasoning.\nThe high-to-low PR ratio in HRM ( _z_ _H_ _/z_ _L_ _≈_ 2 _._ 98) closely matches that measured in the mouse\ncortex ( _≈_ 2 _._ 25). In contrast, conventional deep networks often exhibit _neural collapse_, where\nlast-layer features converge to a low-dimensional subspace [80][,][81][,][82] . HRM therefore departs from the\ncollapse pattern and instead fosters a high-dimensional representation in its higher module. This\nis significant because such representations are considered crucial for cognitive flexibility and are a\nhallmark of higher-order brain regions like the prefrontal cortex (PFC), which is central to complex\nreasoning.\nThis structural parallel suggests the model has discovered a fundamental organizational principle.\nBy learning to partition its representations into a high-capacity, high-dimensional subspace ( _z_ _H_ )\nand a more specialized, low-dimensional one ( _z_ _L_ ), HRM autonomously discovers an organizational\n15\nprinciple that is thought to be fundamental for achieving robust and flexible reasoning in biological\nsystems. This provides a potential mechanistic explanation for the model’s success on complex,\nlong-horizon tasks that are intractable for models lacking such a differentiated internal structure.\nWe emphasize, however, that this evidence is correlational. While a causal link could be tested\nvia intervention (e.g., by constraining the H-module’s dimensionality), such methods are difficult\nto interpret in deep learning due to potential confounding effects on the training process itself.\nThus, the causal necessity of this emergent hierarchy remains an important question for future\ninvestigation.", "level": 3, "page_numbers": [], "subsections": [], "references": [], "equations": [], "images": [], "tables": []}, {"section_id": "section_10", "title": "**5 Related Work**", "content": "**Reasoning and algorithm learning** Given the central role of reasoning problems and their close\nrelation to algorithms, researchers have long explored neural architectures that enable algorithm\nlearning from training instances. This line of work includes Neural Turing Machines (NTM) [83],\nthe Differentiable Neural Computer (DNC) [84], and Neural GPUs [85] –all of which construct iterative\nneural architectures that mimic computational hardware for algorithm execution, and are trained to\nlearn algorithms from data. Another notable work in this area is Recurrent Relational Networks\n(RRN) [62], which executes algorithms on graph representations through graph neural networks.\nRecent studies have integrated algorithm learning approaches with Transformer-based architectures. Universal Transformers extend the standard Transformer model by introducing a recurrent\nloop over the layers and implementing an adaptive halting mechanism. Geiping et al. [86] demonstrate\nthat looped Transformers can generalize to a larger number of recurrent steps during inference than\nwhat they were trained on. Shen et al. [16] propose adding continuous recurrent reasoning tokens\nto the Transformer. Finally, TransNAR [8] combine recurrent graph neural networks with language\nmodels.\nBuilding on the success of CoT-based reasoning, a line of work have introduced fine-tuning methods that use reasoning paths from search algorithms (like A*) as SFT targets [87][,][71][,][70] .\nWe also mention adaptive halting mechanisms designed to allocate additional computational resources to more challenging problems. This includes the Adaptive Computation Time (ACT) for\nRNNs [88] and follow-up research like PonderNet [89], which aims to improve the stability of this allocation process.\nHRM further pushes the boundary of algorithm learning through a brain-inspired computational\narchitecture that achieves exceptional data efficiency and model expressiveness, successfully discovering complex and diverse algorithms from just 1000 training examples.\n**Brain-inspired reasoning architectures** Developing a model with the reasoning power of the\nbrain has long been a goal in brain-inspired computing. Spaun [90] is one notable example, which uses\nspiking neural networks to create distinct modules corresponding to brain regions like the visual\ncortex and prefrontal cortex. This design enables an architecture to perform a range of cognitive\ntasks, from memory recall to simple reasoning puzzles. However, its reasoning relies on handdesigned algorithms, which may limit its ability to learn new tasks. Another significant model is the\nTolman-Eichenbaum Machine (TEM) [91], which is inspired by the hippocampal-entorhinal system’s\nrole in spatial and relational memory tasks. TEM proposes that medial entorhinal cells create a\nbasis for structural knowledge, while hippocampal cells link this basis to sensory information. This\nallows TEM to generalize and explains the emergence of various cell types like grid, border, and\n16\nplace cells. Another approach involves neural sampling models [92], which view the neural signaling\nprocess as inference over a distribution, functioning similarly to a Boltzmann machine. These\nmodels often require hand-made rules to be set up for solving a specific reasoning task. In essence,\nwhile prior models are restricted to simple reasoning problems, HRM is designed to solve complex\ntasks that are hard for even advanced LLMs, without pre-training or task-specific manual design.\n**Hierarchical memory** The hierarchical multi-timescale structure also plays an important role in\nhow the brain processes memory. Models such as Hierarchical Sequential Models [93] and Clockwork\nRNN [94] use multiple recurrent modules that operate at varying time scales to more effectively capture long-range dependencies within sequences, thereby mitigating the forgetting issue in RNNs.\nSimilar mechanisms have also been adopted in linear attention methods for memorizing long contexts (see the Discussions section). Since HRM focuses on reasoning, full attention is applied for\nsimplicity. Incorporating hierarchical memory into HRM could be a promising future direction.", "level": 3, "page_numbers": [], "subsections": [], "references": [], "equations": [], "images": [], "tables": []}, {"section_id": "section_11", "title": "**6 Discussions**", "content": "**Turing-completeness of HRM** Like earlier neural reasoning algorithms including the Universal\nTransformer [95], HRM is computationally universal when given sufficient memory and time constraints. In other words, it falls into the category of models that can simulate any Turing machine,\novercoming the computational limitations of standard Transformers discussed previously in the introduction. Given that earlier neural algorithm reasoners were trained as recurrent neural networks,\nthey suffer from premature convergence and memory intensive BPTT. Therefore, in practice, their\neffective computational depth remains limited, though still deeper than that of a standard Transformer. By resolving these two challenges and being equipped with adaptive computation, HRM\ncould be trained on long reasoning processes, solve complex puzzles requiring intensive depth-first\nsearch and backtracking, and move closer to practical Turing-completeness.\n**Reinforcement learning with chain-of-thought** Beyond fine-tuning using human-annotated CoT,\nreinforcement learning (RL) represents another widely adopted training methodology. However,\nrecent evidence suggests that RL primarily unlocks existing CoT-like capabilities rather than discovering fundamentally new reasoning mechanisms [96][,][97][,][98][,][99] . Additionally, CoT-training with RL\nis known for its instability and data inefficiency, often requiring extensive exploration and careful\nreward design. In contrast, HRM takes feedback from dense gradient-based supervision rather than\nrelying on a sparse reward signal. Moreover, HRM operates naturally in a continuous space, which\nis biologically plausible and avoids allocating same computational resources to each token, even\nthough tokens vary in their reasoning and planning complexity [16] .\n**Linear attention** Recurrence has been explored not only for its capability in universal computation, but also as a means to replace the attention mechanism in Transformers, which suffers from\nquadratic time and memory complexity [100] . Recurrent alternatives offer a more efficient design by\nprocessing input tokens sequentially and predicting the next token at each time step, similar to early\nRNN-based language models.\nSome linear-attention variants, such as Log-linear Attention [101], share an RNN-like state-update that\ncan be interpreted as propagating multi-timescale summary statistics, thereby retaining long-range\ncontext without the quadratic memory growth of standard self-attention. However, substituting the\nattention mechanism alone does not change the fact that Transformers are still fixed-depth, and\nrequire CoT as a compensatory mechanism. Notably, linear attention can operate with a reduced\n17\nkey-value cache over extended contexts, making them more suitable for deployment on resourceconstrained edge devices.", "level": 3, "page_numbers": [], "subsections": [], "references": [], "equations": [], "images": [], "tables": []}, {"section_id": "section_12", "title": "**7 Conclusion**", "content": "This work introduces the Hierarchical Reasoning Model, a brain-inspired architecture that leverages hierarchical structure and multi-timescale processing to achieve substantial computational\ndepth without sacrificing training stability or efficiency. With only 27M parameters and training on just 1000 examples, HRM effectively solves challenging reasoning problems such as ARC,\nSudoku, and complex maze navigation–tasks that typically pose significant difficulties for contemporary LLM and chain-of-thought models.\nAlthough the brain relies heavily on hierarchical structures to enable most cognitive processes,\nthese concepts have largely remained confined to academic literature rather than being translated\ninto practical applications. The prevailing AI approach continues to favor non-hierarchical models.\nOur results challenge this established paradigm and suggest that the Hierarchical Reasoning Model\nrepresents a viable alternative to the currently dominant chain-of-thought reasoning methods, advancing toward a foundational framework capable of Turing-complete universal computation.\n**Acknowledgements** We thank Mingli Yuan, Ahmed Murtadha Hasan Mahyoub and Hengshuai\nYao for their insightful discussions and valuable feedback throughout the course of this work.\n18", "level": 3, "page_numbers": [], "subsections": [], "references": [], "equations": [], "images": [], "tables": []}, {"section_id": "section_13", "title": "**References**", "content": "1. Ian Goodfellow, Yoshua Bengio, and Aaron Courville. _Deep Learning_ . MIT Press, 2016.\n`[http://www.deeplearningbook.org](http://www.deeplearningbook.org)` .\n2. Kaiming He, X. Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image\nrecognition. _2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_,\npages 770–778, 2015.\n3. Lena Strobl. Average-hard attention transformers are constant-depth uniform threshold\ncircuits, 2023.\n4. Tom Bylander. Complexity results for planning. In _Proceedings of the 12th International Joint_\n_Conference on Artificial Intelligence - Volume 1_, IJCAI’91, page 274–279, San Francisco,\nCA, USA, 1991. Morgan Kaufmann Publishers Inc. ISBN 1558601600.\n5. William Merrill and Ashish Sabharwal. A logic for expressing log-precision transformers. In\n_Neural Information Processing Systems_, 2023.\n6. David Chiang. Transformers in DLOGTIME-uniform TC [0] . _Transactions on Machine_\n_Learning Research_, 2025.\n7. Lucas Lehnert, Sainbayar Sukhbaatar, DiJia Su, Qinqing Zheng, Paul McVay, Michael\nRabbat, and Yuandong Tian. Beyond a*: Better planning with transformers via search\ndynamics bootstrapping. In _First Conference on Language Modeling_, 2024.\n8. Wilfried Bounsi, Borja Ibarz, Andrew Dudzik, Jessica B. Hamrick, Larisa Markeeva, Alex\nVitvitskyi, Razvan Pascanu, and Petar Velivckovi’c. Transformers meet neural algorithmic\nreasoners. _ArXiv_, abs/2406.09308, 2024.\n9. William Merrill and Ashish Sabharwal. The parallelism tradeoff: Limitations of log-precision\ntransformers. _Transactions of the Association for Computational Linguistics_, 11:531–545,\n2023. doi: 10.1162/tacl_a_00562.\n10. Jason Wei, Yi Tay, et al. Chain-of-thought prompting elicits reasoning in large language\nmodels, 2022. arXiv preprint arXiv:2201.11903.\n11. William Merrill and Ashish Sabharwal. The expressive power of transformers with chain of\nthought. In _ICLR_, 2024.\n12. Xinyun Chen, Ryan A. Chi, Xuezhi Wang, and Denny Zhou. Premise order matters in\nreasoning with large language models. _ArXiv_, abs/2402.08939, 2024.\n13. Rongwu Xu, Zehan Qi, and Wei Xu. Preemptive answer \"attacks\" on chain-of-thought\nreasoning. In _Annual Meeting of the Association for Computational Linguistics_, 2024.\n14. Pablo Villalobos, Anson Ho, Jaime Sevilla, Tamay Besiroglu, Lennart Heim, and Marius\nHobbhahn. Will we run out of data? limits of llm scaling based on human-generated data.\n_arXiv preprint arXiv:2211.04325_, 2022.\n15. Xinghao Chen, Anhao Zhao, Heming Xia, Xuan Lu, Hanlin Wang, Yanjun Chen, Wei Zhang,\nJian Wang, Wenjie Li, and Xiaoyu Shen. Reasoning beyond language: A comprehensive\nsurvey on latent chain-of-thought reasoning, 2025.\n16. Xuan Shen, Yizhou Wang, Xiangxi Shi, Yanzhi Wang, Pu Zhao, and Jiuxiang Gu.\nTraining large language models to reason in a continuous latent space. _arXiv preprint_\n_arXiv:2412.07423_, 2024.\n19\n17. Evelina Fedorenko, Steven T Piantadosi, and Edward AF Gibson. Language is primarily a\ntool for communication rather than thought. _Nature_, 630(8017):575–586, 2024.\n18. Hongyu Wang, Shuming Ma, Li Dong, Shaohan Huang, Dongdong Zhang, and Furu Wei.\nDeepnet: Scaling transformers to 1,000 layers. _IEEE Transactions on Pattern Analysis and_\n_Machine Intelligence_, 2024.\n19. Timothy P Lillicrap and Adam Santoro. Backpropagation through time and the brain. _Current_\n_Opinion in Neurobiology_, 55:82–89, 2019. ISSN 0959-4388. doi: https://doi.org/10.1016/j.\nconb.2019.01.011.\n20. John D Murray, Alberto Bernacchia, David J Freedman, Ranulfo Romo, Jonathan D Wallis,\nXinying Cai, Camillo Padoa-Schioppa, Tatiana Pasternak, Hyojung Seo, Daeyeol Lee, et al.\nA hierarchy of intrinsic timescales across primate cortex. _Nature neuroscience_, 17(12):1661–\n1663, 2014.\n21. Roxana Zeraati, Yan-Liang Shi, Nicholas A Steinmetz, Marc A Gieselmann, Alexander\nThiele, Tirin Moore, Anna Levina, and Tatiana A Engel. Intrinsic timescales in the\nvisual cortex change with selective attention and reflect spatial connectivity. _Nature_\n_communications_, 14(1):1858, 2023.\n22. Julia M Huntenburg, Pierre-Louis Bazin, and Daniel S Margulies. Large-scale gradients in\nhuman cortical organization. _Trends in cognitive sciences_, 22(1):21–31, 2018.\n23. Victor AF Lamme and Pieter R Roelfsema. The distinct modes of vision offered by\nfeedforward and recurrent processing. _Trends in neurosciences_, 23(11):571–579, 2000.\n24. Andre M Bastos, W Martin Usrey, Rick A Adams, George R Mangun, Pascal Fries, and Karl J\nFriston. Canonical microcircuits for predictive coding. _Neuron_, 76(4):695–711, 2012.\n25. Klara Kaleb, Barbara Feulner, Juan Gallego, and Claudia Clopath. Feedback control guides\ncredit assignment in recurrent neural networks. _Advances in Neural Information Processing_\n_Systems_, 37:5122–5144, 2024.\n26. Timothy P Lillicrap, Adam Santoro, Luke Marris, Colin J Akerman, and Geoffrey Hinton.\nBackpropagation and the brain. _Nature Reviews Neuroscience_, 21(6):335–346, 2020.\n27. François Chollet. On the measure of intelligence (abstraction and reasoning corpus), 2019.\narXiv preprint arXiv:1911.01547.\n28. Francois Chollet, Mike Knoop, Gregory Kamradt, and Bryan Landers. Arc prize 2024:\nTechnical report. _ArXiv_, abs/2412.04604, 2024.\n29. Francois Chollet, Mike Knoop, Gregory Kamradt, Bryan Landers, and Henry Pinkard. Arcagi-2: A new challenge for frontier ai reasoning systems. _arXiv preprint arXiv:2505.11831_,\n2025.\n30. György Buzsáki. Gamma, alpha, delta, and theta oscillations govern cognitive processes.\n_International Journal of Psychophysiology_, 39:241–248, 2000.\n31. György Buzsáki. _Rhythms of the Brain_ . Oxford university press, 2006.\n32. Anja Pahor and Norbert Jaušovec. Theta–gamma cross-frequency coupling relates to the level\nof human intelligence. _Intelligence_, 46:283–290, 2014.\n33. Adriano BL Tort, Robert W Komorowski, Joseph R Manns, Nancy J Kopell, and Howard\nEichenbaum. Theta–gamma coupling increases during the learning of item–context\nassociations. _Proceedings of the National Academy of Sciences_, 106(49):20942–20947, 2009.\n20\n34. Benjamin Scellier and Yoshua Bengio. Equilibrium propagation: Bridging the gap between\nenergy-based models and backpropagation. _Frontiers in Computational Neuroscience_, 11,\n2016.\n35. Guillaume Bellec, Franz Scherr, Anand Subramoney, Elias Hajek, Darjan Salaj, Robert\nLegenstein, and Wolfgang Maass. A solution to the learning dilemma for recurrent\nnetworks of spiking neurons. _Nature Communications_, 11, 07 2020. doi: 10.1038/\ns41467-020-17236-y.\n36. Shaojie Bai, J Zico Kolter, and Vladlen Koltun. Deep equilibrium models. In _Advances in_\n_Neural Information Processing Systems_, pages 690–701, 2019.\n37. Zhengyang Geng, Xinyu Zhang, Shaojie Bai, Yisen Wang, and Zhouchen Lin. On training\nimplicit models. _ArXiv_, abs/2111.05177, 2021.\n38. Katarina Begus and Elizabeth Bonawitz. The rhythm of learning: Theta oscillations as an\nindex of active learning in infancy. _Developmental Cognitive Neuroscience_, 45:100810, 2020.\nISSN 1878-9293. doi: https://doi.org/10.1016/j.dcn.2020.100810.\n39. Shaojie Bai, Zhengyang Geng, Yash Savani, and J. Zico Kolter. Deep Equilibrium\nOptical Flow Estimation . In _2022 IEEE/CVF Conference on Computer Vision and Pattern_\n_Recognition (CVPR)_, pages 610–620, 2022.\n40. Zaccharie Ramzi, Florian Mannel, Shaojie Bai, Jean-Luc Starck, Philippe Ciuciu, and\nThomas Moreau. Shine: Sharing the inverse estimate from the forward pass for bi-level\noptimization and implicit models. _ArXiv_, abs/2106.00553, 2021.\n41. Shaojie Bai, Vladlen Koltun, and J. Zico Kolter. Stabilizing equilibrium models by jacobian\nregularization. In _International Conference on Machine Learning_, 2021.\n42. Daniel Kahneman and P Egan. Thinking, fast and slow (farrar, straus and giroux, new york),\n2011.\n43. Matthew D Lieberman. Social cognitive neuroscience: a review of core processes. _Annu. Rev._\n_Psychol._, 58(1):259–289, 2007.\n44. Randy L Buckner, Jessica R Andrews-Hanna, and Daniel L Schacter. The brain’s default\nnetwork: anatomy, function, and relevance to disease. _Annals of the new York Academy of_\n_Sciences_, 1124(1):1–38, 2008.\n45. Marcus E Raichle. The brain’s default mode network. _Annual review of neuroscience_, 38(1):\n433–447, 2015.\n46. Andrew Westbrook and Todd S Braver. Cognitive effort: A neuroeconomic approach.\n_Cognitive, Affective, & Behavioral Neuroscience_, 15:395–415, 2015.\n47. Richard S. Sutton and Andrew G. Barto. _Reinforcement Learning: An Introduction_ . MIT\nPress, Cambridge, MA, 2018.\n48. Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan\nWierstra, and Martin A. Riedmiller. Playing atari with deep reinforcement learning. _ArXiv_,\nabs/1312.5602, 2013.\n49. Matteo Gallici, Mattie Fellows, Benjamin Ellis, Bartomeu Pou, Ivan Masmitja,\nJakob Nicolaus Foerster, and Mario Martin. Simplifying deep temporal difference learning,\n2025.\n21\n50. Shuo Xie and Zhiyuan Li. Implicit bias of adamw: L inf norm constrained optimization.\n_ArXiv_, abs/2404.04454, 2024.\n51. Lucas Prieto, Melih Barsbey, Pedro A. M. Mediano, and Tolga Birdal. Grokking at the edge of\nnumerical stability. In _The Thirteenth International Conference on Learning Representations_,\n2025.\n52. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\nŁukasz Kaiser, and Illia Polosukhin. Attention is all you need. In _Advances in neural_\n_information processing systems_, pages 5998–6008, 2017.\n53. Meta AI. Llama 3: State-of-the-art open weight language models. Technical report, Meta,\n2024. URL `[https://ai.meta.com/llama/](https://ai.meta.com/llama/)` .\n54. Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer:\nEnhanced transformer with rotary position embedding. _Neurocomputing_, 568:127063, 2024.\n55. Noam M. Shazeer. Glu variants improve transformer. _ArXiv_, abs/2002.05202, 2020.\n56. Biao Zhang and Rico Sennrich. Root mean square layer normalization. _ArXiv_,\nabs/1910.07467, 2019.\n57. Günter Klambauer, Thomas Unterthiner, Andreas Mayr, and Sepp Hochreiter. Selfnormalizing neural networks. In _Neural Information Processing Systems_, 2017.\n58. JAX Developers. _jax.nn.initializers.lecun_normal_ . Google Research, 2025. URL\n```\nhttps://docs.jax.dev/en/latest/_autosummary/jax.nn.initializers.lecun_\n```\n`[normal.html](https://docs.jax.dev/en/latest/_autosummary/jax.nn.initializers.lecun_normal.html)` . Accessed June 22, 2025.\n59. Yann LeCun, Léon Bottou, Genevieve B Orr, and Klaus-Robert Müller. Efficient backprop.\nIn _Neural networks: Tricks of the trade_, pages 9–50. Springer, 2002.\n60. Katie E Everett, Lechao Xiao, Mitchell Wortsman, Alexander A Alemi, Roman Novak,\nPeter J Liu, Izzeddin Gur, Jascha Sohl-Dickstein, Leslie Pack Kaelbling, Jaehoon Lee, and\nJeffrey Pennington. Scaling exponents across parameterizations and optimizers. In _Forty-first_\n_International Conference on Machine Learning_, 2024.\n61. Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization, 2017.\n62. Rasmus Berg Palm, Ulrich Paquet, and Ole Winther. Recurrent relational networks. In _Neural_\n_Information Processing Systems_, 2017.\n63. Jieyi Long. Large language model guided tree-of-thought. _ArXiv_, abs/2305.08291, 2023.\n64. Yilun Du, Jiayuan Mao, and Josh Tenenbaum. Learning iterative reasoning through energy\ndiffusion. _ArXiv_, abs/2406.11179, 2024.\n65. Kyubyong Park. Can convolutional neural networks crack sudoku puzzles? `[https:](https://github.com/Kyubyong/sudoku)`\n`[//github.com/Kyubyong/sudoku](https://github.com/Kyubyong/sudoku)`, 2018.\n66. Single-digit techniques. `[https://hodoku.sourceforge.net/en/tech_singles.php](https://hodoku.sourceforge.net/en/tech_singles.php)` .\nAccessed: 2025-06-16.\n67. Tom Dillion. Tdoku: A fast sudoku solver and generator. `[https://t-dillon.github.io/](https://t-dillon.github.io/tdoku/)`\n`[tdoku/](https://t-dillon.github.io/tdoku/)`, 2025.\n68. Jeffrey Seely, Yuki Imajuku, Tianyu Zhao, Edoardo Cetin, and Llion Jones. Sudoku-bench:\nEvaluating creative reasoning with sudoku variants. _arXiv preprint arXiv:2505.16135_, 2025.\n69. Luke Darlow, Ciaran Regan, Sebastian Risi, Jeffrey Seely, and Llion Jones. Continuous\nthought machines. _arXiv preprint arXiv:2505.05522_, 2025.\n22\n70. DiJia Su, Sainbayar Sukhbaatar, Michael Rabbat, Yuandong Tian, and Qinqing Zheng.\nDualformer: Controllable fast and slow thinking by learning with randomized reasoning\ntraces, 2025.\n71. Lucas Lehnert, Sainbayar Sukhbaatar, DiJia Su, Qinqing Zheng, Paul McVay, Michael\nRabbat, and Yuandong Tian. Beyond a*: Better planning with transformers via search\ndynamics bootstrapping. In _First Conference on Language Modeling_, 2024.\n72. Mubbasir Kapadia, Francisco Garcia, Cory D. Boatright, and Norman I. Badler. Dynamic\nsearch on the gpu. In _2013 IEEE/RSJ International Conference on Intelligent Robots and_\n_Systems_, pages 3332–3337, 2013. doi: 10.1109/IROS.2013.6696830.\n73. Isaac Liao and Albert Gu. Arc-agi without pretraining, 2025. URL `[https:](https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/arc_agi_without_pretraining.html)`\n```\n//iliao2345.github.io/blog_posts/arc_agi_without_pretraining/arc_agi_\n```\n`[without_pretraining.html](https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/arc_agi_without_pretraining.html)` .\n74. Lorenzo Posani, Shuqi Wang, Samuel P Muscinelli, Liam Paninski, and Stefano Fusi.\nRarely categorical, always high-dimensional: how the neural code changes along the cortical\nhierarchy. _bioRxiv_, pages 2024–11, 2025.\n75. Mattia Rigotti, Omri Barak, Melissa R. Warden, Xiao-Jing Wang, Nathaniel D. Daw, Earl K.\nMiller, and Stefano Fusi. The importance of mixed selectivity in complex cognitive tasks.\n_Nature_, 497:585–590, 2013. doi: 10.1038/nature12160.\n76. Valerio Mante, David Sussillo, Krishna V. Shenoy, and William T. Newsome. Contextdependent computation by recurrent dynamics in prefrontal cortex. _Nature_, 503(7474):78–84,\n2013. doi: 10.1038/nature12742.\n77. Earl K. Miller and Jonathan D. Cohen. An integrative theory of prefrontal cortex function.\n_Annual Review of Neuroscience_, 24(1):167–202, 2001. doi: 10.1146/annurev.neuro.24.1.167.\n78. Wolfgang Maass. Real-time computing without stable states: a new framework for neural\ncomputation based on perturbations. _Neural Computation_, 14(11):2531–2560, 2002. doi:\n10.1162/089976602760407955.\n79. Ege Altan, Sara A. Solla, Lee E. Miller, and Eric J. Perreault. Estimating the dimensionality\nof the manifold underlying multi-electrode neural recordings. _PLoS Computational Biology_,\n17(11):e1008591, 2021. doi: 10.1371/journal.pcbi.1008591.\n80. Vardan Papyan, X. Y. Han, and David L. Donoho. Prevalence of neural collapse during the\nterminal phase of deep learning training. _Proceedings of the National Academy of Sciences_,\n117(40):24652–24663, 2020. doi: 10.1073/pnas.2015509117.\n81. Cong Fang, Hangfeng He, Qi Long, and Weijie J. Su. Exploring deep neural networks via\nlayer–peeled model: Minority collapse in imbalanced training. _Proceedings of the National_\n_Academy of Sciences_, 118(43):e2103091118, 2021. doi: 10.1073/pnas.2103091118.\n82. Zhihui Zhu, Tianyu Ding, Jinxin Zhou, Xiao Li, Chong You, Jeremias Sulam, and Qing Qu.\nA geometric analysis of neural collapse with unconstrained features. In _Advances in Neural_\n_Information Processing Systems_, volume 34 of _NeurIPS_, pages 29820–29834, 2021.\n83. Alex Graves, Greg Wayne, and Ivo Danihelka. Neural turing machines, 2014.\n84. Alex Graves, Greg Wayne, Malcolm Reynolds, Tim Harley, Ivo Danihelka, Agnieszka\nGrabska-Barwi´nska, Sergio Gómez Colmenarejo, Edward Grefenstette, Tiago Ramalho, John\nAgapiou, et al. Hybrid computing using a neural network with dynamic external memory.\n_Nature_, 538(7626):471–476, 2016.\n23\n85. Lukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In _ICLR_, 2016.\n86. Jonas Geiping, Sean McLeish, Neel Jain, John Kirchenbauer, Siddharth Singh, Brian R.\nBartoldson, Bhavya Kailkhura, Abhinav Bhatele, and Tom Goldstein. Scaling up test-time\ncompute with latent reasoning: A recurrent depth approach, 2025.\n87. Tiedong Liu and Kian Hsiang Low. Goat: Fine-tuned llama outperforms gpt-4 on arithmetic\ntasks. _ArXiv_, abs/2305.14201, 2023.\n88. Alex Graves. Adaptive computation time for recurrent neural networks. _ArXiv_,\nabs/1603.08983, 2016.\n89. Andrea Banino, Jan Balaguer, and Charles Blundell. Pondernet: Learning to ponder. _ArXiv_,\nabs/2107.05407, 2021.\n90. Chris Eliasmith, Terrence C Stewart, Xuan Choo, Trevor Bekolay, Travis DeWolf, Yichuan\nTang, and Daniel Rasmussen. A large-scale model of the functioning brain. _science_, 338\n(6111):1202–1205, 2012.\n91. James CR Whittington, Timothy H Muller, Shirley Mark, Guifen Chen, Caswell Barry, Neil\nBurgess, and Timothy EJ Behrens. The tolman-eichenbaum machine: unifying space and\nrelational memory through generalization in the hippocampal formation. _Cell_, 183(5):1249–\n1263, 2020.\n92. Lars Buesing, Johannes Bill, Bernhard Nessler, and Wolfgang Maass. Neural dynamics as\nsampling: a model for stochastic computation in recurrent networks of spiking neurons. _PLoS_\n_computational biology_, 7(11):e1002211, 2011.\n93. Salah Hihi and Yoshua Bengio. Hierarchical recurrent neural networks for long-term\ndependencies. In D. Touretzky, M.C. Mozer, and M. Hasselmo, editors, _Advances in Neural_\n_Information Processing Systems_, volume 8. MIT Press, 1995.\n94. Jan Koutník, Klaus Greff, Faustino J. Gomez, and Jürgen Schmidhuber. A clockwork rnn. In\n_International Conference on Machine Learning_, 2014.\n95. Mostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob Uszkoreit, and Lukasz Kaiser.\nUniversal transformers, 2018. arXiv preprint arXiv:1807.03819.\n96. Yiping Wang, Qing Yang, Zhiyuan Zeng, Liliang Ren, Lucas Liu, Baolin Peng, Hao Cheng,\nXuehai He, Kuan Wang, Jianfeng Gao, Weizhu Chen, Shuohang Wang, Simon Shaolei Du,\nand Yelong Shen. Reinforcement learning for reasoning in large language models with one\ntraining example, 2025. URL `[https://arxiv.org/abs/2504.20571](https://arxiv.org/abs/2504.20571)` .\n97. Niklas Muennighoff. s1: Simple test-time scaling. _arXiv preprint arXiv:2502.23456_, 2025.\n98. Liang Wen, Yunke Cai, Fenrui Xiao, Xin He, Qi An, Zhenyu Duan, Yimin Du, Junchen Liu,\nLifu Tang, Xiaowei Lv, Haosheng Zou, Yongchao Deng, Shousheng Jia, and Xiangzheng\nZhang. Light-r1: Curriculum sft, dpo and rl for long cot from scratch and beyond, 2025.\n99. Xuefeng Li, Haoyang Zou, and Pengfei Liu. Limr: Less is more for rl scaling, 2025.\n100. Tri Dao and Albert Gu. Transformers are ssms: Generalized models and efficient algorithms\nthrough structured state space duality. _ArXiv_, abs/2405.21060, 2024.\n101. Han Guo, Songlin Yang, Tarushii Goel, Eric P Xing, Tri Dao, and Yoon Kim. Log-linear\nattention. _arXiv preprint arXiv:2506.04761_, 2025.\n24", "level": 3, "page_numbers": [], "subsections": [], "references": [], "equations": [], "images": [], "tables": []}], "images": [{"image_id": "img_4_0", "page_number": 5, "bbox": [105.40464782714844, 193.83468627929688, 107.69548034667969, 241.41343688964844], "image_path": "", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 13, "height": 270, "format": "PNG", "file_size": 1085}, {"image_id": "img_4_1", "page_number": 5, "bbox": [107.69548034667969, 194.36334228515625, 109.98631286621094, 240.88479614257812], "image_path": "", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 13, "height": 264, "format": "PNG", "file_size": 986}, {"image_id": "img_4_2", "page_number": 5, "bbox": [261.35723876953125, 193.83468627929688, 263.6480712890625, 241.41343688964844], "image_path": "", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 13, "height": 270, "format": "PNG", "file_size": 954}, {"image_id": "img_4_3", "page_number": 5, "bbox": [417.3097839355469, 193.83468627929688, 419.6006164550781, 241.41343688964844], "image_path": "", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 13, "height": 270, "format": "PNG", "file_size": 1224}, {"image_id": "img_9_0", "page_number": 10, "bbox": [299.616455078125, 67.16817474365234, 378.818603515625, 68.36820983886719], "image_path": "", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 826}, {"image_id": "img_9_1", "page_number": 10, "bbox": [299.616455078125, 68.36820220947266, 378.818603515625, 69.5682373046875], "image_path": "", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 618}, {"image_id": "img_9_2", "page_number": 10, "bbox": [299.616455078125, 79.16849517822266, 378.818603515625, 80.3685302734375], "image_path": "", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 778}, {"image_id": "img_9_3", "page_number": 10, "bbox": [299.616455078125, 200.75363159179688, 378.818603515625, 201.95367431640625], "image_path": "", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 401}, {"image_id": "img_9_4", "page_number": 10, "bbox": [299.616455078125, 201.9536590576172, 378.818603515625, 203.15370178222656], "image_path": "", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 1361}, {"image_id": "img_9_5", "page_number": 10, "bbox": [299.616455078125, 203.1536865234375, 378.818603515625, 204.35372924804688], "image_path": "", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 453}, {"image_id": "img_9_6", "page_number": 10, "bbox": [299.616455078125, 204.35372924804688, 378.818603515625, 205.55377197265625], "image_path": "", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 1529}, {"image_id": "img_9_7", "page_number": 10, "bbox": [299.616455078125, 205.55377197265625, 378.818603515625, 206.75381469726562], "image_path": "", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 423}, {"image_id": "img_9_8", "page_number": 10, "bbox": [299.616455078125, 206.7537841796875, 378.818603515625, 207.95382690429688], "image_path": "", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 1384}, {"image_id": "img_9_9", "page_number": 10, "bbox": [299.616455078125, 207.95382690429688, 378.818603515625, 209.15386962890625], "image_path": "", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 415}, {"image_id": "img_9_10", "page_number": 10, "bbox": [299.616455078125, 209.1538543701172, 378.818603515625, 210.35389709472656], "image_path": "", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 1379}, {"image_id": "img_9_11", "page_number": 10, "bbox": [299.616455078125, 210.35389709472656, 378.818603515625, 211.55393981933594], "image_path": "", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 383}, {"image_id": "img_9_12", "page_number": 10, "bbox": [299.616455078125, 211.55392456054688, 378.818603515625, 212.75396728515625], "image_path": "", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 1367}, {"image_id": "img_9_13", "page_number": 10, "bbox": [299.616455078125, 80.36853790283203, 378.818603515625, 81.56857299804688], "image_path": "", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 821}, {"image_id": "img_9_14", "page_number": 10, "bbox": [299.616455078125, 212.7539520263672, 378.818603515625, 213.95399475097656], "image_path": "", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 415}, {"image_id": "img_9_15", "page_number": 10, "bbox": [299.616455078125, 213.95401000976562, 378.818603515625, 215.154052734375], "image_path": "", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 1403}, {"image_id": "img_9_16", "page_number": 10, "bbox": [299.616455078125, 215.15402221679688, 378.818603515625, 216.35406494140625], "image_path": "", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 420}, {"image_id": "img_9_17", "page_number": 10, "bbox": [299.616455078125, 216.35406494140625, 378.818603515625, 217.55410766601562], "image_path": "", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 1483}, {"image_id": "img_9_18", "page_number": 10, "bbox": [299.616455078125, 217.55409240722656, 378.818603515625, 218.75413513183594], "image_path": "", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 412}, {"image_id": "img_9_19", "page_number": 10, "bbox": [299.616455078125, 218.75411987304688, 378.818603515625, 219.95416259765625], "image_path": "", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 1425}, {"image_id": "img_9_20", "page_number": 10, "bbox": [299.616455078125, 219.9541473388672, 378.818603515625, 221.15419006347656], "image_path": "", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 432}, {"image_id": "img_9_21", "page_number": 10, "bbox": [299.616455078125, 221.15419006347656, 378.818603515625, 222.35423278808594], "image_path": "", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 1301}, {"image_id": "img_9_22", "page_number": 10, "bbox": [299.616455078125, 222.35421752929688, 378.818603515625, 223.55426025390625], "image_path": "", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 408}, {"image_id": "img_9_23", "page_number": 10, "bbox": [299.616455078125, 223.5542755126953, 378.818603515625, 224.20883178710938], "image_path": "", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 3, "format": "PNG", "file_size": 753}, {"image_id": "img_9_24", "page_number": 10, "bbox": [299.616455078125, 81.56856536865234, 378.818603515625, 82.76860046386719], "image_path": "", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 820}, {"image_id": "img_9_25", "page_number": 10, "bbox": [73.57389831542969, 67.22271728515625, 121.35700988769531, 75.18656921386719], "image_path": "", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 219, "height": 37, "format": "PNG", "file_size": 9937}, {"image_id": "img_9_26", "page_number": 10, "bbox": [73.57389831542969, 75.18656921386719, 121.35700988769531, 83.15042114257812], "image_path": "", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 219, "height": 37, "format": "PNG", "file_size": 9974}, {"image_id": "img_9_27", "page_number": 10, "bbox": [73.57389831542969, 83.15043640136719, 121.35700988769531, 91.11428833007812], "image_path": "", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 219, "height": 37, "format": "PNG", "file_size": 8809}, {"image_id": "img_9_28", "page_number": 10, "bbox": [73.57389831542969, 91.11427307128906, 121.35700988769531, 99.078125], "image_path": "", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 219, "height": 37, "format": "PNG", "file_size": 8582}, {"image_id": "img_9_29", "page_number": 10, "bbox": [73.57389831542969, 99.078125, 121.35700988769531, 107.04197692871094], "image_path": "", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 219, "height": 37, "format": "PNG", "file_size": 8139}, {"image_id": "img_9_30", "page_number": 10, "bbox": [73.57389831542969, 107.0419921875, 121.35700988769531, 114.89674377441406], "image_path": "", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 219, "height": 36, "format": "PNG", "file_size": 8875}, {"image_id": "img_9_31", "page_number": 10, "bbox": [133.03005981445312, 67.22271728515625, 181.0313720703125, 75.18656921386719], "image_path": "", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 220, "height": 37, "format": "PNG", "file_size": 1794}, {"image_id": "img_9_32", "page_number": 10, "bbox": [133.03005981445312, 75.18656921386719, 181.0313720703125, 83.15042114257812], "image_path": "", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 220, "height": 37, "format": "PNG", "file_size": 9350}, {"image_id": "img_9_33", "page_number": 10, "bbox": [133.03005981445312, 83.15043640136719, 181.0313720703125, 91.11428833007812], "image_path": "", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 220, "height": 37, "format": "PNG", "file_size": 8114}, {"image_id": "img_9_34", "page_number": 10, "bbox": [133.03005981445312, 91.11427307128906, 181.0313720703125, 99.078125], "image_path": "", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 220, "height": 37, "format": "PNG", "file_size": 10181}, {"image_id": "img_9_35", "page_number": 10, "bbox": [299.616455078125, 82.76860809326172, 378.818603515625, 83.96864318847656], "image_path": "", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 891}, {"image_id": "img_9_36", "page_number": 10, "bbox": [133.03005981445312, 99.078125, 181.0313720703125, 107.04197692871094], "image_path": "", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 220, "height": 37, "format": "PNG", "file_size": 9533}, {"image_id": "img_9_37", "page_number": 10, "bbox": [133.03005981445312, 107.0419921875, 181.0313720703125, 114.89674377441406], "image_path": "", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 220, "height": 36, "format": "PNG", "file_size": 1659}, {"image_id": "img_9_38", "page_number": 10, "bbox": [73.57389831542969, 121.66056823730469, 121.35700988769531, 129.62442016601562], "image_path": "", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 219, "height": 37, "format": "PNG", "file_size": 2179}, {"image_id": "img_9_39", "page_number": 10, "bbox": [73.57389831542969, 129.62440490722656, 121.35700988769531, 137.5882568359375], "image_path": "", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 219, "height": 37, "format": "PNG", "file_size": 8481}, {"image_id": "img_9_40", "page_number": 10, "bbox": [73.57389831542969, 137.58827209472656, 121.35700988769531, 145.5521240234375], "image_path": "", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 219, "height": 37, "format": "PNG", "file_size": 2011}, {"image_id": "img_9_41", "page_number": 10, "bbox": [73.57389831542969, 145.55213928222656, 121.35700988769531, 153.5159912109375], "image_path": "", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 219, "height": 37, "format": "PNG", "file_size": 14098}, {"image_id": "img_9_42", "page_number": 10, "bbox": [73.57389831542969, 153.51597595214844, 121.35700988769531, 161.47982788085938], "image_path": "", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 219, "height": 37, "format": "PNG", "file_size": 10140}, {"image_id": "img_9_43", "page_number": 10, "bbox": [73.57389831542969, 161.47984313964844, 121.35700988769531, 169.3345947265625], "image_path": "", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 219, "height": 36, "format": "PNG", "file_size": 7653}, {"image_id": "img_9_44", "page_number": 10, "bbox": [133.03005981445312, 121.66056823730469, 181.0313720703125, 129.62442016601562], "image_path": "", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 220, "height": 37, "format": "PNG", "file_size": 1858}, {"image_id": "img_9_45", "page_number": 10, "bbox": [133.03005981445312, 129.62440490722656, 181.0313720703125, 137.5882568359375], "image_path": "", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 220, "height": 37, "format": "PNG", "file_size": 10685}, {"image_id": "img_9_46", "page_number": 10, "bbox": [299.616455078125, 83.96863555908203, 378.818603515625, 85.16867065429688], "image_path": "", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 611}, {"image_id": "img_9_47", "page_number": 10, "bbox": [133.03005981445312, 137.58827209472656, 181.0313720703125, 145.5521240234375], "image_path": "", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 220, "height": 37, "format": "PNG", "file_size": 10767}, {"image_id": "img_9_48", "page_number": 10, "bbox": [133.03005981445312, 145.55213928222656, 181.0313720703125, 153.5159912109375], "image_path": "", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 220, "height": 37, "format": "PNG", "file_size": 10776}, {"image_id": "img_9_49", "page_number": 10, "bbox": [133.03005981445312, 153.51597595214844, 181.0313720703125, 161.47982788085938], "image_path": "", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 220, "height": 37, "format": "PNG", "file_size": 2133}, {"image_id": "img_9_50", "page_number": 10, "bbox": [133.03005981445312, 161.47984313964844, 181.0313720703125, 169.3345947265625], "image_path": "", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 220, "height": 36, "format": "PNG", "file_size": 1971}, {"image_id": "img_9_51", "page_number": 10, "bbox": [73.57389831542969, 176.86207580566406, 121.35700988769531, 184.825927734375], "image_path": "", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 219, "height": 37, "format": "PNG", "file_size": 12851}, {"image_id": "img_9_52", "page_number": 10, "bbox": [73.57389831542969, 184.82591247558594, 121.35700988769531, 192.78976440429688], "image_path": "", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 219, "height": 37, "format": "PNG", "file_size": 8897}, {"image_id": "img_9_53", "page_number": 10, "bbox": [73.57389831542969, 192.78977966308594, 121.35700988769531, 200.75363159179688], "image_path": "", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 219, "height": 37, "format": "PNG", "file_size": 13152}, {"image_id": "img_9_54", "page_number": 10, "bbox": [73.57389831542969, 200.75363159179688, 121.35700988769531, 208.7174835205078], "image_path": "", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 219, "height": 37, "format": "PNG", "file_size": 11724}, {"image_id": "img_9_55", "page_number": 10, "bbox": [73.57389831542969, 208.7174835205078, 121.35700988769531, 216.68133544921875], "image_path": "", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 219, "height": 37, "format": "PNG", "file_size": 9699}, {"image_id": "img_9_56", "page_number": 10, "bbox": [73.57389831542969, 216.6813507080078, 121.35700988769531, 224.64520263671875], "image_path": "", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 219, "height": 37, "format": "PNG", "file_size": 10934}, {"image_id": "img_9_57", "page_number": 10, "bbox": [299.616455078125, 85.16866302490234, 378.818603515625, 86.36869812011719], "image_path": "", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 873}, {"image_id": "img_9_58", "page_number": 10, "bbox": [133.03005981445312, 176.86207580566406, 181.0313720703125, 184.825927734375], "image_path": "", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 220, "height": 37, "format": "PNG", "file_size": 494}, {"image_id": "img_9_59", "page_number": 10, "bbox": [133.03005981445312, 184.82591247558594, 181.0313720703125, 192.78976440429688], "image_path": "", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 220, "height": 37, "format": "PNG", "file_size": 727}, {"image_id": "img_9_60", "page_number": 10, "bbox": [133.03005981445312, 192.78977966308594, 181.0313720703125, 200.75363159179688], "image_path": "", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 220, "height": 37, "format": "PNG", "file_size": 1601}, {"image_id": "img_9_61", "page_number": 10, "bbox": [133.03005981445312, 200.75363159179688, 181.0313720703125, 208.7174835205078], "image_path": "", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 220, "height": 37, "format": "PNG", "file_size": 1134}, {"image_id": "img_9_62", "page_number": 10, "bbox": [133.03005981445312, 208.7174835205078, 181.0313720703125, 216.68133544921875], "image_path": "", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 220, "height": 37, "format": "PNG", "file_size": 313}, {"image_id": "img_9_63", "page_number": 10, "bbox": [133.03005981445312, 216.6813507080078, 181.0313720703125, 224.20883178710938], "image_path": "", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 220, "height": 35, "format": "PNG", "file_size": 397}, {"image_id": "img_9_64", "page_number": 10, "bbox": [405.8193359375, 64.65902709960938, 534.0592041015625, 226.82708740234375], "image_path": "", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 431, "height": 545, "format": "PNG", "file_size": 34955}, {"image_id": "img_9_65", "page_number": 10, "bbox": [299.616455078125, 86.36869049072266, 378.818603515625, 87.5687255859375], "image_path": "", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 614}, {"image_id": "img_9_66", "page_number": 10, "bbox": [299.616455078125, 87.56873321533203, 378.818603515625, 88.76876831054688], "image_path": "", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 932}, {"image_id": "img_9_67", "page_number": 10, "bbox": [299.616455078125, 88.76876068115234, 378.818603515625, 89.96879577636719], "image_path": "", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 602}, {"image_id": "img_9_68", "page_number": 10, "bbox": [299.616455078125, 89.96878814697266, 378.818603515625, 91.1688232421875], "image_path": "", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 920}, {"image_id": "img_9_69", "page_number": 10, "bbox": [299.616455078125, 69.56822967529297, 378.818603515625, 70.76826477050781], "image_path": "", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 865}, {"image_id": "img_9_70", "page_number": 10, "bbox": [299.616455078125, 91.16883087158203, 378.818603515625, 92.36886596679688], "image_path": "", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 413}, {"image_id": "img_9_71", "page_number": 10, "bbox": [299.616455078125, 92.36885833740234, 378.818603515625, 93.56889343261719], "image_path": "", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 946}, {"image_id": "img_9_72", "page_number": 10, "bbox": [299.616455078125, 93.56888580322266, 378.818603515625, 94.7689208984375], "image_path": "", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 608}, {"image_id": "img_9_73", "page_number": 10, "bbox": [299.616455078125, 94.76892852783203, 378.818603515625, 95.96896362304688], "image_path": "", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 975}, {"image_id": "img_9_74", "page_number": 10, "bbox": [299.616455078125, 95.96895599365234, 378.818603515625, 97.16899108886719], "image_path": "", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 402}, {"image_id": "img_9_75", "page_number": 10, "bbox": [299.616455078125, 97.16899871826172, 378.818603515625, 98.36903381347656], "image_path": "", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 1024}, {"image_id": "img_9_76", "page_number": 10, "bbox": [299.616455078125, 98.36901092529297, 378.818603515625, 99.56904602050781], "image_path": "", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 393}, {"image_id": "img_9_77", "page_number": 10, "bbox": [299.616455078125, 99.56905364990234, 378.818603515625, 100.76908874511719], "image_path": "", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 878}, {"image_id": "img_9_78", "page_number": 10, "bbox": [299.616455078125, 100.76909637451172, 378.818603515625, 101.96913146972656], "image_path": "", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 403}, {"image_id": "img_9_79", "page_number": 10, "bbox": [299.616455078125, 101.96912384033203, 378.818603515625, 103.16915893554688], "image_path": "", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 1064}, {"image_id": "img_9_80", "page_number": 10, "bbox": [299.616455078125, 70.7682876586914, 378.818603515625, 71.96832275390625], "image_path": "", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 555}, {"image_id": "img_9_81", "page_number": 10, "bbox": [299.616455078125, 103.16915130615234, 378.818603515625, 104.36918640136719], "image_path": "", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 408}, {"image_id": "img_9_82", "page_number": 10, "bbox": [299.616455078125, 104.36919403076172, 378.818603515625, 105.56922912597656], "image_path": "", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 1051}, {"image_id": "img_9_83", "page_number": 10, "bbox": [299.616455078125, 105.56922149658203, 378.818603515625, 106.76925659179688], "image_path": "", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 403}, {"image_id": "img_9_84", "page_number": 10, "bbox": [299.616455078125, 106.76924896240234, 378.818603515625, 107.96928405761719], "image_path": "", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 1050}, {"image_id": "img_9_85", "page_number": 10, "bbox": [299.616455078125, 107.96929168701172, 378.818603515625, 109.16932678222656], "image_path": "", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 408}, {"image_id": "img_9_86", "page_number": 10, "bbox": [299.616455078125, 109.16931915283203, 378.818603515625, 110.36935424804688], "image_path": "", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 1053}, {"image_id": "img_9_87", "page_number": 10, "bbox": [299.616455078125, 110.3693618774414, 378.818603515625, 111.56939697265625], "image_path": "", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 383}, {"image_id": "img_9_88", "page_number": 10, "bbox": [299.616455078125, 111.56937408447266, 378.818603515625, 112.7694091796875], "image_path": "", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 1229}, {"image_id": "img_9_89", "page_number": 10, "bbox": [299.616455078125, 112.7694320678711, 378.818603515625, 113.96946716308594], "image_path": "", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 395}, {"image_id": "img_9_90", "page_number": 10, "bbox": [299.616455078125, 113.96944427490234, 378.818603515625, 115.16947937011719], "image_path": "", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 1171}, {"image_id": "img_9_91", "page_number": 10, "bbox": [299.616455078125, 71.96829986572266, 378.818603515625, 73.1683349609375], "image_path": "", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 891}, {"image_id": "img_9_92", "page_number": 10, "bbox": [299.616455078125, 115.16948699951172, 378.818603515625, 116.36952209472656], "image_path": "", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 406}, {"image_id": "img_9_93", "page_number": 10, "bbox": [299.616455078125, 116.36949920654297, 378.818603515625, 117.56953430175781], "image_path": "", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 1278}, {"image_id": "img_9_94", "page_number": 10, "bbox": [299.616455078125, 117.5695571899414, 378.818603515625, 118.76959228515625], "image_path": "", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 434}, {"image_id": "img_9_95", "page_number": 10, "bbox": [299.616455078125, 118.76958465576172, 378.818603515625, 119.96961975097656], "image_path": "", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 1313}, {"image_id": "img_9_96", "page_number": 10, "bbox": [299.616455078125, 119.96961212158203, 378.818603515625, 121.16964721679688], "image_path": "", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 416}, {"image_id": "img_9_97", "page_number": 10, "bbox": [299.616455078125, 121.1696548461914, 378.818603515625, 122.36968994140625], "image_path": "", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 1274}, {"image_id": "img_9_98", "page_number": 10, "bbox": [299.616455078125, 122.36968231201172, 378.818603515625, 123.56971740722656], "image_path": "", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 404}, {"image_id": "img_9_99", "page_number": 10, "bbox": [299.616455078125, 123.56970977783203, 378.818603515625, 124.76974487304688], "image_path": "", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 1244}, {"image_id": "img_9_100", "page_number": 10, "bbox": [299.616455078125, 124.76973724365234, 378.818603515625, 125.96977233886719], "image_path": "", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 389}, {"image_id": "img_9_101", "page_number": 10, "bbox": [299.616455078125, 125.96977996826172, 378.818603515625, 127.16981506347656], "image_path": "", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 1262}, {"image_id": "img_9_102", "page_number": 10, "bbox": [299.616455078125, 73.16834259033203, 378.818603515625, 74.36837768554688], "image_path": "", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 617}, {"image_id": "img_9_103", "page_number": 10, "bbox": [299.616455078125, 127.16980743408203, 378.818603515625, 128.36984252929688], "image_path": "", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 414}, {"image_id": "img_9_104", "page_number": 10, "bbox": [299.616455078125, 128.3698272705078, 378.818603515625, 129.5698699951172], "image_path": "", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 1335}, {"image_id": "img_9_105", "page_number": 10, "bbox": [299.616455078125, 129.56985473632812, 378.818603515625, 130.7698974609375], "image_path": "", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 422}, {"image_id": "img_9_106", "page_number": 10, "bbox": [299.616455078125, 130.76991271972656, 378.818603515625, 131.96995544433594], "image_path": "", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 1337}, {"image_id": "img_9_107", "page_number": 10, "bbox": [299.616455078125, 131.96994018554688, 378.818603515625, 133.16998291015625], "image_path": "", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 431}, {"image_id": "img_9_108", "page_number": 10, "bbox": [299.616455078125, 133.1699676513672, 378.818603515625, 134.37001037597656], "image_path": "", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 1460}, {"image_id": "img_9_109", "page_number": 10, "bbox": [299.616455078125, 134.37001037597656, 378.818603515625, 135.57005310058594], "image_path": "", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 424}, {"image_id": "img_9_110", "page_number": 10, "bbox": [299.616455078125, 135.57003784179688, 378.818603515625, 136.77008056640625], "image_path": "", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 1300}, {"image_id": "img_9_111", "page_number": 10, "bbox": [299.616455078125, 136.7700653076172, 378.818603515625, 137.97010803222656], "image_path": "", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 381}, {"image_id": "img_9_112", "page_number": 10, "bbox": [299.616455078125, 137.97012329101562, 378.818603515625, 138.57012939453125], "image_path": "", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 3, "format": "PNG", "file_size": 687}, {"image_id": "img_9_113", "page_number": 10, "bbox": [299.616455078125, 74.36837005615234, 378.818603515625, 75.56840515136719], "image_path": "", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 938}, {"image_id": "img_9_114", "page_number": 10, "bbox": [299.616455078125, 152.7523193359375, 378.818603515625, 153.95236206054688], "image_path": "", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 936}, {"image_id": "img_9_115", "page_number": 10, "bbox": [299.616455078125, 153.95236206054688, 378.818603515625, 155.15240478515625], "image_path": "", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 861}, {"image_id": "img_9_116", "page_number": 10, "bbox": [299.616455078125, 155.15237426757812, 378.818603515625, 156.3524169921875], "image_path": "", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 851}, {"image_id": "img_9_117", "page_number": 10, "bbox": [299.616455078125, 156.3524169921875, 378.818603515625, 157.55245971679688], "image_path": "", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 598}, {"image_id": "img_9_118", "page_number": 10, "bbox": [299.616455078125, 157.55245971679688, 378.818603515625, 158.75250244140625], "image_path": "", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 925}, {"image_id": "img_9_119", "page_number": 10, "bbox": [299.616455078125, 158.75250244140625, 378.818603515625, 159.95254516601562], "image_path": "", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 853}, {"image_id": "img_9_120", "page_number": 10, "bbox": [299.616455078125, 159.9525146484375, 378.818603515625, 161.15255737304688], "image_path": "", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 922}, {"image_id": "img_9_121", "page_number": 10, "bbox": [299.616455078125, 161.15255737304688, 378.818603515625, 162.35260009765625], "image_path": "", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 969}, {"image_id": "img_9_122", "page_number": 10, "bbox": [299.616455078125, 162.3525848388672, 378.818603515625, 163.55262756347656], "image_path": "", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 656}, {"image_id": "img_9_123", "page_number": 10, "bbox": [299.616455078125, 163.5526123046875, 378.818603515625, 164.75265502929688], "image_path": "", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 932}, {"image_id": "img_9_124", "page_number": 10, "bbox": [299.616455078125, 75.56841278076172, 378.818603515625, 76.76844787597656], "image_path": "", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 883}, {"image_id": "img_9_125", "page_number": 10, "bbox": [299.616455078125, 164.7526397705078, 378.818603515625, 165.9526824951172], "image_path": "", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 819}, {"image_id": "img_9_126", "page_number": 10, "bbox": [299.616455078125, 165.95269775390625, 378.818603515625, 167.15274047851562], "image_path": "", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 890}, {"image_id": "img_9_127", "page_number": 10, "bbox": [299.616455078125, 167.1527099609375, 378.818603515625, 168.35275268554688], "image_path": "", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 650}, {"image_id": "img_9_128", "page_number": 10, "bbox": [299.616455078125, 168.35275268554688, 378.818603515625, 169.55279541015625], "image_path": "", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 1011}, {"image_id": "img_9_129", "page_number": 10, "bbox": [299.616455078125, 169.55276489257812, 378.818603515625, 170.7528076171875], "image_path": "", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 624}, {"image_id": "img_9_130", "page_number": 10, "bbox": [299.616455078125, 170.7528076171875, 378.818603515625, 171.95285034179688], "image_path": "", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 1025}, {"image_id": "img_9_131", "page_number": 10, "bbox": [299.616455078125, 171.95285034179688, 378.818603515625, 173.15289306640625], "image_path": "", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 573}, {"image_id": "img_9_132", "page_number": 10, "bbox": [299.616455078125, 173.1528778076172, 378.818603515625, 174.35292053222656], "image_path": "", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 1065}, {"image_id": "img_9_133", "page_number": 10, "bbox": [299.616455078125, 174.3529052734375, 378.818603515625, 175.55294799804688], "image_path": "", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 621}, {"image_id": "img_9_134", "page_number": 10, "bbox": [299.616455078125, 175.55294799804688, 378.818603515625, 176.75299072265625], "image_path": "", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 1014}, {"image_id": "img_9_135", "page_number": 10, "bbox": [299.616455078125, 76.76842498779297, 378.818603515625, 77.96846008300781], "image_path": "", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 850}, {"image_id": "img_9_136", "page_number": 10, "bbox": [299.616455078125, 176.75296020507812, 378.818603515625, 177.9530029296875], "image_path": "", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 427}, {"image_id": "img_9_137", "page_number": 10, "bbox": [299.616455078125, 177.9530029296875, 378.818603515625, 179.15304565429688], "image_path": "", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 995}, {"image_id": "img_9_138", "page_number": 10, "bbox": [299.616455078125, 179.15304565429688, 378.818603515625, 180.35308837890625], "image_path": "", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 653}, {"image_id": "img_9_139", "page_number": 10, "bbox": [299.616455078125, 180.3530731201172, 378.818603515625, 181.55311584472656], "image_path": "", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 1098}, {"image_id": "img_9_140", "page_number": 10, "bbox": [299.616455078125, 181.5531005859375, 378.818603515625, 182.75314331054688], "image_path": "", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 414}, {"image_id": "img_9_141", "page_number": 10, "bbox": [299.616455078125, 182.75314331054688, 378.818603515625, 183.95318603515625], "image_path": "", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 1157}, {"image_id": "img_9_142", "page_number": 10, "bbox": [299.616455078125, 183.95318603515625, 378.818603515625, 185.15322875976562], "image_path": "", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 413}, {"image_id": "img_9_143", "page_number": 10, "bbox": [299.616455078125, 185.1531982421875, 378.818603515625, 186.35324096679688], "image_path": "", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 956}, {"image_id": "img_9_144", "page_number": 10, "bbox": [299.616455078125, 186.35324096679688, 378.818603515625, 187.55328369140625], "image_path": "", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 431}, {"image_id": "img_9_145", "page_number": 10, "bbox": [299.616455078125, 187.55328369140625, 378.818603515625, 188.75332641601562], "image_path": "", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 1174}, {"image_id": "img_9_146", "page_number": 10, "bbox": [299.616455078125, 77.96846771240234, 378.818603515625, 79.16850280761719], "image_path": "", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 826}, {"image_id": "img_9_147", "page_number": 10, "bbox": [299.616455078125, 188.75331115722656, 378.818603515625, 189.95335388183594], "image_path": "", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 429}, {"image_id": "img_9_148", "page_number": 10, "bbox": [299.616455078125, 189.95333862304688, 378.818603515625, 191.15338134765625], "image_path": "", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 1189}, {"image_id": "img_9_149", "page_number": 10, "bbox": [299.616455078125, 191.1533660888672, 378.818603515625, 192.35340881347656], "image_path": "", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 425}, {"image_id": "img_9_150", "page_number": 10, "bbox": [299.616455078125, 192.3533935546875, 378.818603515625, 193.55343627929688], "image_path": "", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 1162}, {"image_id": "img_9_151", "page_number": 10, "bbox": [299.616455078125, 193.55343627929688, 378.818603515625, 194.75347900390625], "image_path": "", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 405}, {"image_id": "img_9_152", "page_number": 10, "bbox": [299.616455078125, 194.75344848632812, 378.818603515625, 195.9534912109375], "image_path": "", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 1137}, {"image_id": "img_9_153", "page_number": 10, "bbox": [299.616455078125, 195.95350646972656, 378.818603515625, 197.15354919433594], "image_path": "", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 397}, {"image_id": "img_9_154", "page_number": 10, "bbox": [299.616455078125, 197.15353393554688, 378.818603515625, 198.35357666015625], "image_path": "", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 1289}, {"image_id": "img_9_155", "page_number": 10, "bbox": [299.616455078125, 198.35357666015625, 378.818603515625, 199.55361938476562], "image_path": "", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 402}, {"image_id": "img_9_156", "page_number": 10, "bbox": [299.616455078125, 199.5535888671875, 378.818603515625, 200.75363159179688], "image_path": "", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 363, "height": 6, "format": "PNG", "file_size": 1287}, {"image_id": "img_12_0", "page_number": 13, "bbox": [49.406314849853516, 222.96978759765625, 105.10527038574219, 278.6687316894531], "image_path": "", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 271, "height": 271, "format": "PNG", "file_size": 1914}, {"image_id": "img_12_1", "page_number": 13, "bbox": [114.79092407226562, 222.96978759765625, 170.4898681640625, 278.6687316894531], "image_path": "", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 271, "height": 271, "format": "PNG", "file_size": 1702}, {"image_id": "img_12_2", "page_number": 13, "bbox": [180.175537109375, 222.96978759765625, 235.87448120117188, 278.6687316894531], "image_path": "", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 271, "height": 271, "format": "PNG", "file_size": 1821}, {"image_id": "img_12_3", "page_number": 13, "bbox": [245.5601348876953, 222.96978759765625, 301.25909423828125, 278.6687316894531], "image_path": "", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 271, "height": 271, "format": "PNG", "file_size": 1796}, {"image_id": "img_12_4", "page_number": 13, "bbox": [310.9447937011719, 222.96978759765625, 366.64373779296875, 278.6687316894531], "image_path": "", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 271, "height": 271, "format": "PNG", "file_size": 1489}, {"image_id": "img_12_5", "page_number": 13, "bbox": [376.3293762207031, 222.96978759765625, 432.0283203125, 278.6687316894531], "image_path": "", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 271, "height": 271, "format": "PNG", "file_size": 1603}, {"image_id": "img_12_6", "page_number": 13, "bbox": [441.7139892578125, 222.96978759765625, 497.4129333496094, 278.6687316894531], "image_path": "", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 271, "height": 271, "format": "PNG", "file_size": 1558}, {"image_id": "img_12_7", "page_number": 13, "bbox": [507.0986328125, 222.96978759765625, 562.797607421875, 278.6687316894531], "image_path": "", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 271, "height": 271, "format": "PNG", "file_size": 1671}, {"image_id": "img_12_8", "page_number": 13, "bbox": [49.01103210449219, 301.5858459472656, 99.47600555419922, 328.4783630371094], "image_path": "", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 304, "height": 162, "format": "PNG", "file_size": 1361}, {"image_id": "img_12_9", "page_number": 13, "bbox": [537.1239013671875, 292.93707275390625, 563.0203857421875, 337.09393310546875], "image_path": "", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 156, "height": 266, "format": "PNG", "file_size": 837}, {"image_id": "img_12_10", "page_number": 13, "bbox": [101.883056640625, 301.5858459472656, 152.3480224609375, 328.4783630371094], "image_path": "", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 304, "height": 162, "format": "PNG", "file_size": 1266}, {"image_id": "img_12_11", "page_number": 13, "bbox": [167.01974487304688, 292.93707275390625, 192.91624450683594, 337.09393310546875], "image_path": "", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 156, "height": 266, "format": "PNG", "file_size": 865}, {"image_id": "img_12_12", "page_number": 13, "bbox": [219.8917694091797, 292.93707275390625, 245.78826904296875, 337.09393310546875], "image_path": "", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 156, "height": 266, "format": "PNG", "file_size": 776}, {"image_id": "img_12_13", "page_number": 13, "bbox": [272.7637939453125, 292.93707275390625, 298.6602783203125, 337.09393310546875], "image_path": "", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 156, "height": 266, "format": "PNG", "file_size": 757}, {"image_id": "img_12_14", "page_number": 13, "bbox": [325.63580322265625, 292.93707275390625, 351.53228759765625, 337.09393310546875], "image_path": "", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 156, "height": 266, "format": "PNG", "file_size": 852}, {"image_id": "img_12_15", "page_number": 13, "bbox": [378.5078430175781, 292.93707275390625, 404.40435791015625, 337.09393310546875], "image_path": "", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 156, "height": 266, "format": "PNG", "file_size": 840}, {"image_id": "img_12_16", "page_number": 13, "bbox": [431.3798522949219, 292.93707275390625, 457.2763671875, 337.09393310546875], "image_path": "", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 156, "height": 266, "format": "PNG", "file_size": 839}, {"image_id": "img_12_17", "page_number": 13, "bbox": [431.3798522949219, 292.93707275390625, 457.2763671875, 337.09393310546875], "image_path": "", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 156, "height": 266, "format": "PNG", "file_size": 839}, {"image_id": "img_13_0", "page_number": 14, "bbox": [375.6044616699219, 147.11178588867188, 530.95166015625, 244.20376586914062], "image_path": "", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 1536, "height": 960, "format": "PNG", "file_size": 126102}, {"image_id": "img_13_1", "page_number": 14, "bbox": [375.6044616699219, 250.28823852539062, 530.95166015625, 347.3802185058594], "image_path": "", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 1536, "height": 960, "format": "PNG", "file_size": 56716}, {"image_id": "img_13_2", "page_number": 14, "bbox": [218.12127685546875, 146.72341918945312, 373.46844482421875, 243.81539916992188], "image_path": "", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 1536, "height": 960, "format": "PNG", "file_size": 111242}, {"image_id": "img_13_3", "page_number": 14, "bbox": [218.250732421875, 250.2235107421875, 373.597900390625, 347.31549072265625], "image_path": "", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 1536, "height": 960, "format": "PNG", "file_size": 59816}, {"image_id": "img_13_4", "page_number": 14, "bbox": [98.95703125, 138.5029754638672, 212.14901733398438, 219.87451171875], "image_path": "", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 3323, "height": 2391, "format": "PNG", "file_size": 238050}, {"image_id": "img_13_5", "page_number": 14, "bbox": [113.0677261352539, 230.02838134765625, 189.82008361816406, 328.41961669921875], "image_path": "", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 1540, "height": 1974, "format": "PNG", "file_size": 121657}, {"image_id": "img_13_6", "page_number": 14, "bbox": [189.77040100097656, 243.8154296875, 207.1175079345703, 326.73199462890625], "image_path": "", "caption": null, "alt_text": null, "figure_number": null, "image_type": "figure", "width": 1222, "height": 5550, "format": "PNG", "file_size": 115307}], "tables": [{"table_id": "table_9_0", "page_number": 10, "bbox": [203.88656616210938, 67.27725219726562, 282.54327392578125, 137.75189208984375], "caption": null, "table_number": null, "data": [["", "", "", "", "8", "", "7", "", ""], ["3", "", "", "", "4", "", "", "", ""], ["", "3", "8", "4", "", "", "", "2", ""], ["", "", "6", "", "", "3", "", "", "8"], ["9", "", "", "", "", "", "", "", "6"], ["", "", "", "5", "", "", "", "", ""], ["", "", "", "", "", "2", "", "", "1"], ["", "2", "5", "", "3", "", "", "8", ""]], "headers": ["", "8", "4", "", "", "5", "", "6", ""], "markdown": "|  | 8 | 4 |  |  | 5 |  | 6 |  |\n| --- | --- | --- | --- | --- | --- | --- | --- | --- |\n|  |  |  |  | 8 |  | 7 |  |  |\n| 3 |  |  |  | 4 |  |  |  |  |\n|  | 3 | 8 | 4 |  |  |  | 2 |  |\n|  |  | 6 |  |  | 3 |  |  | 8 |\n| 9 |  |  |  |  |  |  |  | 6 |\n|  |  |  | 5 |  |  |  |  |  |\n|  |  |  |  |  | 2 |  |  | 1 |\n|  | 2 | 5 |  | 3 |  |  | 8 |  |"}, {"table_id": "table_9_1", "page_number": 10, "bbox": [212.5867919921875, 160.7707061767578, 273.9248046875, 214.9903564453125], "caption": null, "table_number": null, "data": [["5", "9", "6", "4", "7", "8", "1"], ["3", "8", "4", "9", "6", "1", "2"], ["1", "6", "2", "7", "3", "5", "9"], ["7", "2", "8", "5", "1", "4", "3"], ["9", "3", "5", "1", "8", "2", "7"], ["4", "7", "9", "6", "2", "3", "5"]], "headers": ["6", "1", "3", "8", "9", "7", "4"], "markdown": "| 6 | 1 | 3 | 8 | 9 | 7 | 4 |\n| --- | --- | --- | --- | --- | --- | --- |\n| 5 | 9 | 6 | 4 | 7 | 8 | 1 |\n| 3 | 8 | 4 | 9 | 6 | 1 | 2 |\n| 1 | 6 | 2 | 7 | 3 | 5 | 9 |\n| 7 | 2 | 8 | 5 | 1 | 4 | 3 |\n| 9 | 3 | 5 | 1 | 8 | 2 | 7 |\n| 4 | 7 | 9 | 6 | 2 | 3 | 5 |"}], "metadata": {"title": "Hierarchical Reasoning Model", "authors": ["Guan Wang", "Jin Li", "Yuhao Sun", "Xing Chen", "Changling Liu", "Yue Wu", "Meng Lu", "Sen Song", "Yasin Abbasi Yadkori"], "abstract": null, "keywords": [], "doi": null, "arxiv_id": null, "publication_date": null, "journal": null, "conference": null, "pages": null, "volume": null, "issue": null, "url": null, "file_hash": null, "file_size": 0, "page_count": 24, "language": "en", "creation_date": null, "modification_date": null, "producer": "pikepdf 8.15.1", "creator": "arXiv GenPDF (tex2pdf:)"}, "processing_time": 21.63880968093872, "error_message": null, "confidence_score": 1.0}], "processing_stats": {"total_parsers_used": 2, "successful_parsers": 2, "final_quality": "excellent", "final_confidence": 1.0, "total_processing_time": 1527.3650059700012, "content_length": 76885, "section_count": 15, "image_count": 186, "table_count": 2}, "created_at": "2025-08-19T08:23:08.340984"}